{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "name": "Kalika Kay NLP Day 1 Assignments.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmiR91SN7deG"
      },
      "source": [
        "# Q & A\r\n",
        "*Try to answer the following questions without looking up the answers:*\r\n",
        "\r\n",
        "What properties of language make it difficult to analyze?\r\n",
        "Grammar, syntax, variances (different languages), typos, evolution (it evolves over time), slang, and semantics.\r\n",
        "\r\n",
        "What is the difference between a lexicon and a grammar?\r\n",
        "Lexicon is the entirety of the language grammar is how you use it.\r\n",
        "\r\n",
        "What is the difference between syntax and morphology?\r\n",
        "\r\n",
        "Morphology is the study of words and other meaningful units of language. Syntax is the study of sentences and phrases, and the rules of grammar that sentences obey.\r\n",
        "\r\n",
        "What is the difference between a word stem and a lemma?\r\n",
        "The stem of a word is not it's lemma. It's just what the word would look like minus the suffixes and the prefixes. The lemma is, like the root of the word.\r\n",
        "\r\n",
        "Name 4 examples of parts of speech.\r\n",
        "Noun, Adverb, adjective, conjugation.\r\n",
        "\r\n",
        "What are some examples of named entities?\r\n",
        "\r\n",
        "People, Places, Pets (sometimes?), etc. \r\n",
        "\r\n",
        "What are the steps in the NLP workflow?\r\n",
        "Acquire, Build/Store corpus, preprocess, analyize, vector, model, deploy. \r\n",
        "\r\n",
        "Provide 3 examples of real-world NLP applications.\r\n",
        "\r\n",
        "Chatbots, speech to text, digital assistants."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IihPCGvd34PH"
      },
      "source": [
        "# Text Acquisition & Ingestion Assignment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhLA_w7p34PM"
      },
      "source": [
        "import json\n",
        "import requests\n",
        "import feedparser\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZ3ytdXA34PR"
      },
      "source": [
        "### Iterate through the list of article URLs below, scraping the text from each one and saving it to a text file. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDbZqPpT34PU"
      },
      "source": [
        "articles = ['http://lite.cnn.io/en/article/h_eac18760a7a7f9a1bf33616f1c4a336d',\n",
        "            'http://lite.cnn.io/en/article/h_de3f82f17d289680dd2b47c6413ebe7c',\n",
        "            'http://lite.cnn.io/en/article/h_72f4dc9d6f35458a89af014b62e625ad',\n",
        "            'http://lite.cnn.io/en/article/h_aa21fe6bf176071cb49e09d422c3adf0',\n",
        "            'http://lite.cnn.io/en/article/h_8ad34a532921c9076cdc9d7390d2f1bc',\n",
        "            'http://lite.cnn.io/en/article/h_84422c79110d9989177cfaf1c5f45fe7',\n",
        "            'http://lite.cnn.io/en/article/h_d010d9580abac3a44c6181ec6fb63d58',\n",
        "            'http://lite.cnn.io/en/article/h_fb11f4e9d7c5323e75b337d9e9e5e368',\n",
        "            'http://lite.cnn.io/en/article/h_7b27f0b131067f8ece6238ac559670ab',\n",
        "            'http://lite.cnn.io/en/article/h_8cae7f735fa9573d470f802063ceffe2',\n",
        "            'http://lite.cnn.io/en/article/h_72c3668280e82576fcc2602b0fa70c14',\n",
        "            'http://lite.cnn.io/en/article/h_d20658fb0e20212051cda0e0a7248c8a',\n",
        "            'http://lite.cnn.io/en/article/h_56611c43d7928120d2ae21666ccc7417',\n",
        "            'http://lite.cnn.io/en/article/h_bda0394e3c5ee7054ee65c022bca7695']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbR68Xj1tVuV"
      },
      "source": [
        "PATH = 'drive/MyDrive/content/articles/'\r\n",
        "i = 0\r\n",
        "for article in articles: \r\n",
        "  response = requests.get(article)\r\n",
        "  content = response.text\r\n",
        " \r\n",
        "  #class name = afe4286c\r\n",
        "  soup = BeautifulSoup(content, 'lxml')\r\n",
        "\r\n",
        "  text_list = [t.get_text(separator=\"\\n\") for t in soup.find_all(name='div', attrs={'class': 'afe4286c'})]\r\n",
        "  \r\n",
        "  with open(PATH + f'article_{i}.txt', 'wb') as f:\r\n",
        "    f.write(str(text_list).encode())\r\n",
        "  i += 1\r\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgGlAM2X34Pc"
      },
      "source": [
        "### Ingest the text files generated via web scraping into a corpus and print the corpus statistics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmybAnNB34Pf"
      },
      "source": [
        "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\r\n",
        "\r\n",
        "DOC_PATTERN = r'.*\\.txt'\r\n",
        "corpus = PlaintextCorpusReader(PATH, DOC_PATTERN)\r\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Yll4e-B2DkQ"
      },
      "source": [
        "def corpus_stats(corpus):\r\n",
        "  print(\"corpus statistics:\")\r\n",
        "  print(f'Number of Documents: {str(len(corpus.fileids()))}')\r\n",
        "  print(f'Number of paragraphs: {str(len(corpus.paras()))}')\r\n",
        "  print(f'Number of sentences: {str(len(corpus.sents()))}')\r\n",
        "  print(f'Number of words: {str(len(corpus.words()))}')\r\n",
        "  print(f'Vocabulary: {str(len(set(w.lower() for w in corpus.words())))}')\r\n",
        "  print(f'avg chars per word: {str(round(len(corpus.raw())/len(corpus.words()), 1))}')\r\n",
        "  print(f'avg words per sentence: {str(round(len(corpus.words())/len(corpus.sents()), 1))}')"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9aFvbFr4CEd",
        "outputId": "7d7ed4fa-92ca-4234-8b6b-7afd2f52f3e0"
      },
      "source": [
        "import nltk\r\n",
        "corpus_stats(corpus)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "corpus statistics:\n",
            "Number of Documents: 14\n",
            "Number of paragraphs: 14\n",
            "Number of sentences: 395\n",
            "Number of words: 14102\n",
            "Vocabulary: 3058\n",
            "avg chars per word: 5.0\n",
            "avg words per sentence: 35.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HgmEWYd4eI1",
        "outputId": "98b14ab1-6fca-4a67-a2fc-437ab1040c87"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGP_4yfR34Pi"
      },
      "source": [
        "### Parse the O'Reilly Radar RSS feed below, extract the text from each post, and save it to a text file.\n",
        "\n",
        "The content of each post contains HTML tags. Strip those out using the same approach you used for web scraping so that only text is saved to the files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDbqkFCF34Pl"
      },
      "source": [
        "feed = 'http://feeds.feedburner.com/oreilly/radar/atom'"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-a9JgWH34Po"
      },
      "source": [
        "parsed = feedparser.parse(feed)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FeqHAJ44-N5"
      },
      "source": [
        "posts = parsed.entries"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "NaeOb8P75D3l",
        "outputId": "c921318a-c771-4c13-92ab-1f5241fbcf8c"
      },
      "source": [
        "posts[0][\"summary\"]"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2020 has been a year of great challenges for so many, but it’s not all negative. Around the world, organizations and their workforces have risen to the occasion, recognizing the importance of expanding their knowledge, taking on new tasks, and bettering themselves both personally and professionally. With the uptick in virtual conferencing, remote work, and, [&#8230;]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqK4w9sa34Pr"
      },
      "source": [
        "### Ingest the text files generated via RSS parsing into a corpus and print the corpus statistics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd4x36-934Ps"
      },
      "source": [
        "PATH = 'drive/MyDrive/content/rss/'\r\n",
        "\r\n",
        "for i, post in enumerate(posts):\r\n",
        "  text = post.summary\r\n",
        "\r\n",
        "  with open(PATH + f'post_{i}.txt', 'wb') as f:\r\n",
        "    f.write(text.encode())"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6GSS7SD5fju",
        "outputId": "f983d4a6-81a2-4fea-e5f6-b3fdafb4a561"
      },
      "source": [
        "DOC_PATTERN = r'.*\\.txt'\r\n",
        "corpus = PlaintextCorpusReader(PATH, DOC_PATTERN)\r\n",
        "\r\n",
        "corpus_stats(corpus)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "corpus statistics:\n",
            "Number of Documents: 60\n",
            "Number of paragraphs: 60\n",
            "Number of sentences: 197\n",
            "Number of words: 4515\n",
            "Vocabulary: 1467\n",
            "avg chars per word: 4.9\n",
            "avg words per sentence: 22.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BDwjO8t34Pw"
      },
      "source": [
        "### Make an API call to the Hacker News API to retrieve their Ask, Show, and Job category items. \n",
        "\n",
        "- URL: https://hacker-news.firebaseio.com/v0/askstories.json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7vyWOSN34Px",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8eb541d4-6af2-4f91-c248-632e730b677a"
      },
      "source": [
        "url = 'https://hacker-news.firebaseio.com/v0/askstories.json'\r\n",
        "\r\n",
        "response = requests.get(url).json()\r\n",
        "\r\n",
        "print(response)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[25562022, 25561398, 25560185, 25553818, 25558927, 25559755, 25559274, 25554464, 25560180, 25559174, 25553772, 25540583, 25538586, 25556199, 25537230, 25559143, 25555746, 25556563, 25557720, 25546445, 25525457, 25541269, 25541616, 25558741, 25557852, 25525426, 25548802, 25553613, 25530700, 25533487, 25547050, 25541964, 25557927, 25552885, 25542676, 25557631, 25545469, 25551133, 25544753, 25538405, 25531729, 25550111, 25546557, 25545136, 25542679, 25551290, 25540343, 25541939, 25559571, 25542290, 25533051, 25538258, 25538128, 25541828, 25528481, 25535752, 25540059, 25544961, 25550627, 25528596, 25543287, 25555295, 25543087, 25526708, 25542812, 25530559, 25535332, 25528837, 25533472, 25525590, 25539594, 25539230, 25537569, 25535792, 25533505, 25534981, 25526280, 25533682, 25542189, 25549841, 25548696, 25527401, 25526579, 25545525, 25549864, 25525446, 25536672, 25539190, 25533770, 25527006, 25526511, 25537279, 25534307, 25550782, 25542445, 25534168, 25553809, 25553555]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q12-lJiM9MpV",
        "outputId": "e65cae3b-c988-4581-ff3e-72445e33b1a9"
      },
      "source": [
        "print(len(response))"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "98\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEasqCkr34P0"
      },
      "source": [
        "### Once you have retrieved the item IDs from the URL above, retrieve each item by adding the item ID to the URL below, extract the item's text property, and save the text from each item to disk as its own document.\n",
        "\n",
        "- URL: https://hacker-news.firebaseio.com/v0/item/ITEM_ID_HERE.json\n",
        "\n",
        "The content of some items may contain HTML tags. Strip those out using the same approach you used for web scraping so that only text is saved to the files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Vp7yfWB34P1"
      },
      "source": [
        "for id in response:\r\n",
        "  url = f'https://hacker-news.firebaseio.com/v0/item/{id}.json'\r\n",
        "  article = requests.get(url).json()\r\n",
        "  if \"text\" in article:\r\n",
        "    soup = BeautifulSoup(article['text'], 'lxml')\r\n",
        "    PATH = 'drive/MyDrive/content/api/'\r\n",
        "    \r\n",
        "    with open(PATH + f'{id}.txt', 'wb') as f:\r\n",
        "      f.write(text.encode())   "
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw9-v8sK34P5"
      },
      "source": [
        "### Ingest the text files generated via API into a corpus and print the corpus statistics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcsiPdkx34P7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea126665-7a66-43a5-fc0b-fe5743283897"
      },
      "source": [
        "DOC_PATTERN = r'.*\\.txt'\r\n",
        "corpus = PlaintextCorpusReader(PATH, DOC_PATTERN)\r\n",
        "\r\n",
        "corpus_stats(corpus)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "corpus statistics:\n",
            "Number of Documents: 77\n",
            "Number of paragraphs: 77\n",
            "Number of sentences: 231\n",
            "Number of words: 5852\n",
            "Vocabulary: 61\n",
            "avg chars per word: 5.3\n",
            "avg words per sentence: 25.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdOqbkzt-g2Q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}