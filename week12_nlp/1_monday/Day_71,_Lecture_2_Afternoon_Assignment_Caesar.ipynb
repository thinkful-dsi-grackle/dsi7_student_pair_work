{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3-final"
    },
    "colab": {
      "name": "Day 71, Lecture 2: Afternoon Assignment_Caesar.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IihPCGvd34PH"
      },
      "source": [
        "# Text Acquisition & Ingestion Assignment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzI9-vz6-qmY",
        "outputId": "f4d1655b-5ec6-4933-9a43-08cdfba91e6e"
      },
      "source": [
        "!pip install feedparser"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting feedparser\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/21/faf1bac028662cc8adb2b5ef7a6f3999a765baa2835331df365289b0ca56/feedparser-6.0.2-py3-none-any.whl (80kB)\n",
            "\r\u001b[K     |████                            | 10kB 13.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 20kB 18.8MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 30kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 40kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 51kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 61kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 71kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 3.5MB/s \n",
            "\u001b[?25hCollecting sgmllib3k\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/bd/3704a8c3e0942d711c1299ebf7b9091930adae6675d7c8f476a7ce48653c/sgmllib3k-1.0.0.tar.gz\n",
            "Building wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-cp36-none-any.whl size=6066 sha256=c73cf90c25543e25a28dd6a203c0bd6d13fed21dc452a2bed1dfc67bcbb98096\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/80/5a/444ba08a550cdd241bd9baf8bae44be750efe370adb944506a\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser\n",
            "Successfully installed feedparser-6.0.2 sgmllib3k-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhLA_w7p34PM"
      },
      "source": [
        "import json\n",
        "import requests\n",
        "import feedparser\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZ3ytdXA34PR"
      },
      "source": [
        "### Iterate through the list of article URLs below, scraping the text from each one and saving it to a text file. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDbZqPpT34PU"
      },
      "source": [
        "articles = ['http://lite.cnn.io/en/article/h_eac18760a7a7f9a1bf33616f1c4a336d',\n",
        "            'http://lite.cnn.io/en/article/h_de3f82f17d289680dd2b47c6413ebe7c',\n",
        "            'http://lite.cnn.io/en/article/h_72f4dc9d6f35458a89af014b62e625ad',\n",
        "            'http://lite.cnn.io/en/article/h_aa21fe6bf176071cb49e09d422c3adf0',\n",
        "            'http://lite.cnn.io/en/article/h_8ad34a532921c9076cdc9d7390d2f1bc',\n",
        "            'http://lite.cnn.io/en/article/h_84422c79110d9989177cfaf1c5f45fe7',\n",
        "            'http://lite.cnn.io/en/article/h_d010d9580abac3a44c6181ec6fb63d58',\n",
        "            'http://lite.cnn.io/en/article/h_fb11f4e9d7c5323e75b337d9e9e5e368',\n",
        "            'http://lite.cnn.io/en/article/h_7b27f0b131067f8ece6238ac559670ab',\n",
        "            'http://lite.cnn.io/en/article/h_8cae7f735fa9573d470f802063ceffe2',\n",
        "            'http://lite.cnn.io/en/article/h_72c3668280e82576fcc2602b0fa70c14',\n",
        "            'http://lite.cnn.io/en/article/h_d20658fb0e20212051cda0e0a7248c8a',\n",
        "            'http://lite.cnn.io/en/article/h_56611c43d7928120d2ae21666ccc7417',\n",
        "            'http://lite.cnn.io/en/article/h_bda0394e3c5ee7054ee65c022bca7695']"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfDWUtDr34PX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "cc166ad4-930a-4069-9b2a-b2771aed4e55"
      },
      "source": [
        "path = 'cnn_articles/'\n",
        "\n",
        "for i, url in enumerate(articles):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text)\n",
        "    text = soup.find('div', {'class':'afe4286c'}).text\n",
        "    with open(path + f'article_{i}.txt', 'wb') as f:\n",
        "        f.write(text.encode())"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgGlAM2X34Pc"
      },
      "source": [
        "### Ingest the text files generated via web scraping into a corpus and print the corpus statistics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmybAnNB34Pf"
      },
      "source": [
        "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "doc_pattern = r'.*\\.txt'\n",
        "corpus = PlaintextCorpusReader(path, doc_pattern)\n",
        "corpus.fileids()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\c\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['article_0.txt',\n",
              " 'article_1.txt',\n",
              " 'article_10.txt',\n",
              " 'article_11.txt',\n",
              " 'article_12.txt',\n",
              " 'article_13.txt',\n",
              " 'article_2.txt',\n",
              " 'article_3.txt',\n",
              " 'article_4.txt',\n",
              " 'article_5.txt',\n",
              " 'article_6.txt',\n",
              " 'article_7.txt',\n",
              " 'article_8.txt',\n",
              " 'article_9.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus Statistics\n\nNumber of documents: 14\n\nNumber of paragraphs: 14\n\nNumber of sentences: 427\n\nNumber of words: 13824\n\nVocabulary: 2955\n\nAvg chars per word: 5\n\nAvg words per sentence: 32\n\n\n"
          ]
        }
      ],
      "source": [
        "def corpus_stats(corpus):\n",
        "    print(\n",
        "        f\"Corpus Statistics\\n\\n\"\n",
        "        f\"Number of documents: {len(corpus.fileids())}\\n\\n\"\n",
        "        f\"Number of paragraphs: {len(corpus.paras())}\\n\\n\"\n",
        "        f\"Number of sentences: {len(corpus.sents())}\\n\\n\"\n",
        "        f\"Number of words: {len(corpus.words())}\\n\\n\"\n",
        "        f\"Vocabulary: {len(set(w.lower() for w in corpus.words()))}\\n\\n\"\n",
        "        f\"Avg chars per word: {round(len(corpus.raw())/len(corpus.words()))}\\n\\n\"\n",
        "        f\"Avg words per sentence: {round(len(corpus.words())/len(corpus.sents()))}\\n\\n\"\n",
        "    )\n",
        "    \n",
        "corpus_stats(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGP_4yfR34Pi"
      },
      "source": [
        "### Parse the O'Reilly Radar RSS feed below, extract the text from each post, and save it to a text file.\n",
        "\n",
        "The content of each post contains HTML tags. Strip those out using the same approach you used for web scraping so that only text is saved to the files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDbqkFCF34Pl"
      },
      "source": [
        "feed = 'http://feeds.feedburner.com/oreilly/radar/atom'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-a9JgWH34Po"
      },
      "source": [
        "parsed = feedparser.parse(feed)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2020 has been a year of great challenges for so many, but it’s not all negative. Around the world, organizations and their workforces have risen to the occasion, recognizing the importance of expanding their knowledge, taking on new tasks, and bettering themselves both personally and professionally. With the uptick in virtual conferencing, remote work, and, [&#8230;]'"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "parsed.entries[0].summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqK4w9sa34Pr"
      },
      "source": [
        "### Ingest the text files generated via RSS parsing into a corpus and print the corpus statistics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd4x36-934Ps"
      },
      "source": [
        "path = 'rss_articles/'\n",
        "\n",
        "for i, entry in enumerate(parsed.entries):\n",
        "    text = entry.summary\n",
        "    with open(path + f'article_{i}.txt', 'wb') as f:\n",
        "        f.write(text.encode())"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus Statistics\n\nNumber of documents: 60\n\nNumber of paragraphs: 60\n\nNumber of sentences: 197\n\nNumber of words: 4515\n\nVocabulary: 1467\n\nAvg chars per word: 5\n\nAvg words per sentence: 23\n\n\n"
          ]
        }
      ],
      "source": [
        "doc_pattern = r'.*\\.txt'\n",
        "corpus = PlaintextCorpusReader(path, doc_pattern)\n",
        "\n",
        "corpus_stats(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BDwjO8t34Pw"
      },
      "source": [
        "### Make an API call to the Hacker News API to retrieve their Ask, Show, and Job category items. \n",
        "\n",
        "- URL: https://hacker-news.firebaseio.com/v0/askstories.json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7vyWOSN34Px"
      },
      "source": [
        "ids = []\n",
        "for cat in ['ask', 'show', 'job']:\n",
        "    url = f'https://hacker-news.firebaseio.com/v0/{cat}stories.json'\n",
        "    response = requests.get(url)\n",
        "    print(response)\n",
        "    print(f'Added {len(response.json())} from {cat}stories')\n",
        "    ids.extend(response.json())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Response [200]>\n",
            "Added 90 from askstories\n",
            "<Response [200]>\n",
            "Added 41 from showstories\n",
            "<Response [200]>\n",
            "Added 60 from jobstories\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "191"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "len(ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEasqCkr34P0"
      },
      "source": [
        "### Once you have retrieved the item IDs from the URL above, retrieve each item by adding the item ID to the URL below, extract the item's text property, and save the text from each item to disk as its own document.\n",
        "\n",
        "- URL: https://hacker-news.firebaseio.com/v0/item/ITEM_ID_HERE.json\n",
        "\n",
        "The content of some items may contain HTML tags. Strip those out using the same approach you used for web scraping so that only text is saved to the files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Vp7yfWB34P1"
      },
      "source": [
        "url = f'https://hacker-news.firebaseio.com/v0/item/{ids[0]}.json'\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.json()['text'])\n",
        "soup.text"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I love asking new colleagues this question, figured I would open it here as well.What are you surprised isn’t being worked on more?'"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "path = 'api_articles/'\n",
        "\n",
        "for i, id_ in enumerate(ids):\n",
        "    url = f'https://hacker-news.firebaseio.com/v0/item/{id_}.json'\n",
        "    response = requests.get(url)\n",
        "    if 'text' in response.json().keys():\n",
        "        soup = BeautifulSoup(response.json()['text'])\n",
        "        text = soup.text\n",
        "        with open(path + f'article_{i}.txt', 'wb') as f:\n",
        "            f.write(text.encode())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw9-v8sK34P5"
      },
      "source": [
        "### Ingest the text files generated via API into a corpus and print the corpus statistics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcsiPdkx34P7"
      },
      "source": [
        "doc_pattern = r'.*\\.txt'\n",
        "corpus = PlaintextCorpusReader(path, doc_pattern)\n",
        "\n",
        "corpus_stats(corpus)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus Statistics\n\nNumber of documents: 79\n\nNumber of paragraphs: 79\n\nNumber of sentences: 295\n\nNumber of words: 8582\n\nVocabulary: 2177\n\nAvg chars per word: 5\n\nAvg words per sentence: 29\n\n\n"
          ]
        }
      ]
    }
  ]
}