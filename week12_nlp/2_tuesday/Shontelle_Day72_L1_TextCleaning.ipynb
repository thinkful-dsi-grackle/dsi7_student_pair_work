{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "name": "Shontelle_Day72_L1_TextCleaning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iCUbt7u4JIO"
      },
      "source": [
        "# Text Data Cleaning and Preprocessing Assignment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IhjCHxTcRzD",
        "outputId": "437667df-805e-460d-caa5-f8f4758c32ab"
      },
      "source": [
        "!pip install feedparser"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.6/dist-packages (6.0.2)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.6/dist-packages (from feedparser) (1.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuqMJsi-4JIS"
      },
      "source": [
        "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
        "from nltk import sent_tokenize\n",
        "from nltk import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "import json\n",
        "import requests\n",
        "import feedparser\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCY5p4L54JIY"
      },
      "source": [
        "### Read the O'Reilly RSS plain text file articles into a corpus using the NLTK's PlaintextCorpusReader."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmNkoVRZ4JIc"
      },
      "source": [
        "feed = 'http://feeds.feedburner.com/oreilly/radar/atom'\n",
        "parsed = feedparser.parse(feed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DpGSP6YcWzO",
        "outputId": "5b5a9e48-eba5-4803-bd00-73db50250158"
      },
      "source": [
        "len(parsed.entries)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFoPAHSocAnx",
        "outputId": "10e816c2-b158-49a8-e8d9-edfa902ffe73"
      },
      "source": [
        "PATH = '/content/rss_posts/' \n",
        "\n",
        "for i, entry in enumerate(parsed.entries):\n",
        "  text = ' '.join(map(str, [line.text for line in BeautifulSoup(entry['content'][0]['value']).find_all(['p','i'])]))\n",
        "  print(text)\n",
        "  print('----------------')\n",
        "  with open(PATH + f'post_{i}.txt', 'wb') as f:\n",
        "    f.write(text.encode())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020 has been a year of great challenges for so many, but it’s not all negative. Around the world, organizations and their workforces have risen to the occasion, recognizing the importance of expanding their knowledge, taking on new tasks, and bettering themselves both personally and professionally. With the uptick in virtual conferencing, remote work, and, for some, reentering the job market, new technology adoption was accelerated, driving the workforce to build new skills. While 2020 was the year of the global COVID-19 pandemic, it will also be commemorated as the year online learning prevailed. As vaccine development persists and life gets back to normal, with it will come a more future-proof workforce ready to share their new knowledge with the world.  Since the onset of the pandemic, online courses and programs have seen dramatic spikes in consumption and enrollment, and O’Reilly has been no different. A big contributor to O’Reilly’s continued success during these unprecedented times has been its live virtual training courses. This year, more than 900,000 users have registered for live events through O’Reilly online learning—a 96% increase from last year. This functionality also allowed O’Reilly to introduce its Superstream Series, a new lineup of virtual conferences featuring expert speakers delivering talks and training sessions on the most important topics and emerging trends in technology.  So what are the trends driving this uptick in learning? Companies are increasingly interested in understanding how to successfully adjust to remote work and effectively manage time. And individual O’Reilly members are looking to build and expand on their technical skills in everything from software architecture and microservices to AI and programming languages. But which topics are the brightest minds in technology most focused on? We’ve compiled the top 20 live online training courses of 2020 to shed some light on what those in the know want to know. Top 20 live online training courses of 2020 For a more in-depth analysis of the hot technology topics of 2020, based on data from O’Reilly online learning, stay tuned for our upcoming report, Wrapping Up 2020 (and What to Expect for 2021): Trends on O’Reilly online learning. \n",
            "----------------\n",
            "It has long seemed to me that functional programming is, essentially, programming viewed as mathematics. Many ideas in functional programming came from Alonzo Church’s Lambda Calculus, which significantly predates anything that looks remotely like a modern computer. Though the actual history of computing runs differently: in the early days of computing, Von Neumann’s ideas were more important than Church’s, and had a tremendous influence on the design of early computers—an influence that continues to the present. Von Neumann’s thinking was essentially imperative: a program is a list of commands that run on a machine designed to execute those commands.  So, what does it mean to say that functional programming is programming “viewed as mathematics”? Von Neumann was a “mathematician,” and programming of all kinds found its first home in Mathematics departments. So, if functional programming is mathematical, what does that mean? What kind of math? I’m not thinking of any specific branch of mathematics. Yes, the Lambda Calculus has significant ties to set theory, logic, category theory, and many other branches of mathematics. But let’s start with grade school mathematics and assignment statements; they’re basic to any programming language. We’re all familiar with code like this: Mathematically, this is nonsense. An equation is a statement about a relationship that holds true. i can equal i; it can’t equal i+1. And while i++ and i+=1 no longer look like equations, they are equally nonsensical; once you’ve said that i equals something, you can’t say it equals something else. “Variables” don’t change values; they’re immutable. Immutability is one of the most important principles of functional programming. Once you’ve defined a variable, you can’t change it. (You can create a new one in a different function scope, but that’s a different matter.) Variables, in functional programming, are invariant; and that’s important. You may be wondering “what about loops? How can I write a for loop?” Not only do you have to do without index variables, you can’t modify any of the variables in the loop body.  Setting aside the (solvable) problem of iteration, there’s no reason you can’t write code in (almost) any non-functional language that has this same effect. Just declare all your variables final or const. In the long run, functional programming is more about a specific kind of discipline than about language features. Programming languages can enforce certain rules, but in just about any modern language it’s possible to follow those rules without language support. Another important principle of functional programming is that functions are “first class entities.” That is, there are minimal restrictions about where you can use a function. You can also have functions without names, often called “lambdas” (which refers directly to the Lambda Calculus, in which functions were unnamed).  In Python, you can write code like this: The “key” is an anonymous function that returns a specific column of an array; that function is then used for sorting. Personally, I’m not overly fond of “anonymous functions”; it’s often clearer to write the anonymous function as a regular, named function. So I might write this: The ability to use functions as arguments to functions gives you a very nice way to implement the “strategy pattern”: I often get the sense that all programmers really want from functional programming is first-class functions and lambdas. Lambdas were added to Python very early on (1.0) but didn’t reach Java until Java 8.  Another consequence of thinking mathematically (and possibly a more important one) is that functions can’t have side-effects and, given the same arguments, will always return the same value. If a mathematician (or a high school trig student) writes they don’t have to deal with the possibility that sin(x) sets some global variable to 42, or will return a different value every time it’s called. That just can’t happen; in math, the idea of a “side-effect” is meaningless. All the information that sin(x) provides is encapsulated in the return value. In most programming languages, side-effects happen all too easily, and in some, they’re almost an obsession. Again, creating functions that have no side-effects is a matter of exercising discipline. A programming language can enforce this rule, but you can follow it whether or not your language makes you do it. We don’t have cartoon devils looking over our shoulders saying “Go ahead; make a side effect. No one will notice.” Functional languages vary the degree to which they enforce the lack of side-effects. If you’re a purist, anything that interacts with the real world is a side-effect. Printing a document? Changing a row in a database? Displaying a value on the user’s screen? Those are all side-effects (they aren’t completely encapsulated in the value returned by the function), and they have to be “hidden” using a mechanism like monads in Haskell. And that’s the point at which many programmers get confused and throw up their hands in despair. (I’ll only point you to Real World Haskell.) In both Java and Python, lambda functions can have side-effects, which means that, strictly speaking, they aren’t really “functional.” Guido van Rossum’s discussion of the addition of Lambdas to Python is worth reading; among other things, he says “I have never considered Python to be heavily influenced by functional languages, no matter what people say or think.” Streams are often associated with functional languages; they’re essentially long (perhaps infinite) lists that are evaluated lazily—meaning that elements of the string are only evaluated as they’re needed. Maps apply a function to every element of a list, returning a new list—and that includes streams, which (for these purposes) are specialized lists. That’s an incredibly useful feature; it’s a great way to write a loop without having to write a loop—and without even knowing how much data you have. You can also create “filters” that choose whether to pass any element of the stream to the output, and you can chain maps and filters together. If you think this sounds like a Unix pipeline, you’re right. Streams, maps, filters, and the act of chaining them together really have as much to do with the Unix shell as they do with functional languages. Another way to avoid writing loops is to use “comprehensions,” a feature of Python. It’s easy to get very fond of list comprehensions; they’re compact, they eliminate off-by-one errors, and they’re very flexible. Although comprehensions look like a compact notation for a traditional loop, they really come from set theory—and their closest computational “relatives” are to be found in relational databases, rather than functional programming. Here’s a comprehension that applies a function to every element of a list: The most general way to avoid traditional loops is to use recursion: a function that calls itself. Here’s the recursive equivalent to the previous comprehension: Recursion is a mainstay of functional languages: you don’t have indices being modified, and you’re not even modifying the resulting list (assuming that append doesn’t count as modification).  However, recursion has its own problems. It’s hard to wrap your mind around recursion; you still need to do a lot of your own bookkeeping (in this case, passing in a vector so a result can be returned); and except in one (common) special case, called “tail recursion,” it can be a performance nightmare. I started by saying that functional programming was programming considered as “math,” and that’s at least partially correct. But is that claim useful? There are many branches of mathematics that map onto programming concepts in different ways. Functional programming only represents one of them. If you’re a topologist, you may well like graph databases. But discussing which branch of mathematics corresponds to which programming practices isn’t really helpful. Remembering high school algebra may help when thinking about immutability, statelessness, and the absence of side-effects; but most programmers will never study the real mathematical origins of functional programing. Lambdas are great; functions as arguments in method calls is great; even recursion is (sometimes) great; but we’re fooling ourselves if we think programmers are going to start using Java as if it were Haskell. But that’s OK; for Java programmers, the value of Lambdas isn’t some mathematical notion of “functional,” but in providing a huge improvement over anonymous inner classes. The tools to be functional are there, should you choose to use them. In college, I learned that engineering was about making tradeoffs. Since then, I’ve heard very few programmers talk about tradeoffs—but those tradeoffs are still central to good engineering. And while engineering uses a lot of mathematics, engineering isn’t mathematics, in part because mathematics doesn’t deal in tradeoffs. Using “mathematics” as a way to think about a particular style of disciplined coding maybe be useful, particularly if that discipline leads to fewer bugs. It’s also useful to use the tools of mathematics to make good tradeoffs between rigor, performance, and practicality—which may lead you in an entirely different direction. Be as functional as you need to (but no more). \n",
            "----------------\n",
            "You start with a single component, the nand gate. Using this as the fundamental building block, you will build all other components necessary. today we are unveiling Recursive Belief-based Learning (ReBeL), a general RL+Search algorithm that can work in all two-player zero-sum games, including imperfect-information games. ReBeL builds on the RL+Search algorithms like AlphaZero that have proved successful in perfect-information games. Unlike those previous AIs, however, ReBeL makes decisions by factoring in the probability distribution of different beliefs each player might have about the current state of the game, which we call a public belief state (PBS). In other words, ReBeL can assess the chances that its poker opponent thinks it has, for example, a pair of aces. We demonstrate our claim by implementing tensor algebra and stochastic gradient descent using lambda expressions for loss functions as a pipelined operator in a main memory database system. Our approach enables common machine learning tasks to be performed faster than by extended disk-based database systems or as well as dedicated tools by eliminating the time needed for data extraction. This work aims to incorporate gradient descent and tensor data types into database systems, allowing them to handle a wider range of computational tasks.\n",
            "----------------\n",
            "Figuring out what shapes proteins fold into is known as the “protein folding problem”, and has stood as a grand challenge in biology for the past 50 years. In a major scientific advance, the latest version of our AI system AlphaFold has been recognised as a solution to this grand challenge by the organisers of the biennial Critical Assessment of protein Structure Prediction (CASP). The organizers even worried DeepMind may have been cheating somehow. So Lupas set a special challenge: a membrane protein from a species of archaea, an ancient group of microbes. For 10 years, his research team tried every trick in the book to get an x-ray crystal structure of the protein. “We couldn’t solve it.” But AlphaFold had no trouble. It returned a detailed image of a three-part protein with two long helical arms in the middle. The model enabled Lupas and his colleagues to make sense of their x-ray data; within half an hour, they had fit their experimental results to AlphaFold’s predicted structure. “It’s almost perfect,” Lupas says. “They could not possibly have cheated on this. I don’t know how they do it.” Far more useful (and to me, more impressive) than AlphaGo. Purpose-First Programming — Some students resist the cognitively-heavy tasks of simulating program execution. The secret to teaching those folks to program may be “purpose-first programming”: She used Github repositories and expert interviews to identify a few programming plans (just like Elliot Soloway and Jim Spohrer studied years ago) that were in common use in a domain that her participants cared about. She then taught those plans. Students modified and combined the plans to create programs that the students found useful. Rather than start with syntax or semantics, she started with the program’s purpose. Very reminiscent of the late 90s Perl and PHP copy-and-change coding boom that got orders of magnitude more people programming than were coming through CS courses at the time. She used Github repositories and expert interviews to identify a few programming plans (just like Elliot Soloway and Jim Spohrer studied years ago) that were in common use in a domain that her participants cared about. She then taught those plans. Students modified and combined the plans to create programs that the students found useful. Rather than start with syntax or semantics, she started with the program’s purpose.\n",
            "----------------\n",
            "This month’s collection of interesting articles that point to important trends is dominated by AI. That’s not surprising; AI has probably been the biggest single category all year. But its dominance over other topics seems to be increasing. That’s partly because there’s more research into why AI fails; partly because we’re beginning to see AI in embedded systems, ranging from giant gas and oil wells to the tiny devices that Pete Warden is working with.\n",
            "----------------\n",
            " McIlroy keeps coming up. He’s the smartest of all of us and the least remembered (or written down)… McIlroy sat there and wrote —on a piece of paper, now, not on a computer— TMG [a proprietary yacc-like program] written in TMG… And then! He now has TMG written in TMG, he decided to give his piece of paper to his piece of paper and write down what came out (the code). Which he did. And then he came over to my editor and he typed in his code, assembled it, and (I won’t say without error, but with so few errors you’d be astonished) he came up with a TMG compiler, on the PDP-7, written in TMG. And it’s the most basic, bare, impressive self-compilation I’ve ever seen in my life. all of the ROS World videos, including all the lightning talks we propose a simple approach called Language Shaped Learning (LSL): if we have access to explanations at training time, we encourage the model to learn representations that are not only helpful for classification, but are predictive of the language explanations. Mondays: Algorithms; Wednesdays: Theory of Computation; Fridays: Theory of Computation; Sundays: Livestream/bonus\n",
            "----------------\n",
            "Apple was responsible for more edits in 2019 than Mapbox accounted for in its entire corporate history. See also the 2020: Curious Cases of Corporations in OpenStreetMap talk from State of the Map. (via Simon Willison)     Azerbaijan, frustrated at a peace process that it felt delivered nothing, used its Caspian Sea oil wealth to buy arms, including a fleet of Turkish Bayraktar TB2 drones and Israeli kamikaze drones (also called loitering munitions, designed to hover in an area before diving on a target). […] Azerbaijan used surveillance drones to spot targets and sent armed drones or kamikaze drones to destroy them, analysts said. […] Their tally, which logs confirmed losses with photographs or videos, listed Armenian losses at 185 T-72 tanks; 90 armored fighting vehicles; 182 artillery pieces; 73 multiple rocket launchers; 26 surface-to-air missile systems, including a Tor system and five S-300s; 14 radars or jammers; one SU-25 war plane; four drones and 451 military vehicles. an efficient, single-machine system for performing data mining tasks on large graphs. Some graph mining applications include: Finding frequent subgraphs; Generating the motif/graphlet distribution; Finding all occurrences of a subgraph. Peregrine is highly programmable, so you can easily develop your own graph mining applications using its novel, declarative, graph-pattern-centric API. To write a Peregrine program, you describe which graph patterns you are interested in mining, and what to do with each occurrence of those patterns. You provide the what and the runtime handles the how. I found that the marginal returns of researchers are rapidly declining. There is what’s called a “standing on toes” effect: researcher productivity declines as the field grows. Because ML has recently grown very quickly, this makes better ML models much harder to find.  \n",
            "----------------\n",
            "Terminal/CLI Epub reader Simply put, ur-technical debt arises when my ideas diverge from my code. That divergence is inevitable with an iterative process. […] “[I]f you develop a program for a long period of time by only adding features and never reorganizing it to reflect your understanding of those features, then eventually that program simply does not contain any understanding and all efforts to work on it take longer and longer.” a real-time [REST and GraphQL] API and App dashboard for managing SQL database content.\n",
            "----------------\n",
            "If software is such stuff as dreams are made on, how do we talk about nightmares? Software is not the tangible, kickable stuff our senses are tuned to, so we draw on metaphor to communicate and reason about it. The 1970s offered up spaghetti code to describe the tangle of unstructured control flow. This has inspired many software-as-pasta descriptions, from lasagne for layered architectures to ravioli for—pick a decade—objects, components, modules, services, and microservices. Beyond its disordered arrangement, however, spaghetti has little to offer us as a metaphor. It doesn’t provide us with a useful mental model for talking about code, and has far too many positive associations. If you love both ravioli and spaghetti, it’s not obvious that one of these is worse for your software architecture than the other. A metaphor is a mapping that we use to describe one thing in terms of another—sometimes because we want to show something familiar from an unfamiliar angle, as in poetry, but sometimes because we want to show something unfamiliar or abstract in a more familiar light, as in software. To be considered good, a metaphor has to offer a number of points of useful correspondence with what is being described. Pasta doesn’t quite do this. Another quality of a good metaphor is that it should not have too many obvious points of conflict. It will never map its target perfectly—a metaphor is a conceit not an identity—but a good metaphor is one whose key qualities don’t contradict the very thing we are trying to say, whose points of difference don’t distract from the mental model being shared. We sometimes talk about code decay and software rot. These terms give a sense of degradation over time. This seems accurate and relatable. They also suggest a response: cleaning (we brush our teeth to reduce the chance of tooth decay) or treatment (we treat wood to avoid it rotting). So far so good… but the problem with these metaphors is they refer to natural processes that happen independently of anything we do. If you don’t brush your teeth, you will experience decay. If you don’t touch code, it doesn’t intrinsically degrade. The third quality of a metaphor that makes it effective is familiarity to its audience. Explaining something unfamiliar in terms of something else that is also unfamiliar can be a long road to travel a short distance (or to end up where you started). If you are familiar with the concept of entropy in statistical mechanics, with the second law of thermodynamics, and with the idea that work is needed to reduce entropy and increase order in a system, then software entropy might strike you as a descriptive metaphor—and not simply because the word work transfers happily from the world of thermodynamics to the day-to-day experience of developers. If, however, these concepts are not accessible and require explanation, then, regardless of its other merits, software entropy may not be the best way to talk about accidental complexity in code. Perhaps the most popular metaphor in use is based on financial debt, originating with Ward Cunningham in 1992. As Martin Fowler described in 2003: Technical Debt is a wonderful metaphor developed by Ward Cunningham to help us think about this problem. In this metaphor, doing things the quick and dirty way sets us up with a technical debt, which is similar to a financial debt. Like a financial debt, the technical debt incurs interest payments, which come in the form of the extra effort that we have to do in future development because of the quick and dirty design choice. When we look at technical debt, we see a metaphor that checks all three boxes: it has a number of useful points of correspondence; the points of difference don’t overwhelm the core idea; it is familiar. Furthermore, it brings with it a useful working vocabulary. For example, consider what the following debt-related words suggest to you in a software context: repayment, consolidation, creditworthiness, write-off, borrowing. Although we know that by definition no metaphor is perfect, there are two common ways in which the metaphor is misapplied: assuming technical debt is necessarily something bad; equating technical debt with a financial debt value. The emphasis of the former is misaligned and the latter is a category error. If we are relying on the common experience of our audience, financial debt is almost always thought of as a burden. If we take that together with the common experience of code quality and nudge it with leading descriptions such as “quick and dirty,” it is easy to see how in everyday use technical debt has become synonymous with poor code and poor practice. We are, however, drawing too heavily on the wrong connotation. Rather than reckless debt, such as from gambling, we should be thinking more along the lines of prudent debt, such as a mortgage. A mortgage should be offered based on our credit history and our ability to pay and, in return, we are able to buy a house that might otherwise have been beyond our reach. Similarly, Ward’s original motivation was to highlight how debt in code can be used for competitive advantage: Shipping first time code is like going into debt. A little debt speeds development so long as it is paid back promptly with a rewrite. This comes with a clear caveat and implication: a debt is a loan. A debt is for repayment, not for running up: The danger occurs when the debt is not repaid. Every minute spent on not-quite-right code counts as interest on that debt. Entire engineering organizations can be brought to a stand-still under the debt load of an unconsolidated implementation. As in the real world, how we run up debt and how we manage it turn out to be more complex than the simplicity of our best intentions. There are teams that make time-saving decisions wisely, revisiting and addressing them later in a timely manner. But in most cases where debt is incurred, discussed, and lamented, codebases reflect the firefight of different priorities, skills, and people. It’s still technical debt, but it lacks the prudence and intention of Ward’s original purpose. There are also teams and tools that embrace the debt metaphor so tightly that they forget it’s a metaphor. They treat it literally and numerically, converting code quality into a currency value on a spreadsheet or dashboard. The consequences of this thinko range from being a harmless fiction largely ignored by developers and managers to a more damaging numerology that, even though it’s well intentioned, can mislead development effort. If we’re going to quantify it, what is it we’re quantifying? Do we list off code smells? What is the debt value of a code smell? Is it constant per kind of code smell? For example, is duplicate code characterised by a single cost? And are code smells independent of one another? Consider that, for example, duplication is sometimes used to reduce coupling, so the debit becomes a credit in that context. We can conclude that a code smell is not an isolated thing with a single look-up debt value, so this is clearly a more complex problem dependent on many factors. As a multivariable problem, what does it depend on? And how? And how do we know? And what would the value or—more likely—value distribution reflect? The cost of fixing? Or, more honestly, an estimate of the cost of fixing? But even if we are somehow able to conjure a number out of this ever-growing list of considerations—and even if that number has some relation to observed reality—we have put a number to the wrong quantity. We have, in fact, missed the whole point of the metaphor. Technical debt is not the cost of repaying the debt: it is the cost of owning the debt. These are not the same. That is the message of the technical debt metaphor: it is not simply a measure of the specific work needed to repay the debt; it is the additional time and effort added to all past, present, and future work that comes from having the debt in the first place. By taking the metaphor literally, we have robbed it of its value. Its value is to offer us a figure of speech not of currency, a mental model for talking and reasoning about qualities of our code that are not simply stated in code. No matter how well meant, pushing any metaphor beyond its applicability leads to metaphor shear. It is, after all, metaphor and not identity.\n",
            "----------------\n",
            "Network and Distributed System Security Symposium We present Lifty, a domain-specific language for data-centric applications that manipulate sensitive data. A Lifty programmer annotates the sources of sensitive data with declarative security policies, and the language statically and automatically verifies that the application handles the data according to the policies. Moreover, if verification fails, Lifty suggests a provably correct repair, thereby easing the programmer burden of implementing policy enforcing code throughout the application. Create a schema by using visual blocks system. GraphQL Editor will transform them into code.\n",
            "----------------\n",
            "1. Basic Processor & Memory hierarchy; 2. Advanced Out-of-Order Processor; 3. Data-parallel processors; 4. Micro-controller introduction; 5. Multicore;  6. RISC-V core; 7. Advanced Multicore; 8. Multicore programming; 9. Graphics Processing Unit (GPU); 10. Heterogeneous SoC; 11. GPU Programming; 12. Application-Specific Instruction-Set Processor (ASIP); 13 PULP: Parallel Ultra-Low-Power Computing; 14. Architecture in the Future – Wrap-up Next-generation reliable, safe, concise, and functional-first programming language.\n",
            "Flix is a principled and flexible functional-, logic-, and imperative- programming language that takes inspiration from F#, Go, OCaml, Haskell, Rust, and Scala. Flix looks like Scala, but its type system is closer to that of OCaml and Haskell. Its concurrency model is inspired by Go-style processes and channels. Flix compiles to JVM bytecode, runs on the Java Virtual Machine, and supports full tail call elimination. supports first-class Datalog constraints enriched with lattice semantics. Abundance, connectivity, healthspan, capital, AR and Spatial Web, smart devices, human-level AI, AI-Human collaboration, software shells, renewable energy, insurance industry switches to prevention, autonomous vehicles and flying cars, on-demand production and delivery, knowledge, advertising, cellular agriculture, brain-computer interfaces, VR, sustainability/environment, and CRISPR. Level -2: No Authentication; Level -1: All Passwords = “password”; Level 0: Hardcode Everywhere; Level +1: Move Secrets into a Config File; Level +2: Encrypt the Config File; Level +3: Use a Secret Manager; Level +4: Dynamic Ephemeral Credentials\n",
            "----------------\n",
            "The programming world used to be split into functional languages, object-oriented languages, and everything else (mostly procedural languages). One “was” a functional programmer (at least as a hobby) writing Lisp, Haskell, or Erlang; or one “was” an OO programmer (at least professionally), writing code in Java or C++.  (One never called oneself a “procedural programmer”; when these names escaped from academia in the 1990s, calling yourself a “procedural programmer” would be akin to wearing wide ties and bell-bottom jeans.) But this world has been changing. Over the past two decades, we’ve seen the rise of hybrid programming languages that combine both functional and object-oriented features. Some of these languages (like Scala) were multi-paradigm from the beginning. Others, like Python (in the transition from Python 2 to 3) or Java (with the introduction of Lambdas in Java 8) are object-oriented or procedural languages to which functional features were added. Although we think of C++ as an object-oriented language, it has also been multi-paradigm from the beginning. It started with C, a procedural language, and added object-oriented features. Later, beginning with the Standard Template Library, C++ was influenced by many ideas from Scheme, a descendant of LISP.  JavaScript was also heavily influenced by Scheme, and popularized the idea of anonymous functions and functions as first class objects. And JavaScript was object-oriented from the start, with a prototype-based object model and syntax (though not semantics) that gradually evolved to become similar to Java’s. We’ve also seen the rise of languages combining static and dynamic typing (TypeScript in the JavaScript world; the addition of optional type hinting in Python 3.5; Rust has some limited dynamic typing features). Typing is another dimension in paradigm space. Dynamic typing leads to languages that make programming fun and where it’s easy to be productive, while strict typing makes it significantly easier to build, understand, and debug large systems. It’s always been easy to find people praising dynamic languages, but, except for a few years in the late 00s, the dynamic-static paradigmatic hasn’t attracted as much attention. Why do we still see holy wars between advocates of functional and object-oriented programming? That strikes me as a huge missed opportunity. What might “multi-paradigm programming” mean? What would it mean to reject purity and use whatever set of features provide the best solution in any given context? Most significant software is substantial enough that it certainly has components where an object-oriented paradigm makes more sense, and components where a functional paradigm is superior.  For example, look at a “functional” feature like recursion.  There are certainly algorithms that make much more sense recursively (Towers of Hanoi, or printing a sorted binary tree in order); there are algorithms where it doesn’t make much of a difference whether you use loops or recursion (whenever tail recursion optimizations will work); and there are certainly cases where recursion will be slow and memory-hungry. How many programmers know which solution is best in any situation? These are the sort of questions we need to start asking. Design patterns have been associated with object-oriented programming from the beginning. What kinds of design patterns make sense in a multi-paradigm world? Remember that design patterns aren’t “invented”; they’re observed, they’re solutions to problems that show up again and again, and that should become part of your repertoire. It’s unfortunate that functional programmers tend not to talk about design patterns; when you realize that patterns are observed solutions, statements like “patterns aren’t needed in functional languages” cease to make sense. Functional programmers certainly solve problems, and certainly see the same solutions show up repeatedly. We shouldn’t expect those problems and solutions to be the same problems and solutions that OO programmers observe. What patterns yield the best of both paradigms? What patterns might help to determine which approach is most appropriate in a given situation? Programming languages represent ways of thinking about problems. Over the years, the paradigms have multiplied, along with the problems we’re interested in solving. We now talk about event-driven programming, and many software systems are event-driven, at least on the front end. Metaprogramming was popularized by JUnit, the first widely used tool to rely on this feature that’s more often associated with functional languages; since then, several drastically different versions of metaprogramming have made new things possible in Java, Ruby, and other languages. We’ve never really addressed the problem of how to make these paradigms play well together; so far, languages that support multiple paradigms have left it to the programmers to figure out how to use them. But simply mixing paradigms ad hoc probably isn’t the ideal way to build large systems–and we’re now building software at scales and speeds that were hard to imagine only a few years ago. Our tools have improved; now we need to learn how to use them well. And that will inevitably involve blending paradigms that we’ve long viewed as distinct, or even in conflict.  Thanks to Kevlin Henney for ideas and suggestions!\n",
            "----------------\n",
            "Would the mental focus on a specific hypothesis prevent us from making a discovery? To test this, we made up a dataset and asked students to analyze it. […] The most notable “discovery” in the dataset was that if you simply plotted the number of steps versus the BMI, you would see an image of a gorilla waving at you (Fig. 1b). my issue was the fact that the systems doing the flashing were running the yocto images and perl and the guy writing the perl was also responsible for writing the thing that actually updates the car. that thing (the car-side updater) is about ~100k lines of C in a single file. code reviews were always a laugh riot.\n",
            "----------------\n",
            "system security starts at the hardware layer free home server for your comics and ebooks library a relational database management system developed by the Carnegie Mellon Database Group. The research goal of the NoisePage project is to develop high-performance system components that support autonomous operation and optimization as a first-class design principle.\n",
            "----------------\n",
            "Second-tier Scottish football club Inverness Caledonian Thistle doesn’t have a camera operator for matches at their stadium so the club uses an AI-controlled camera that’s programmed to follow the ball for their broadcasts. But in a recent match against Ayr United, the AI controller kept moving the camera off the ball to focus on the bald head of the linesman, making the match all but unwatchable. No fans allowed in the stadium either, so the broadcast was the only way to watch. Basically at midnight at the end of 1927, the clocks went back 5 minutes and 52 seconds. On average, UX improvements have substantially decreased since 2006–2008: from 247% to 75% (a 69% decrease). This difference is statistically significant (p = 0.01) — we can be quite confident that average improvement scores are lower now than they were 12–14 years ago. This hands-on course explores a selection of techniques from Programming Languages and Human-Computer Interaction that can help us create useful, usable programming languages and programming tools. We will cover strategies for designing programming systems—e.g., need finding, formative studies, user-centered design broadly. We will also cover tools and techniques that help us build user-friendly programming systems—e.g., program synthesis, structure editors, abstraction design, program slicing. For the final project, individuals or teams will develop a usable abstraction, language, or programming tool of their own design.\n",
            "----------------\n",
            "Perhaps the most important event this month isn’t technical, but the start of the US Justice Dept.’s lawsuit against Google. That will certainly play out over years rather than months, but it’s significance is less about this particular case than the idea that legal and regulatory systems will play a large role in the evolution of technology in the US. In the short term, it’s worth watching the CPPA, GDPR, California’s Props 22 and 24, and FCC interference with social media’s enforcement of rules around community behavior.  Long term, this is only the beginning.\n",
            "----------------\n",
            "in this paper, we semi-automatically learn error-inducing patterns from a corpus of common Java coding errors and from changes that caused operational anomalies at Facebook specifically. We combine the mutations with instrumentation that measures which tests exactly visited the mutated piece of code. Results on more than 15,000 generated mutants show that more than half of the generated mutants survive Facebook’s rigorous test suite of unit, integration, and system tests. is the companion tutorial for the paper “Algorithms for Causal Reasoning in Probability trees” by Genewein T. et al. (2020). Probability trees are one of the simplest models of causal generative processes.They possess clean semantics and are strictly more general than causal Bayesian networks, being able to e.g. represent causal relations that causal Bayesian networks can’t. Even so, they have received little attention from the AI and ML community. In this tutorial we present new algorithms for causal reasoning in discrete probability trees that cover the entire causal hierarchy (association, intervention, and counterfactuals), operating on arbitrary logical and causal events. a method in occupational safety for avoiding mistakes by pointing at important indicators and calling out the status. reduce the imperative shell and move code into the functional core\n",
            "----------------\n",
            "In this paper, we investigate “split-second phantom attacks,” a scientific gap that causes two commercial advanced driver-assistance systems (ADASs), Telsa Model X (HW 2.5 and HW 3) and Mobileye 630, to treat a depthless object that appears for a few milliseconds as a real obstacle/object. 1. Prefer running your system in the cloud over local emulation. 2. CI/CD Pipelines are not enough, local deployment automation is crucial. 3. For AWS Serverless & “Function-as-a-Service” – monolithic functions are OK. 4. Consider adopting a monorepo, with tooling appropriate for monorepos. 5. Implement all three pillars of observability. I reverse engineered mcdonald’s internal api and I’m currently placing an order worth $18,752 every minute at every mcdonald’s in the US to figure out which locations have a broken ice cream machine.\n",
            "----------------\n",
            "“On peut interroger n’importe qui, dans n’importe quel état; ce sont rarement les réponses qui apportent la vérité, mais l’enchaînement des questions.““You can interrogate anyone, no matter what their state of being.  It’s rarely their answers that unveil the truth, but the sequence of questions that you have to ask.“–  Inspector Pastor in La Fée Carabine, by Daniel Pennac The authors’ jobs all involve asking questions.  A lot of questions. We do so out of genuine curiosity as well as professional necessity: Q is an ML/AI consultant, Chris is a product manager in the AI space, and Shane is an attorney.  While we approach our questions from different angles because of our different roles,  we all have the same goal in mind: we want to elicit truth and get people working with us to dig deeper into an issue. Preferably before things get out of hand, but sometimes precisely because they have. A recent discussion led us down the path of our favorite questions: what they are, why they’re useful, and when they don’t work so well.  We then each chose our top three questions, which we’ve detailed in this article. We hope you’re able to borrow questions you haven’t used before, and even cook up new questions that are more closely related to your personal and professional interests. Before we get too far, let’s explore what we mean by a “good question.” For one, it’s broad and open-ended.  It’s a lot less “did this happen?” and more “what happened?”  It encourages people to share their thoughts and go deep. There’s an implied “tell me more” in an open-ended question.  Follow it with silence, and (as any professional interrogator will tell you) people will fill in extra details. They will get to what happened, along with when and how and why.  They will tell a full story, which may then lead to more questions, which branch into other stories. All of this fills in more pieces to the puzzle.  Sometimes, it sheds light on parts of the puzzle you didn’t know existed. By comparison, yes/no questions implicitly demand nothing more than what was expressly asked.  That makes them too easy to dodge. Two, a good question challenges the person asking it as much as (if not more than) the person who is expected to answer.  Anyone can toss out questions at random, in an attempt to fill the silence. To pose useful questions requires that you first understand the present situation, know where you want to wind up, and map out stepping-stones between the two. Case in point: the Daniel Pennac line that opened this piece was uttered by a detective who was “interviewing” a person in a coma.  As he inspected their wounds, he asked more questions to  explore their backstory, and that helped him to piece together his next steps of the investigation.  Perhaps Inspector Pennac was inspired by Georg Cantor, who once said: “To ask the right question is harder than to answer it.” Three, a good question doesn’t always have a right answer.  Some of them don’t have any answer at all.  And that’s fine. Sometimes the goal of asking a question is to break the ice on a topic, opening a discussion that paints a larger picture. Four, sometimes a question is effective precisely because it comes from an unexpected place or person. While writing this piece, one author pointed out (spoiler alert) that the attorney asked all of the technical questions, which seems odd, until you realize that he’s had to ask those because other people did not. When questions seem to come out of nowhere—but they are really born of experience—they can shake people out of the fog of status quo and open their eyes to new thoughts. The opinions presented here are personal, do not reflect the view of our employers, and are not professional product, consulting, or legal advice. Source: Q The backstory: This is the kind of question you sometimes have to ask three times. The first time, someone will try to hand you the company’s mission statement or slogan. The second time, they’ll provide a description of the company: industry vertical, size, and revenue. So you ask again, this time with an emphasis on the really. And then you wait for the question to sink in, and for the person to work backwards from all of the company’s disparate activities to see what it’s all truly for. Which will be somewhere between the raison d’etre and the sine qua non. Taking the time to work this out is like building a mathematical model: if you understand what a company truly does, you don’t just get a better understanding of the present, but you can also predict the future. It guides decisions such as what projects to implement, what competitors to buy, and whom to hire into certain roles. As a concrete example, take Amazon. Everyone thinks it’s a store. It has a store, but at its core, Amazon is a delivery/logistics powerhouse.  Everything they do has to end with your purchases winding up in your hot little hands. Nothing else they do matters—not the slick website, not the voice-activated ordering, not the recommendation engine—unless they get delivery and logistics down. How I use it: I explore this early in a consulting relationship. Sometimes even early in the sales cycle. And I don’t try to hide it; I’ll ask it, flat-out, and wait for people to fill the silence. Why it’s useful: My work focuses on helping companies to start, restart, and assess their ML/AI efforts. Understanding the company’s true purpose unlocks the business model and sheds light on what is useful to do with the data. As a bonus, it can also highlight cases of conflict. Because sometimes key figures have very different ideas of what the company is and what it should do next. When it doesn’t work so well: This question can catch people off-guard.  Since I work in the AI space, people sometimes have a preconceived notion that I’ll only talk about data and models.  Hearing this question from an ostensibly technical person can be jarring… though, sometimes, that can actually help the conversation along.  So it’s definitely a double-edged sword. Source: Chris The backstory: Ideation is about coming up with the “best” ideas. What is the best way to solve this problem? What is the most important? What is best for the business? The problem with “best” is that it is tied up with all of the biases and assumptions someone already has. To get to what really matters we have to understand the edge of what is good or bad. The gray area between those tells you the shape of the problem. Half the time this question will give you real, bad ideas.  What has been surprising to me is that the other half of the time, the so-called “bad” idea is really a “good” idea in disguise.  You just have to relax certain assumptions. Often these assumptions were just set at some point without a reason or much to back it up. How I use it: I like to ask this after going through a lot of the “best” questions in an ideation session. It can be adapted to focus on different types of “bad,” like “stupid,” “wasteful,” and “unethical.”  Ask follow up questions about why they believe the idea is “bad” and why it might actually be “good.” Why it’s useful: How can you truly know what is good without also knowing what is bad? When it doesn’t work so well: When I was a design consultant working for clients in highly regulated industries (.e.g banking, insurance, etc.), I found this can be a difficult question to ask. In those cases you will need to get your legal team to either grant the attorney/client privilege to ask the questions, or ask the prompt/response in such a way that it protects people in the conversation. Source: Shane The backstory: In the early days of ML training data, companies and research teams frequently used “some stuff we found on the Internet” as a source for training data. This approach has two problems: (1) there may not be an appropriate license attached to the data, and (2) the data may not be a good representative sample for the intended use. It’s worth noting that the first issue is not just limited to images collected from the Internet. In recent years a number of research datasets (including Stanford’s Brainwash, Microsoft’s MS Celeb, and Duke’s MTMC) were withdrawn for reasons including a lack of clarity around the permission and rights granted by people appearing in the datasets. More recently, at least one company has earned itself significant PR and legal controversy for collecting training data sources from social media platforms under circumstances that were at least arguably a violation of both the platform’s terms of service and platform users’ legal rights.  The safest course of action is also the slowest and most expensive: obtain your training data as part of a collection strategy that includes efforts to obtain the correct representative sample under an explicit license for use as training data. The next best approach is to use existing data collected under broad licensing rights that include use as training data even if that use was not the explicit purpose of the collection. How I use it: I like to ask this as early as possible.  You don’t want to invest your time, effort, and money building models only to later realize that you can’t use them, or that using them will be much more expensive than anticipated because of unexpected licenses or royalty payments. It’s also a good indirect measure of training data quality: a team that does not know where their data originated is likely to not know other important details about the data as well. Why it’s useful: No matter how the data is collected, a review by legal counsel before starting a project—and allow me to emphasise the word before—can prevent significant downstream headaches. When it doesn’t work so well:  This question is most useful when asked before the model goes into production. It loses value once the model is on sale or in service, particularly if it is embedded in a hardware device that can’t be easily updated. Source: Shane The backstory: One of the most interesting aspects of machine learning (ML) is its very broad applicability across a variety of industries and use cases. ML can be used to identify cats in photos as well as to guide autonomous vehicles. Understandably, the potential harm caused by showing a customer a dog when they expected to see a cat is significantly different from the potential harm caused by an autonomous driving model failing to properly recognize a stop sign.  Determining the risk profile of a given model requires a case-by-case evaluation but it can be useful to think of the failure risk in three broad categories: How I use it: I use this question to determine both the potential risk from an individual failure and the potential aggregate risk from a systemic failure.  It also feeds back into my question about training data: some relatively minor potential harms are worth additional investment in training data and testing if they could inconvenience millions, or billions, of users or create a significant negative PR cycle for a company. Why it’s useful: This is the sort of question that gets people thinking about the importance of their model in the overall business. It can also be a helpful guide that companies invest in such a model, and the kinds of business processes that are amenable to models.  Remember that models that work nearly perfectly can still fail spectacularly in unusual situations. When it doesn’t work so well: We don’t always have the luxury of time or accurate foresight. Sometimes a business does not know how a model will be used: a model is developed for Product X and repurposed for Product Y, a minor beta feature suddenly becomes an overnight success, or a business necessity unexpectedly forces a model into widespread production. Source: Q The backstory: A consultant is an agent of change. When a prospect contacts me to discuss a project, I find it helpful to compare the cost of the desired change to the cost of another-change or even to the cost of the not-change. “What happens if you don’t do this? What costs do you incur, what exposures do take on now? And six months from now?” A high cost of doing nothing means that this is an urgent matter. Some consultants will tell you that a high cost of doing nothing is universally great (it means the prospect is ready to move) and a low cost is universally bad (the prospect isn’t really interested).  I see it differently: we can use that cost of doing nothing as a guide to how we define the project’s timeline, fee structure, and approach. If the change is extremely urgent—a very high cost of doing nothing—it may warrant a quick fix now, soon followed by a more formal approach once the system is stable. A low cost of doing nothing, by comparison, means that we can define the project as “research” or “an experiment,” and move at a slower pace. How I use it: I will ask this one, flat-out, once a consulting prospect has outlined what they want to do. Why it’s useful: Besides helping to shape the structure of the project, understanding the cost of doing nothing can also shed light on the prospect’s motivations. That, in turn, can unlock additional information that can be relevant to the project. (For example, maybe the services I provide will help them reach the desired change, but that change won’t really help the company. Perhaps I can refer them to someone else in that case.) When it doesn’t work so well: Sometimes people don’t have a good handle on the risks and challenges they (don’t) face. They may hastily answer that this is an urgent matter when it’s not; or they may try to convince you that everything is fine when you can clearly see that the proverbial house is on fire. When you detect that their words and the situation don’t align, you can ask them to shed light on their longer-term plans. That may help them to see the situation more clearly. Source: Chris The backstory: This is something that was inspired from the intersection of an incredibly boring decision-science book and roadmap planning. Decision trees and roadmaps are very useful when building out the possible spaces of the future. However, for both decision trees and roadmaps we are usually overly optimistic in how we will proceed.  We fail at properly considering failure.  To appropriately plan for the future we must consider the different ways we can be wrong. Sometimes it will be at a certain decision point (“we didn’t get enough signups to move forward”) or an event trigger (“we see too many complaints”).  If we consider this wrong-ness and the possible next step, we can start to normalize this failure and make better decisions. How I use it:  It’s best to ask this when you find that certainty is at a high point for the project. More often than not, people don’t consider ways to detect that they need to change course. Why it’s useful: You build a map into the future based on what you can detect. This helps make hard decisions easier because you are effectively practicing the decision process before you are in the heat of the moment. When it doesn’t work so well: When things are currently going “wrong” it can be a sensitive subject for people. I’ve found it is easier to talk about how to get out of a current wrong situation than considering additional future situations. Source: Shane The backstory: Imagine you employ a vendor to provide or enrich your training data, or you pay for consulting services related to ML. What happens to the information used by the vendors to build your product?  Their downstream rights there run the gamut from “absolutely nothing” to “retaining a full copy of the training data, labels, trained models, and test results.” The median position, in my observation, tends to be that the vendor retains control of any new techniques and information derived from the work that would be useful in general, such as new methods of programmatically applying error correction to a trained model, but not the specific data used to train the model or the resulting trained model. From the customer perspective, downstream rights are tied to competition/cost tradeoffs and the rights associated with training data.  A company that considers ML a competitive advantage likely will not want their models or derivative data available to competitors, and they must balance this against the business consideration that vendors which retain downstream rights typically charge lower fees (because reselling that data or models can be a source of revenue). In addition, training data usually comes with contractual limitations and customers of ML services need to ensure they are not granting downstream rights that they don’t have in their upstream agreements. Finally, some kinds of training data, such as medical records or classified government data, may forbid unauthorized access or use in systems that lack adequate safeguards and audit logs. How I use it: This question is less relevant to companies that have an entirely in-house workflow (they generate their own training data, train their own models, and use models with their own employees and tools).  It is highly relevant to companies that buy or sell ML services, use external vendors for part of their workflow, or handle sensitive data. Why it’s useful:  The notion of downstream rights is not a new question, nor is it specific to the ML world.  Almost all vendor relationships involve delineating the intellectual property (IP) and tools that each party brings to the project, as well as the ownership of new IP developed during the project. Helping founders to recognize and establish those boundaries early on can save them a lot of trouble later. When it doesn’t work so well: This is a question a company definitely wants to answer before they’ve provided data or services to a counterparty.  These issues can be very difficult to resolve once data has been shared or work has begun. Source: Q The backstory: A risk is a potential change that comes with consequences.  To properly manage risk—to avoid those consequences—you need to identify those changes in advance (perform a risk assessment) and sort out what to do about them (devise your risk mitigation plans). That’s where this trio of questions comes in: “What if?” is the key to a risk assessment, as it opens the discussion on ways a project may deviate from its intended path.  “Then?” explores the consequences of that deviation. The “What next?” starts the discussion on how to handle them. “What if … our data vendor goes out of business? Then? Our business is hamstrung. What next? We’d better have a backup data vendor in the wings.  Or better yet, keep two vendors running concurrently so that we can switch over with minimal downtime.” “What if … something changes, and the model’s predictions are wrong most of the time? Then? We’re in serious trouble, because that model is used to automate purchases. What next? We should implement monitors around the model, so that we can note when it’s acting out of turn. We should also add a ‘big red button’ so that a person can quickly, easily, and completely shut it down if it starts to go haywire.” How I use it:  Once we’ve sorted out what the client wants to achieve, I’ll round out the picture by walking them through some “What if? Then? What next?” scenarios where things don’t work out. Why it’s useful: It’s too easy to pretend the not-intended outcomes don’t exist if you don’t bring them up. I want my clients to understand what they’re getting into, so they can make informed decisions on whether and how to proceed. Going through even a small-scale risk assessment like this can shed light on the possible downside loss that’s lurking alongside their desired path. All of that risk can weigh heavily on their investment, and possibly even wipe out any intended benefit. When it doesn’t work so well: The business world, especially Western business culture, has a strange relationship with positive attitudes. This energy can be infectious and it can help to motivate a team across the finish line. It can also convince people to pretend that the non-intended outcomes are too remote or otherwise not worth consideration. That’s usually when they find out, the hard way, what can really go wrong. How to handle this varies based on your role in the company, internal company politics, your ability to bring about change, and your ability to weather a storm. Source: Chris The backstory: The most important question is one that isn’t expected. It is one that leads to unexpected answers. We don’t have dialog for dialog sake; we do it to learn something new. Sometimes the thing we learn is that we aren’t aligned. I’ve found that the most unexpected thing is something that we wouldn’t choose based on our current thought process. Randomly choosing a question from a collection appropriate for your domain is really valuable. If you are building something for the web, what kinds of questions could you ask about a web project? This is helpful when the checklists of things to do get too large to try all of them. Pick a few at random. You can take it a step further and pick questions from outside of your domain. This can simply be a list of provocations that require a high amount of interpretation by you to make sense. This is because randomness doesn’t work without the lens of human intuition.  Randomness without this intuition is just garbage. We do the work to bridge from random questions to some new idea related to our problem. We build the analogies in our mind even when something is seemingly not connected at first. How I use it: When you find that you keep asking the same questions. I have decks of cards like Oblique Strategies for provocations, Triggers for domain-specific questions, and others that can provide randomness. Domain-specific random questions can also be very impactful. Eventually, I expect models like GPT-n to provide appropriate random questions to prompts. Why it’s useful: Even with all of the questions we ask to get out of bias, we are still biased. We still have assumptions we don’t realize. Randomness doesn’t care about your biases and assumptions. It will ask a question that you think on the surface is stupid, but when you think about it is important. When it doesn’t work so well: With teams that are high on certainty they may think of the random question as a toy or distraction. The people I’ve found to be incredibly confident in their world trivialize the need to question bias. They will even try to actively subvert the process sometimes. If you hide the fact that a question was randomly chosen, it can go over better. If you’re collecting facts—names, numbers, times—then narrow questions will suffice.  But if you’re looking to understand the bigger picture, if you want to get a meeting out of a rut, if you want people to reflect before they speak, then open-ended questions will serve you well.  Doubly so when they come from an unexpected source and at an unexpected time. The questions we’ve documented here have helped us in our roles as an AI consultant, a product manager, and an attorney. (We also found it interesting that we use a lot of the same questions, which tells us how widely applicable they are.) We hope you’re able to put our favorite questions to use in your work. Perhaps they will even inspire you to devise and test a few of your own. One point we hope we’ve driven home is that your goal in asking good questions isn’t to make yourself look smarter. Nor is it to get the answers you want to hear. Instead, your goal is to explore a problem space, shed light on new options, and mitigate risk. With that new, deeper understanding, you’re more prepared to work on the wicked problems that face us in the workplace and in the world at large.\n",
            "----------------\n",
            "Differential dataflow programs look like many standard “big data” computations, borrowing idioms from frameworks like MapReduce and SQL. However, once you write and run your program, you can change the data inputs to the computation, and differential dataflow will promptly show you the corresponding changes in its output. Promptly meaning in as little as milliseconds. In this work, we create a true Many-to-Many multilingual translation model that can translate directly between any pair of 100 languages. Our focus on non-English-Centric models brings gains of more than 10 BLEU when directly translating between non-English directions while performing competitively with the best single systems of WMT.\n",
            "----------------\n",
            "Automerge is designed for creating local-first software, i.e. software that treats a user’s local copy of their data (on their own device) as primary, rather than centralising data in a cloud service.  In this paper, we present HangFix, a software hang bug fixing framework which can automatically fix a hang bug that is triggered and detected in production cloud environments. HangFix first leverages stack trace analysis to localize the hang function and then performs root cause pattern matching to classify hang bugs into different types based on likely root causes. Next, HangFix generates effective code patches based on the identified root cause patterns. We have implemented a prototype of HangFix and evaluated the system on 42 real-world software hang bugs in 10 commonly used cloud server applications. Our results show that HangFix can successfully fix 40 out of 42 hang bugs in seconds.\n",
            "----------------\n",
            "Focusing on the data entry and storage aspects, this article offers practical recommendations for organizing spreadsheet data to reduce errors and ease later analyses. The basic principles are: be consistent, write dates like YYYY-MM-DD, do not leave any cells empty, put just one thing in a cell, organize the data as a single rectangle (with subjects as rows and variables as columns, and with a single header row), create a data dictionary, do not include calculations in the raw data files, do not use font color or highlighting as data, choose good names for things, make backups, use data validation to avoid data entry errors, and save the data in plain text files. To our knowledge, this is the first exploration of a practical general purpose real number type that both reflects the mathematical laws of the real numbers, and also supports exact comparisons in situations in which that’s normally expected. Here, we report a universal fabrication scheme to enable printing and room-temperature sintering of the metal nanoparticle on paper/fabric for FPCBs and directly on the human skin for on-body sensors with a novel sintering aid layer. Consisting of polyvinyl alcohol (PVA) paste and nanoadditives in the water, the sintering aid layer reduces the sintering temperature. Together with the significantly decreased surface roughness, it allows for the integration of a submicron-thick conductive pattern with enhanced electromechanical performance. Various on-body sensors integrated with an FPCB to detect health conditions illustrate a system-level example. A presentation of several novel ways to visualize 25 years of the Gartner Hype Cycle.\n",
            "----------------\n",
            "The field of AI product management continues to gain momentum. As the AI product management role advances in maturity, more and more information and advice has become available. Our previous articles in this series introduce our own take on AI product management, discuss the skills that AI product managers need, and detail how to bring an AI product to market. One area that has received less attention is the role of an AI product manager after the product is deployed. In traditional software engineering, precedent has been established for the transition of responsibility from development teams to maintenance, user operations, and site reliability teams. New features in an existing product often follow a similar progression. For traditional software, the domain knowledge and skills required to develop new features differ from those necessary to ensure that the product works as intended. Because product development and product operations are distinct, it’s logical for different teams and processes to be responsible for them. In contrast, many production AI systems rely on feedback loops that require the same technical skills used during initial development. Similarly, in “Building Machine Learning Powered Applications: Going from Idea to Product,” Emmanuel Ameisen states: “Indeed, exposing a model to users in production comes with a set of challenges that mirrors the ones that come with debugging a model.” As a result, at the stage when product managers for other types of products might shift to developing new features (or to other projects altogether), an AI product manager and the rest of the original development team should remain heavily involved. One reason for this is to tackle the (likely) lengthy backlog of ML/AI model improvements that will be discovered after the product engages with the real world. Another, of course, is to ensure that the product functions as expected and desired over time. We describe the final responsibility of the AI PM as coordinating with the engineering, infrastructure, and site reliability teams to ensure all shipped features can be supported at scale. This article offers our perspective into the practical details of the AI PM’s responsibilities in the latter parts of the AI product cycle, as well as some insight into best practices in execution of those responsibilities. In Bringing an AI Product to Market, we distinguished the debugging phase of product development from pre-deployment evaluation and testing. This distinction assumes a slightly different definition of debugging than is often used in software development. We define debugging as the process of using logging and monitoring tools to detect and resolve the inevitable problems that show up in a production environment. Emmanuel Ameisen again offers a useful framework for defining errors in AI/ML applications: “…three areas in particular are most important to verify: inputs to a pipeline, the confidence of a model and the outputs it produces.” To support verification in these areas, a product manager must first ensure that the AI system is capable of reporting back to the product team about its performance and usefulness over time.  This may manifest in several ways, including the collection of explicit user feedback or comments via channels outside of the product team, and the provision of mechanisms to dispute the output of the AI system where applicable. Proper AI product monitoring is essential to this outcome. From a technical perspective, it is entirely possible for ML systems to function on wildly different data. For example, you can ask an ML model to make an inference on data taken from a distribution very different from what it was trained on—but that, of course, results in unpredictable and often undesired performance. Therefore, deployed AI products should include validation steps to ensure that model inputs and outputs are within generally expected limits, before a model training or inference task is accepted as successful. Ideally, AI PMs would steer development teams to incorporate I/O validation into the initial build of the production system, along with the instrumentation needed to monitor model accuracy and other technical performance metrics. But in practice, it is common for model I/O validation steps to be added later, when scaling an AI product. Therefore, the PM should consider the team that will reconvene whenever it is necessary to build out or modify product features that: The composition of these teams will vary between companies and products, but a typical cross-functional team would likely include representatives from Data Science (for product-level experimentation and inference task validation), Applied Science (for model performance and evaluation), ML Engineering (for data and feature engineering, as well as model pipeline support) and Software/Feature Engineering (for integration with the full stack of the AI product—such as UI/UX, cloud services, and dev ops tools). Working together, this post-production development team should embrace continuous delivery principles, and prioritize the integration of any additional necessary instrumentation that was not already implemented during the model development process. Finally, the AI PM must work with production engineering teams to design and implement the alerting and remediation framework. Considerations include where to set thresholds for each persona, alert frequency, and the degree of remediation automation (both what’s possible and desired). During testing and evaluation, application performance is important, but not critical to success. In the production environment, when the outputs of an ML model are often a central (yet hidden) component of a greater application, speed and reliability are critically important. It is entirely possible for an AI product’s output to be absolutely correct from the perspective of accuracy and data quality, but too slow to be even remotely useful. Consider the case of autonomous vehicles: if the outputs from even one of the many critical ML models that comprise the vehicle’s AI-powered “vision” are delivered after a crash, who cares if they were correct? In engineering for production, AI PMs must take into account the speed at which information from ML/AI models must be delivered (to validation tasks, to other systems in the product, and to users). Technologies and techniques—such as engineering specifically for GPU/TPU performance and caching—are important tools in the deployment process, but they are also additional components that can fail, and thus be responsible for the failure of an AI product’s core functionality. An AI PM’s responsibility is to ensure that the development team implements proper checks prior to release, and—in the case of failure—to support the incident response teams, until they are proficient in resolving issues independently. AI product managers must also consider availability: the degree to which the service that an AI product provides is available to other systems and users. Service Level Objectives (SLOs) provide a useful framework for encapsulating this kind of decision. In an incident management blog post, Atlassian defines SLOs as: “the individual promises you’re making to that customer… SLOs are what set customer expectations and tell IT and DevOps teams what goals they need to hit and measure themselves against. SLOs can be useful for both paid and unpaid accounts, as well as internal and external customers.” Service Level Indicators, Objectives, and Agreements (SLIs, SLOs, and SLAs) are well-known, frequently used, and well-documented tools for defining the availability of digital services.  For cloud infrastructure some of the most common SLO types are concerned with availability, reliability and scalability. For AI products, these same concepts must be expanded to cover not just infrastructure, but also data and the system’s overall performance at a given task. While useful, these constructs are not beyond criticism. Chief among the challenges are: choosing the correct metrics to begin with, measuring and reporting once metrics are selected, and the lack of incentive for a service provider to update the service’s capabilities (which leads to outdated expectations). Despite these concerns, service level frameworks can be quite useful, and should be in the AI PM’s toolkit when designing the kind of experience that an AI product should provide. You must also take durability into account when building a post-production product plan. Even if well-designed, multi-layer fault detection and model retraining systems are carefully planned and implemented, every AI-powered system must be robust to the ever-changing and naturally stochastic environment that we (humans) all live in. Product managers should assume that any probabilistic component of an AI product will break at some point. A good AI product will be able to self-detect and alert experts upon such a failure; a great AI product will be able to detect the most common problems and adjust itself automatically—without significant interruption of services for users, or high-touch intervention by human experts. There are many ways to improve AI product durability, including: It’s worth noting that model durability and retraining can raise legal and policy issues. For example, in many regulated industries, changing any core functionality of an AI system’s decision-making capability (i.e., objective functions, major changes to hyperparameters, etc.) require not only disclosure, but also monitored testing.  As such, an AI Product Manager’s responsibility here extends to releasing not only a usable product, but one that can be ethically and legally consumed. It’s also important to remember that no matter what the approach to developing and maintaining a highly durable AI system, the product team must have access to high quality, relevant metrics on both model performance and functionality. Proper monitoring (and the software instrumentation necessary to perform it) is essential to the success of an AI product. However, monitoring is a loaded term. The reasons for monitoring AI systems are often conflated, as are the different types of monitoring and alerting provided by off-the-shelf tools. Emmanuel Ameisen once again provides a useful and concise definition of model monitoring as a way to “track the health of a system. For models, this means monitoring their performance and the equity of their predictions.” The simplest case of model monitoring is to compute key performance metrics (related to both model fit and inference accuracy) regularly. These metrics can be combined with human-determined thresholds and automated alerting systems to inform when a model has “drifted” beyond normal operating parameters. While ML monitoring is a relatively new product area, standalone commercial products (including Fiddler and superwise.ai) are available, and monitoring tools are incorporated into all the major machine learning platforms. Separate from monitoring for model freshness, Ameisen also mentions the need to apply technical domain experience in designing monitoring systems that detect fraud, abuse, and attack from external actors. AI PMs should consult with Trust & Safety and Security teams to combine the best principles and technical solutions with existing AI product functionality. In some specific domains—such as financial services or medicine—no easy technical solutions exist. In this case, it is the responsibility of the AI product team to build tools to detect and mitigate fraud and abuse in the system. As we’ve mentioned previously, it’s not enough to simply monitor an AI system’s performance characteristics. It is even more important to consistently ensure that the AI product’s user-facing and business purposes are being fulfilled. This responsibility is shared by the development team with Design, UX Research, SRE, Legal, PR, and Customer Support teams. The AI PM’s responsibility is again to orchestrate reasonable and easily repeatable mitigations to any problems. It is crucial to design and implement specific alerting capabilities for these functions and teams. If you simply wait for complaints, they will arise far too late in the cycle for your team to react properly. No matter how well you research, design, and test an AI system, once it is released, people are going to complain about it. Some of those complaints will likely have merit, and responsible stewardship of AI products requires that users are given the ability to disagree with the system’s outputs and escalate issues to the product team. It is also entirely possible for this feedback to show you that the system is underserving a particular segment of the population, and that you may need a portfolio of models to serve more of the user base. As an AI PM, you have the responsibility to build a safe product for everyone in the population who might use it. This includes consideration of the complexities that come into play with intersectionality. For example, an AI product might produce great outcomes for wealthy, American, cisgender, heterosexual, White women—and although it might be tempting to assume those outcomes would apply to all women, such an assumption would be incorrect. Returning to previous anti-bias and AI transparency tools such as Model Cards for Model Reporting (Timnit Gebru, et al.) is a great option at this point. It is important not to pass this development task off to researchers or engineers alone; it is an integral part of the AI product cycle. If done right, users will never be aware of all the product monitoring and alerting that is in place, but don’t let that trick you. It’s essential to success. One question that an AI PM might ask when pondering these post-production requirements is: “This seems hard; can’t I just buy these capabilities from someone else?” This is a fair question, but—as with all things related to machine learning and artificial intelligence—the answer is far from a binary yes or no. There are many tools available to help with this process, from traditional vendors and bleeding edge startups alike. Deciding what investment to make in MLOps tooling is an inherently complex task. However, careful consideration and proactive actions often lead to defendable competitive advantages over time. Uber (the developer of Michelangelo), Airbnb (developer of zipline), and Google have all taken advantage of superior tooling and operations skills to build market-leading AI products. Nearly every ML/AI library touts full end-to-end capabilities, from enterprise-ready stacks (such as H20.ai, MLFlow, and Kubeflow) to the highly specialized and engineer-friendly (such as Seldon.io) and everything in-between (like Dask). Enterprise level-frameworks often provide deep and well-supported integration with many common production systems; smaller companies might find this integration unnecessary or overly cumbersome. Regardless, it’s a safe bet that getting these off-the-shelf tools to work with your AI product in the exact ways you need them to will be costly (if not financially, then at least in time and human labor). That said—from a scale, security and features perspective—such capabilities may be required in many mature AI product environments. On the other hand, building and scaling a software tool stack from scratch requires a significant sustained investment in both developer time and technology. Facebook, Uber, AirBnB, Google, Netflix, and other behemoths have all spent millions of dollars to build their ML development platforms; they also employ dozens to hundreds of employees, each tasked with building and scaling their internal capabilities. The upside here is that such end-to-end development to deployment frameworks and tools eventually become a competitive advantage, in and of themselves. However, it’s worth noting that in such environments, employing a single AI PM is not feasible. Instead, a cadre of PMs focused on different components of the AI product value chain are needed. Building great AI products is a significant, cross-disciplinary, and time-consuming undertaking, even for the most mature and well-resourced companies. However, what ML and AI can accomplish at scale can be well worth the investment.  Although a return on investment is never guaranteed, our goal is to provide AI PMs with the tools and techniques needed to build highly engaging and impactful AI products in a wide variety of contexts. In this article, we focused on the importance of collaboration between product and engineering teams, to ensure that your product not only functions as intended, but is also robust to both the degradation of its effectiveness and the uncertainties of its operating environment. In the world of machine learning and artificial intelligence, a product release is just the beginning. Product managers have a unique place in the development ecosystem of ML/AI products, because they cannot simply guide the product to release and then turn it over to IT, SRE, or other post-production teams. AI product managers have a responsibility to oversee not only the design and build of the system’s capabilities, but also to coordinate the team during incidents, until the development team has completed enough knowledge transfer for independent post-production operation. The evolution of AI-enabled product experiences is accelerating at breakneck speed. In parallel, the emerging role of AI product management continues to evolve at a similar pace, to ensure that the tools and products delivered to the market provide true utility and value to both customers and businesses. Our goal in this four-part series on AI product management is to increase community awareness and empower individuals and teams to improve their skill sets in order to effectively steer AI product development toward successful outcomes. The best ML/AI products that exist today were brought to market by teams of PhD ML/AI scientists and developers who worked in tandem with resourceful and skilled product teams.  All were essential to their success. As the field of AI continues to mature, so will the exciting field of AI product management. We can’t wait to see what you build!   We would like to thank the many people who have  contributed their expertise to the early drafts of the articles in this series, including: Emmanuel Ameisen, Chris Albon, Chris Butler, Ashton Chevalier, Hilary Mason, Monica Rogati, Danielle Thorp, and Matthew Wise.\n",
            "----------------\n",
            "CG/SQL is a code generation system for the popular SQLite library that allows developers to write stored procedures in a variant of Transact-SQL (T-SQL) and compile them into C code that uses SQLite’s C API to do the coded operations. CG/SQL enables engineers to create highly complex stored procedures with very large queries, without the manual code checking that existing methods require. (1) Social media is addictive, and we are powerless to resist it. The concept of addiction does not encompass the full range of pleasures, risks, and uses that people create with technology. (2) Technology companies can fix the problems they create with better technology. Some technology cannot be fixed by more design, and some technology should not be built at all. (3) Growth and engagement metrics are the best drivers of decision making at tech companies. Many of the most important parts of digital well-being cannot be captured by quantitative metrics. (4) Our health and well-being depend on spending less time with screens and social media platforms. Health and well-being cannot be reduced to the single variable of screen time. a web-based multi-emulator (RetroArch/libretro) designed to recreate the experience of playing console games with a single controller in a room full of friends.\n",
            "----------------\n",
            "The release of GPT-3 has reinvigorated a discussion of creativity and artificial intelligence. That’s a good discussion to have, primarily because it forces us to think carefully about what we mean when we use words like “creativity” and “art.” As I’ve argued in the past, each time we have this discussion, we end up raising the bar. Each time an AI system does something that looks “intelligent” or creative, we end up deciding that’s not what intelligence really is.  And that’s a good thing. AI is likely to teach us more about what intelligence and creativity are not than about what they are. I’m not terribly interested in whether AI can imitate human creativity. “Can an AI create a ‘new’ poem that reads as if it were written by Keats, or a new piano sonata that sounds like Beethoven” isn’t a question that’s worth asking. Of course it can—if not now, it will be able to in the near future. We really don’t need a new Beethoven sonata; the 32 he wrote are enough. Nor do we need a new Keats ode, limited though his output was. Or a new Rembrandt. Imitation is ultimately a party trick: clever and amusing, but not really important. Sure, if you want texts for greeting cards or elevator music (or maybe even commercial pop), algorithms may do the trick. What’s really important is the transition between different forms of creativity. How do you get something that’s qualitatively new, and not just imitation? Creativity isn’t about the artifacts as much as it’s about the transitions. How do you get from Bach to Haydn? How do you get from Haydn to Beethoven? And, even within the career of a single artist: how do you get from the beginning to the end? How do you get from Beethoven’s first piano sonata, which sounds like Haydn, to the last, which at some points anticipates jazz? Artists aren’t stagnant. But I have no idea how even to ask whether an AI system can “mature” or “grow” in its output. Great artists (and yes, I’m presuming a lot with the word “great”) frequently work by defining themselves against what came before. This is particularly clear with artists of the Romantic period in Germany, England, and France. The term Romanticism didn’t come around until some years later, but they left behind several manifestos describing what they were trying to do, and how it was different from what came before. That is still how artists work: Nnedi Okorafor’s Africanfuturism is important as a way of defining and directing her own work. The importance of these defining statements isn’t so much about “rightness” (in the sense of “this is what art is or should be”) but in setting a direction for their project. Can a machine do that? Can it decide how its work will be different from what came before? It’s not clear to me that it can’t, but that’s a significant step beyond any machine learning projects we currently have. Although artists work by projecting their work into the future, by defining something “new,” their work is also derived from what came before—possibly as misinterpretation, but almost always as revision. Listen to the Beatles, and you hear something that was really built on the backbone of blues, filtered through some British pop-cultural trends. At the end of the “His Dark Materials” trilogy, Phillip Pullman thanks all the authors he stole from. Or, as T. S. Eliot said, “Immature poets imitate; mature poets steal; bad poets deface what they take, and good poets make it into something better, or at least something different.” Artists break from the past by reinterpreting that past.\n",
            " For AI-generated art–where does that sense of “different” come from? Can artificial intelligence learn to steal and reinterpret? Where does that engagement with history, current events and even selfhood come from?  Without that, there’s no basis for reinterpretation aside from random perturbation. It’s not clear that a sense of history couldn’t come from a big model trained on a gigantic corpus (although the best we can do now is build models that have no idea what they are saying). What kind of model would take that corpus and make something that was different, something that hadn’t been seen before? Would we care if Hamlet wasn’t written by Shakespeare in a specific historical context, but in 2025 by a computer trained on an archive of Elizabethan politics, drama, and history? (Never mind that an archive of Elizabethan drama would be very skimpy; most plays from that period were never published.) Would anyone care about the opera Nixon in China if it didn’t reflect a composer’s, and a librettist’s, thinking about historical events?\n",
            " There’s another way in which I find computed-generated artworks unsatisfactory, particularly in music. AI-generated music is often interesting over the short term, but fails at larger-scale structure. Much of music history, from four-bar blues to Beethoven’s massive experiments in sonata form, is about building structures that can be interesting over the long term, whether “long term” means a few minutes of a blues song to several hours of opera. That was Beethoven’s project; more recently, it was the project of rock groups like Pink Floyd. It seems conceivable that a model could learn to generate such longer-form structures, but I’ve yet to see one that does it satisfactorily. Is this where collaboration between humans and machines comes in, and if so, what does that say about creativity? A machine could conceivably do the pattern-matching and combinatorics, assembling a creative work out of news clippings and stylistic mimicry. But a human still needs to supply the sense of history that makes the work something we care about. A human still needs to provide the structure that makes art works more than brief curiosities. Is it possible for a human could tweak a model like GPT-3 to give it that sense of direction and context? What kinds of user interfaces would facilitate this kind of interaction? I can’t answer those questions, but that sounds like a much more interesting form of digital collaboration than having an algorithm write hundreds of dull poems or songs and “curating” a few that aren’t boring. That’s just a recipe for greeting-card sentiment and elevator music. I mean no disrespect to Hallmark—mass-produced poetry for greeting cards serves a purpose—but when we think about what kinds of creativity we want, and how that creativity will be mediated by AI, we should demand more.\n",
            "----------------\n",
            " To analyze the possible consequences, we study experimentally the behavior of algorithms powered by Artificial Intelligence (Q-learning) in a workhorse oligopoly model of repeated price competition. We find that the algorithms consistently learn to charge supracompetitive prices, without communicating with one another. The high prices are sustained by collusive strategies with a finite phase of punishment followed by a gradual return to cooperation. This finding is robust to asymmetries in cost or demand, changes in the number of players, and various forms of uncertainty. RAND researchers developed Hedgemony, a wargame designed to teach U.S. defense professionals how different strategies could affect key planning factors in the trade space at the intersection of force development, force management, force posture, and force employment. The game presents players, representing the United States and its key strategic partners and competitors, with a global situation, competing national incentives, constraints, and objectives; a set of military forces with defined capacities and capabilities; and a pool of periodically renewable resources. The players are asked to outline their strategies and are then challenged to make difficult choices by managing the allocation of resources and forces in alignment with their strategies to accomplish their objectives within resource and time constraints. Run Android applications on any GNU/Linux operating system.\n",
            "----------------\n",
            "On its own, using a simple DC voltage as the input, the device outputs not just simple spikes, as some other devices can manage, but the whole array of neural activity—bursts of spikes, self-sustained oscillations, and other stuff that goes on in your brain connected communities; the Truth Sandwich; pre-bunking; distributed debunking; localize the context; humor over rumor.\n",
            "----------------\n",
            "This month, the big surprise is that there’s no significant technology news about COVID. And there is more news than ever about legislation and regulation. I suspect that the legal system will be a big driver for technology over the next year. Another trend that doesn’t quite count as technology news but that definitely bears watching is that college enrollment in the US is down. Grad schools are up, 4 year colleges are down slightly; the big hit is in 2 year colleges. COVID is probably the biggest contributing factor, but regardless of the cause, this is an inauspicious trend.\n",
            "----------------\n",
            " So he then examined the mechanism the coffee maker used to receive firmware updates. It turned out they were received from the phone with—you guessed it—no encryption, no authentication, and no code signing. Big machines are sometimes more efficient. But they cost more, so fewer can be produced with a finite budget. Small machines are cheaper and may benefit from improvement over time, driven by experience in building more units. When does this experience lead to greater overall efficiency? We derive an approximation which, given a learning rate, tells how much smaller a machine must be to overcome an initial efficiency disadvantage. The first work that specifically addressed the detection of automated accounts in online social networks dates back to January 2010.\n",
            "----------------\n",
            "A program P is incremental if repeating P with a changed input is faster than from-scratch computation. Adapton offers programming language abstractions for incremental computation. Keep your migration scripts away from your production code; Keep it low-tech, don’t deserialize; Write tests to exercise each migration script individually; Consider running long migrations online; Consider versioning your documents. However, we do not look at image formats from a general point of view, but rather think of ways to glitch them. When we look at PNG from the point of view of glitch, what kind of peculiarity does it have?\n",
            "----------------\n",
            "This class examines ethical frameworks, modern ethical concerns related to computer science and technology, and clear oral and written communication. Topics we will explore include policy vacuums created by new technology, copyright and patent, software bugs and liability, freedom of speech, privacy, security, employment and job markets, warfare and state-building, wealth discrepancy and consumerism, environmental impact, and changing cultural norms and social contracts. What follows is part position paper and part ahistorical review. This essay introduces the term hardware lottery to describe when a research idea wins because it is compatible with available software and hardware and not because the idea is superior to alternative research directions. We argue that choices about software and hardware have often played a decisive role in deciding the winners and losers in early computer science history. a Bitsy game about learning to rely on others and fighting against hopelessness, together. If you are writing down “rules” and insisting that developers abide by them, it’s probably because your developers are continuously doing things you wish they wouldn’t. Usually, this isn’t because your developers don’t understand “the rules” and/or don’t like you—it’s because they know what the organization values, and those values are in conflict with your “rules,” and they’re trying to deliver that value.\n",
            "----------------\n",
            "a higher level taxonomy that I use to think about concurrent performance. We’ll group the performance of concurrent operations into six broad levels running from fast to slow, with each level differing from its neighbors by roughly an order of magnitude in performance. without more geographic representation, they’ll produce a global vision for AI ethics that reflects the perspectives of people in only a few regions of the world, particularly North America and northwestern Europe. […] This lack of regional diversity reflects the current concentration of AI research (pdf): 86% of papers published at AI conferences in 2018 were attributed to authors in East Asia, North America, or Europe. And fewer than 10% of references listed in AI papers published in these regions are to papers from another region. Patents are also highly concentrated: 51% of AI patents published in 2018 were attributed to North America. As a result, the local model is only useful for queries with a very strong “signal.” Apple’s system, for example, uses the local model to estimate the popularity of emojis, but the results are only useful for the most popular emojis (i.e. where the “signal” is strongest). The local model is typically not used for more complex queries, like those used in the U.S. Census [3] or applications like machine learning.\n",
            "----------------\n",
            "AI is a field where value, in the form of outcomes and their resulting benefits, is created by machines exhibiting the ability to learn and “understand,” and to use the knowledge learned to carry out tasks or achieve goals. AI-generated benefits can be realized by defining and achieving appropriate goals. These goals depend on who the stakeholder is; in other words, the person or company receiving the benefits. There are three potential stakeholders for AI applications, with a single application often involving all three. They are business stakeholders, customers, and users. Each type of stakeholder has different and unique goals; each group is most interested in having their specific objectives met, or problems solved. My book, AI for People and Business, introduces a framework that highlights the fact that both people and businesses can benefit from AI in unique and different ways. A typical social media platform needs to satisfy all three stakeholders. In the case of Twitter, the business stakeholder’s top goals are likely centered around profits and revenue growth. Customer stakeholders are the people and companies that advertise on the platform, and are most concerned with ROI on their ad spend. User stakeholders are interested in benefiting from the platform’s functionality: staying up-to-date, quickly finding new people and topics to follow, and engaging with family and friends. Goals should be defined specifically and at a granular level for each stakeholder and relevant use case. Twitter has no doubt went through this exercise long ago; but if we imagine Twitter taking its first steps towards AI, some specific and granular goals could be to build a recommendation engine that helps users find the most relevant people to follow (a goal for users), while also building an AI-powered advertising targeting engine that best matches ads with those most likely to be interested in the product or service being advertised (for customers). This in turn would increase the platform’s value for users and thus increase engagement, which would result in more eyes to see and interact with ads, which would mean better ROI on ad spend for customers, which would then achieve the goal of increased revenue and customer retention (for business stakeholders). The key is to start with small and easily identifiable AI projects that will trickle value upwards towards a company’s highest priority goals. For companies early in their AI journey, setting appropriate goals helps create a foundation from which to build AI maturity. It also helps companies learn how to translate existing AI capabilities into solving specific real-world problems and use cases. In my book, I introduce the Technical Maturity Model: I define technical maturity as a combination of three factors at a given point of time. These factors are: There’s a lot of overlap between these factors.  Defining them precisely isn’t as important as the fact that you need all three. Higher levels of experience, technical sophistication, and technical competence increase technical maturity. Increased AI technical maturity boosts certainty and confidence, which in turn, results in better and more efficient AI-powered outcomes and success. Technical maturity is a major factor behind why some companies are very successful with AI, while other companies struggle to get started and/or achieve success. Turning an AI idea into actual benefits is difficult and requires the “right” goals, leadership, expertise, and approach. It also requires buy-in and alignment at the C-level. Identifying, prioritizing, and goal-setting for AI opportunities is a multi-functional team effort that should include business folks, domain experts, and AI practitioners and researchers. This helps ensure alignment with company goals, while also including necessary business and domain expertise. AI initiatives may also require significant considerations for governance, compliance, ethics, cost, and risk. Further, while the technical details of AI are complex, the outputs of AI techniques are relatively simple. In most cases, AI solutions are built to map a set of inputs to one or more outputs, where the outputs fall into a small group of possibilities. Outputs from trained AI models include numbers (continuous or discrete), categories or classes (e.g., spam or not-spam), probabilities, groups/segments, or a sequence (e.g., characters, words, or sentences). Therefore, AI techniques don’t just solve real-world problems out of the box. They don’t automatically generate revenue and growth, maximize ROI, or keep users engaged and loyal. Likewise, AI doesn’t inherently optimize supply chains, detect diseases, drive cars, augment human intelligence, or tailor promotions to different market segments. Setting a company-wide goal of reducing customer churn by 25% is great, but, unfortunately, is far too broad for most AI applications. That’s why customer churn reduction is not a natural output of AI techniques. The mismatch between goals like reducing customer churn and actual AI outputs must be properly handled and mapped. AI goals should be appropriate for a given company’s technical maturity, and should be chosen to maximize the likelihood of success, prove value, and build a foundation from which to create increasingly sophisticated AI solutions that achieve higher-level business goals. A crawl, walk, run approach is a good analogy for this. Goals should be well-formed, meaning they are stakeholder-specific, map actual AI outputs to applications and use cases that achieve business goals, and are appropriately sized. For companies early in their AI maturity, appropriately-sized goals mean that they should be small and specific enough to experiment with, and prove potential value from, relatively quickly (think lean methodologies and incremental). As AI maturity increases, a non-incremental, holistic, and organization-wide AI vision and strategy should be created to achieve hierarchically-aligned AI goals of varying granularity—goals that drive all AI initiatives and development. This should be accompanied by a transition from incremental thinking to big vision, “applied AI transformation” thinking. Let’s consider the overall goal of reducing customer churn. In an early stage of AI maturity, we can build AI solutions that reduce search friction (e.g., Netflix and Amazon recommendation engines), increase stickiness through personalized promotions and content that is more relevant and engaging, create a predictive model to identify customers most likely to churn and take appropriate preventative actions, or automate and optimize results in areas that are outside of a person’s primary area of expertise (e.g., automated retirement portfolio rebalancing and maximized ROI). When transitioning to developing a bigger AI vision and strategy, we may create a prioritized product roadmap consisting of a suite of recommendation engines and an AI-based personalized loyalty program, for example. At the individual goal level, and for each well-formed goal, the same multi-functional team mentioned earlier must work collaboratively to determine what AI opportunities are available, select and prioritize the ones to pursue, and determine the technical feasibility of each. There are frameworks like SMART to help characterize well-formed goals, but since AI is a field that I characterize as scientific innovation (like R&D), characteristics like being achievable and time-bound may not be the best goals. Results are typically achieved through a scientific process of discovery, exploration, and experimentation, and these processes are not always predictable. Given the scientific nature of AI, goals are better expressed as well-posed questions and hypotheses around a specific and intended benefit or outcome for a certain stakeholder. With well-formed goals, data scientists and machine learning engineers can then apply the scientific method to test different approaches in order to determine the validity of the hypothesis, and assess whether a given approach is feasible and can achieve the goal. For example, by introducing the “Frequently bought together” recommendations (and other recommendations), Amazon was able to increase average customer shopping cart size and order amount (i.e., up-sell and cross-sell), which in turn increases average revenue per customer, which in turn increases Amazon’s e-commerce generated revenue per quarter. McKinsey estimates that up to 35% of Amazon’s revenue and 75% of everything watched on Netflix comes from AI-powered recommendations. But when defining an AI project, the goal or hypothesis in this case isn’t to increase top-line revenue for the company, but rather to posit that building an application that groups products by likelihood to be purchased together will increase average customer order size, which in turn will have an upward impact on top level goals like increasing average revenue per customer and top-line revenue. Another example would be setting a goal around building a well-performing AI model that can predict demand (number of units likely to be purchased) for a specific product for a given day, time, and weather conditions. If accurate, this prediction can help a retailer ensure that they do not run out of stock, which means that there is no lost revenue because a product is out of stock. An added benefit is improved customer experience, which results in happier and more loyal customers who are able to buy the products they want whenever they want to buy it. This same approach can be applied to virtually any other application of AI. AI and machine learning technologies have come a long way in terms of capabilities and accessibility, but off-the-shelf AI solutions aren’t yet available for specific industries or business domains, companies, sets of data, applications, and use cases. The key to success with AI is assembling a multi-functional team that defines appropriate goals, then letting these goals drive the AI initiatives and projects.\n",
            "----------------\n",
            "In our paper, A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild, ACM Multimedia 2020, we aim to lip-sync unconstrained videos in the wild to any desired target speech.\n",
            "----------------\n",
            "“There are lot of definitions of what a developer is […] It’s not just people who write code.” […] Microsoft has even given these “civilian” programmers a persona: Mort. […] The fictional “Mort” is a skilled professional, anyone from a business analyst to a construction site cost estimator, who needs computers to perform specific functions without mastering the intricacies of full-blown programming. We don’t need complete, perfect solutions; we need partial solutions in situations where we don’t have all the information, and we need the ability to explore those solutions with an (artificially) intelligent partner.\n",
            "----------------\n",
            "In a conversation with Kevlin Henney, we started talking about the kinds of user interfaces that might work for AI-assisted programming. This is a significant problem: neither of us were aware of any significant work on user interfaces that support collaboration. However, as software developers, many of us have been practicing effective collaboration for years. It’s called pair programming, and it’s not at all like the models we’ve seen for interaction between an AI system and a human. Most AI systems we’ve seen envision AI as an oracle: you give it the input, it pops out the answer. It’s a unidirectional flow from the source to the destination. This model has many problems; for example, one reason medical doctors have been slow to accept AI may be that it’s good at giving you the obvious solution (“that rash is poison ivy”), at which point the doctor says “I knew that…” Or it gives you a different solution, to which the doctor says “That’s wrong.” Doctors worry that AI will “derail clinicians’ conversations with patients,” hinting that oracles are unwelcome in the exam room (unless they’re human).  Shortly after IBM’s Watson beat the world Jeopardy champions, IBM invited me to see a presentation about it. For me, the most interesting part wasn’t the Jeopardy game Watson played against some IBM employees; it was when they showed the set of answers Watson considered before selecting its answer, weighted with their probabilities. That level was the real gold.  We don’t need an AI system to tell us something obvious, or something we can Google in a matter of seconds. We need AI when the obvious answer isn’t the right one, and one of the possible but rejected answers is. What we really need is the ability to have a dialog with the machine. We still don’t have the user interface for that. We don’t need complete, perfect solutions; we need partial solutions in situations where we don’t have all the information, and we need the ability to explore those solutions with an (artificially) intelligent partner. What is the logic behind the second, third, fourth, and fifth solutions? If we know the most likely solution is wrong, what’s next? Life is not like a game of Chess or Go—or, for that matter, Jeopardy.  What would this look like? One of the most important contributions of Extreme Programming and other Agile approaches was that they weren’t unidirectional. These methodologies stressed iteration: building something useful, demo-ing it to the customer, taking feedback, and then improving. Compared to Waterfall, Agile gave up on the master plan and specification that governed the project’s shape from start to finish, in favor of many mid-course corrections. That cyclic process, which is about collaboration between software developers and customers, may be exactly what we need to get beyond the “AI as Oracle” interaction. We don’t need a prescriptive AI writing code; we need a round trip, in which the AI makes suggestions, the programmer refines those suggestions, and together, they work towards a solution. That solution is probably embedded in an IDE. Programmers might start with a rough description of what they want to do, in an imprecise, ambiguous language like English. The AI could respond with a sketch of what the solution might look like, possibly in pseudo-code. The programmer could then continue by filling in the actual code, possibly with extensive code completion (and yes, based on a model trained on all the code in GitHub or whatever). At this point, the IDE could translate the programmer’s code back into pseudo-code, using a tool like  Pseudogen (a promising new tool, though still experimental). Any writer, whether of prose or of code, knows that having someone tell you what they think you meant does wonders for revealing your own lapses in understanding. MISIM is another research project that envisions a collaborative role for AI.  It watches the code that a developer is writing, extracting its meaning and comparing it with similar code.  It then makes suggestions about rewriting code that looks buggy or inefficient, based on the structure of similar programs. Although its creators suggest that MISIM could eventually lead to machines that program themselves, that’s not what interests me; I’m more interested in the idea that MISIM is helping a human to write better code. AI is still not very good at detecting and fixing bugs; but it is very good at asking a programmer to think carefully when it looks like something is wrong. Is this pair programming with a machine? Maybe.  It is definitely enlisting the machine as a collaborator, rather than as a surrogate. The goal isn’t to replace the programmers, but to make programmers better, to work in ways that are faster and more effective. Will it work? I don’t know; we haven’t built anything like it yet. It’s time to try.\n",
            "----------------\n",
            "Don’t push the responsibility of maintaining invariants required by your class on to its callers.  a scalable open-source multi-model database natively supporting graph, document and search. All supported data models & access patterns can be comnbined in queries allowing for maximal flexibility.\n",
            "----------------\n",
            "A VS Code extension for visualizing data structures while debugging. Like the VS Code’s watch view, but with rich visualizations of the watched value. an integrated dataflow environment for end-users. It allows users to interact with modules that implement functionality for different domains from a single user interface and combine these modules in creative ways. a library that allows you to write declarative logic programs in Rust, with a Datalog-like syntax From initial testing, the generated code is very fast. Variants of transitive closure for large graphs (~1000 nodes) run at comparable speed to compiled Souffle, and at a fraction of the compilation time.\n",
            "----------------\n",
            "Compared to the last few months, there are relatively few items about COVID. And almost no items about Blockchains, though the one item I’ve listed, about China’s Blockchain Services Network, may be the most important item here. I’m seeing a steady stream of articles about various forms of no-code/low-code programming. While many programmers scoff at the idea of programming-without-programming, spreadsheets are an early example of low-code programing. Excel is hardly insignificant. On the AI front, the most significant change is discussion (see the thread below) of a “Deep Learning Recession,” as companies under pressure from COVID look for results and can’t find them.\n",
            "----------------\n",
            "app that automatically tracks how you spend time on your devices I’ve done some previous digging into natural language SQL queries — there’s a good amount of research around this. But the error rate is always too high for useful production deployment and the systems I’ve looked at never handled ambiguity well. The target user for this is someone who knows nothing about SQL so ambiguity is guaranteed. I worked for Cleargraph, a startup which built a natural language query layer on top of RDBMSes and which we sold to Tableau. We reached the same conclusions as you: that such a system must properly handle ambiguous queries, and users need to explicitly understand the results they’re viewing. Futuristic Sci-Fi and Cyberpunk Graphical User Interface Framework for Web Apps.\n",
            "----------------\n",
            "The Covid-19 pandemic has changed how people and businesses spend and operate.  Over the coming pages we’ll explore ways in which our current world is already very different from the one we knew just a few months ago, as well as predictions of our “new normal” once the proverbial boat stops rocking.  Specifically, we’ll see this through the lens of decision-making: how has Covid-19 changed the way we think? And what does this mean for our purchase patterns and business models? You’re used to a certain level of uncertainty in your life, sure.  But the pandemic has quickly turned up the uncertainty on even basic planning. Your dishwasher, piano, or clothes dryer is making an odd sound. Do you proactively call a repair service to check it out?  Your ounce of prevention will also cost you two weeks’ wondering whether the repair technician was an asymptomatic carrier.  If you hold off, you’re placing a bet that the appliance lasts long enough for treatment to become widely available, because you certainly don’t want it to break down just as infection rates spike. Stresses on a system reveal that some of our constants were really variables in disguise.  “I can always leave my house.”  “I can get to the gym on Friday.”  “If I don’t go grocery shopping tonight, I can always do it tomorrow.  It’s not like they’ll run out of food.”  These weren’t exactly bold statements in January.   But by March, many cities’ shelter-in-place orders had turned those periods into question marks. Even as cities are starting to relax those restrictions, there’s the worry that they may suddenly return as the virus continues to spread. As this reality sets in, some of us are even weighing what we call “acceptance purchases”: items which show that we’re in this for the long haul.  Your gym isn’t closed, but it’s as good as closed since the city can quickly order it to shut down if local case counts climb again.  So maybe it’s time to buy that fancy exercise bike.  And ride-hailing services were appealing until using them increased your exposure to the virus.  Maybe now you’ll buy that car you sometimes think about?  You had considered downsizing your home, but you’ll appreciate the extra space if you’re spending more time indoors. Those sorts of purchases are meant to last you for years, though, which means they’re only wise investments if the pandemic (and its impact on the local economy) continues for a long time.  What if we see improved prevention or widespread treatment within a few months?  Do we want to try to offload an exercise bike or a car that we no longer need? The longer you hold off on making those decisions, the greater the chances that you’ll make those purchases too late. It’s tough to make a decision when you can’t rely on your near-term world falling within some certain, predictable scope.  You try to keep all of your options open at all times so that you can be ready for any possibility.  But that’s a lot of extra strain on your brain.  And it’s tiring. The pandemic has made a number of businesses less desirable or outright inaccessible.  Doing that work yourself reduces the impact around the uncertainty of when they’ll return.  It’s also a lot more responsibility for you. Congratulations on running a restaurant, cafe, bar, cinema, gym, school, daycare, office, and storage facility.  You get to buy workout equipment, cooking gear, hair care tools, teaching supplies, and anything else needed to backfill services to which you used to outsource. You’re responsible for the decisions on which models of equipment to buy, as well as the upkeep thereof. You suddenly need to know a lot of things about a lot of things, but you don’t have the time to become an expert in any one of them. Welcome to the diseconomies of non-scale: being small and self-sufficient is expensive. Like a factory, hotel event space, or a fast-food kitchen, you find yourself constantly partitioning or re-tooling rooms to compensate for your limited space.  The gym becomes the reading room becomes the video meeting space becomes the work area.  (You and your spouse flip a coin to see who gets the real office and who gets that basement corner.  Hint: if you want the nicer space, make sure you’re on more video calls. And pretend that you can’t get Zoom backgrounds to work.) The kitchen table flips between a dining area and a school, three times a day.  The bathroom becomes the hair salon every week and quickly switches back again.  All of these swaps take time, effort, and mental energy, what economists collectively refer to as switching costs.  They add up. Quickly. Nor is this just about the size of the home.  It’s a matter of how much you were using it before the pandemic started, the number of spaces present, and the ratio of people to square feet.  If you live in a sprawling, suburban house, but every room was already dedicated to some function or someone’s personal space, then you’re just marginally better off than the person occupying a small, urban apartment. This isn’t just about “working from home,” either.  That usually means that you have a space set aside in your house to work and to take calls, and you have the house to yourself during working hours. Experienced remote-work professionals will tell you that we’re living a very different scenario.  This increasing need to run a standalone, be-everything home means that we are suffering from the curse of generalism: we’re our children’s teachers, our cooks, our housekeepers, our barbers, our IT department, and our fix-it crew.  We’re becoming more self-sufficient, but at the expense of having less time to specialize in our main jobs. All of this means that we’re spending a lot of time just getting by, and not much time advancing. In most places, you don’t need to be “connected” to get by day-to-day.  Whom you know is less important because, with even a modest income, you can get most of what you need.  You mostly care about diversifying your professional network, because that helps you to find a new job, which is what provides you the money that allows you to compensate for not knowing anyone else. When a pandemic triggers stresses in our supply chains, that idea breaks down.  Whom you  know in your personal sphere suddenly counts a lot more.  Instead of money, it’s your social network that gets you through the day. Do you know someone whose job gives them leading indicators on the spread of the virus?  Early in the pandemic, friends of medical professionals got some advance warning of what was to come.  They could gather information from their professional spheres to let their personal networks know that something nasty was brewing.  The same holds for anyone whose business sells protective gear or cleaning supplies.  Over casual drinks, they might mention: “It’s weird … we’re getting a lot of new orders, and not from our usual customers.  Something’s up.  You may want to buy some extras, just in case.”  (This, by the by, shows the value of keeping an eye on your company’s data.  If you don’t have systems to tell you when your sales numbers are abnormal, you may miss information that you already had in-hand.  And in this case, it would have been time-sensitive info.) Communities of shared experience are home to these socially-strong yet professionally-diverse networks.  Family and close friendships top the list, with religious and ethnic ties running a close second.  (People who were part of the same wave of immigration from the same country often forge ties that are as strong as family.)  Neighbors and people who share a hobby are also in there, though to a lesser degree. Within these groups there’s always somebody who has a quick tip, someone who “knows a guy,” someone you can pull aside for a quick “Hey can I ask you about …”  Maybe your niece works at a big grocery chain, and she can tell you when the shipments of hand sanitizer arrive.  In December, this would have been a trivial mention.  Today, when goods are scarce, this is timely information and it can make a difference. Personal networks often have the benefit of being geographically dispersed. Your best friend can ship you cleaning supplies, since they are plentiful in his part of the country.  Your extended family, which stretches from Paris to Singapore, can tell you how their cities are handling shelter-in-place rules.  Chatting with those far-flung aunts and uncles gives you several weeks’ advance notice on how your city’s rules may turn out.  That reduces your uncertainty, which makes it easier for you to prepare, which reduces your stress and decision fatigue. Your ability to forge new relationships can compensate for a smaller social network.  If you don’t have a relative who works at Target, you can ask someone who works there, so long as you have the skill to spot whom to ask.  You have to be able to read people, to see who would be receptive to that question. And you also need to tell whether this would be a simple favor, or something that merits monetary compensation.  The value on that information just increased by a wide margin; shouldn’t the price follow? Relationship-building also counts in the B2B setting.  Such was the case with grocery chain Trader Joe’s.  They’ve managed to avoid shortages during this pandemic, most notably in toilet paper.  When other stores seemed to run out, Trader Joe’s always magically had some in stock.  That’s because they were able to strike a deal with an unnamed hotel chain to buy supplies that were going unused due to dramatic cuts in travel. Granted, Trader Joe’s very business model—white-labeling manufacturers’ goods—smoothed this road.  But their ability to forge that relationship counted just as much as their ability to execute on selling the goods. We can reduce our pandemic-driven stresses by reducing the uncertainty.  To do that, we can trace chains of knock-on effects to determine what changes are coming, and plan accordingly.   For example:  “many restaurants have closed up,” therefore, “there’s less waste from restaurants,” therefore, “there’s less food for rats,” therefore, “expect rats to get more bold.”  So be careful when taking out your trash.  “The pandemic has drastically cut air travel,” therefore “airlines will have less revenue,” therefore “airlines will furlough employees,” therefore “businesses those employees patronized—from in-airport restaurants to hotel shuttle services to their at-home economies—will suffer.” Though we can trace just one chain of effects at a time, multiple paths spin out of every “what next?” and spread out like a spider-crack in a window.  They connect down the line to weave a fabric of impacts. Case in point: WSJ’s Scott McCartney points out that the sudden drop in air travel has upset airlines’ ability to set prices, since they take such a data-driven approach. People who work in the ML/AI field will tell you that this is not just an airline problem: a sudden shift will upend any predictive models built on past behaviors, regardless of industry.  That will affect other fields’ dynamic pricing, yes, but also fraud detection (your credit card reflects a lot of outlier purchases, times, and locations since February) and demand forecasts (a knock-on effect of our collective outlier purchases).  That, in turn, ties to inventory management, which is tied to supply chains, which involve all of the players in the shipping industry, which is tied to fuel consumption and vehicle maintenance… As with any tightly-coupled, complex system, all of these connections work in our favor until they suddenly don’t.  Expect pandemic-related changes to cascade, revealing both endogenous and systemic problems that are related in unexpected ways. One problem with tight coupling, Charles Perrow notes in Normal Accidents, is that materials only have one path to take through the system.  If a component in the middle breaks, everything backs up so the entire system is as good as broken.  You can repair or re-create the old paths (when possible) or create new connections between components.  In Covid-speak, that means our long-term solutions fall into “partially restoring and re-thinking the pre-pandemic life” and “creating new ways to handle the day-to-day when there’s a highly infectious disease running around.”  There are business opportunities in both camps. We mostly assume the phrases “contactless” and “touch-free” refer to electronic payments.  Those are very much in demand right now, but the touch-free space now extends to the wider notion of strangers not interacting in-person, and not handling the same objects at the same time.  That opens the door to online learning, telemedicine, tele-anything.  If you can provide your service at a distance, you have a lot of new prospects. Entertainment already had a firm footing in the online world thanks to video streaming services. The pandemic, and its dramatically reduced cinema attendance, has provided them even more leverage as some movies will have a shorter time on the big screen before they shift to online video.  (As a side note, there’s another chain of knock-on effects to explore: since studios have been known to time releases to coincide with certain seasons and to have a better shot at industry awards, how will that change when films head into living rooms that much sooner?)  Other groups, like Chicago’s Lyric Opera and New York’s Met Opera, are hosting performances online as their subscribers can no longer attend in-person. Still, it’s become more difficult for performances that rely on people being in the same space. Stand-up comedians from Nimesh Patel to Dave Chappelle have recently been able to pull off outdoor gigs with live-but-socially-distanced audiences. Fire-spinning and belly-dance performer Dawn Xiana Moon, of Raks Inferno and Raks Geek, combined multiple streaming services to simulate all of her performers being “on-stage” at the same time. This required her to leverage her technology background, a skill set that is admittedly rare in the live-act world, and she’d still prefer a single platform that just works. By comparison, TV and movie studios have yet to explain how they will manage to film while keeping cast and crew socially distant. The bottom line is that companies that create tools to improve filming multiple, simultaneous, geographically-dispersed teams will have a lot of customers.  (And for movies, will we see an increase in animation to fill the gap?) People are also demanding more of their home internet infrastructure to support increased school- and work-related loads.  (The authors know several people who have shelled out to their ISPs for greater bandwidth.)  That also means a greater load on mid-tier services like social media sites, videoconference services, and the aforementioned streaming video platforms.  If you sell networking hardware or provide network operations services to those companies, you will have no shortage of work. If the pandemic continues long enough, we expect to see a deeper penetration of home broadband service, especially wireless broadband.  This is another touch-free offering, as it permits your provider to establish and troubleshoot internet connectivity issues without sending a technician into your home.  (As another knock-on effect, this means providers will be able to limit field technicians’ service radius to their towers and datacenters, which should let them cover more territory on the same number of staff.) Traditional, multi-year lease commercial real estate was already experiencing disruption due to coworking spaces.  They’ll now both suffer as companies rethink their post-pandemic office needs.  Doubly so since some newly-remote workers are taking the opportunity to move out of state. (Yet another knock-on effect: without office workers, what will happen to the lunch spots and bars that lined the dense urban-business landscape?)  You also have retailers abandoning spaces since there are far too few customers to browse stores. Two types of consumers may pick up that inventory, though.  The first, in the short term, Amazon may convert some old mall spaces into distribution centers. Other businesses will undoubtedly find ways to repurpose empty urban office spaces at deep-discount prices. Second, and in the longer term, we’ll accept that our homes are simply not large enough to be our Everything Place.  People who choose to remain in dense urban environments will want their apartments to be more like standalone houses, which means having space for in-unit washer/dryer, multiple bathrooms, and multiple rooms to serve as offices. Perhaps cities will divvy up old office buildings into large apartments to meet that need.  That’s admittedly more of a stretch, if for no other reason than the time scale involved for the construction effort and the zoning law changes.  For now, some percentage of urban residents will simply pack up for the suburbs, or even more rural areas out of state. Let’s face it: if the restaurant scene has dwindled and public transit feels like too much of a coronavirus risk, then urban living has lost a lot of its luster. Wherever we choose to live, we’ll need more support for running our homes.  This will include ways to make the most of our limited space, such as smaller-scale workout equipment and compact storage, and increased support for DIY repairs, like video tutorials from manufacturers on how to service their products.  This is another stretch, but if the pandemic lasts long enough, manufacturers will modify their products to make them easier to service.  That, or cheaper to just throw away and replace when they encounter a problem. The difficulties of schooling don’t end with table space and bandwidth needs.  There are also the socioemotional concerns such as college students learning how to live away from home, and how the K-12 set learns to socialize when they don’t interact in person.  Not to mention, who will do the teaching?  In March, when stay-at-home orders started to hit US cities, many parents suddenly had to balance their full-time jobs with being full-time teachers.  (Technology consultants Sarah Aslanifar and Bobby Norton jokingly refer to their new roles as, “working from home-school.”)  Businesses took a double hit as they had to scramble to find a way for people to work from home, and then those same people spent the next several weeks distracted during the workday. Some parents have since formed social “pods” with neighbors whom they trust to perform compatible pandemic hygiene.  Some of those have evolved into educational pods, wherein parents spring for someone to teach their group of kids.  An article in MIT Tech Review mentions a price tag of $10,000 per student, per semester.  This isn’t accessible to everyone; but for high-earning parents, it’s a simple economic decision: the cost to outsource schooling is smaller than the amount of money they’ll earn when they can perform their day jobs at full capacity. Higher education was already experiencing some disruption—boot camps and certificate programs on one end, and students questioning their post-college job prospects on the other—and the onset of the pandemic has increased the pressure.  This goes beyond the last few months of sorting out whether and how to open campuses for autumn 2020.  Parents and students alike also question the price tag of a fancy four-year college when students will be attending classes from their kitchen table.  (One SNL sketch framed the experience as “University of Phoenix Online, with worse tech support.”)  For the time being, colleges can busy themselves by shoring up courseware and videoconferencing platforms in order to set autumn 2020 classes in motion.  They’ll quickly need to sort out other near-term concerns (shoring up lost profits from empty student housing) as well as their future prospects (demonstrating their value compared to vocational programs, especially if the job market suffers over the long term).   If schools can’t sort this out on their own, they’ll likely pay someone to sort it out for them.  There’s also a business opportunity in providing a centralized, one-stop SaaS platform such that colleges won’t have to cobble together their own with a mix of one-off tools. One silver lining of working from home is that your job prospects just opened up.  Covid-19 has forced a lot of companies to admit that the old “this work can’t be done from home” excuse doesn’t hold up.  Some of them are even starting to like it: they see how much money they were burning on an office for people who already knew they’d be more effective working from home.  Many of them will scratch that line item from next year’s budget. This means we’ll see more remote hiring in the sectors that can support it.  That will establish a clear boundary between the companies that see the benefits (“we’re now able to hire across the country for these hard-to-fill roles”) and those that do not (“we’re only hiring people who live in this city, for when we go back to the office”).  Big tech-sector names like Google and Facebook have already announced plans to extend work-from-home support, while Twitter and Atlassian have flat-out said that their crews can work from home indefinitely.  In some fields, failing to provide a remote-work option may limit your talent pool.  It will be the equivalent of running an office space in the suburbs when most companies, and their prospective employees, exist in the dense urban center and have no desire to commute. Just as we’ll pay for help adapting to the current state of things, we’ll also pay for some semblance of “the old normal.” People generally like meeting up, whether one-on-one for a tea or in larger groups for a party.  We’re already using videoconferencing tools to hang out with friends and family, and to attend events.  But we’re adapting to the tools more than the other way around.  Right now services like Hangouts, Meet, and  Zoom are still very much designed for, well, video versions of office conference calls: one person speaks at a time, and you get a “Brady Bunch” grid view of attendees.  Expect the incumbent vendors as well as new upstarts to create tools that are better suited for [specific interaction]-over-video, like conferences, classroom teaching, or music lessons. We’re really feeling this in online conferences.  While webinar tools fulfill the mission of letting a person deliver a talk to a large number of attendees, they don’t support other aspects of an in-person event.  Randomly bumping into people and “hallway track” sessions have forged long-term bonds between conference attendees, far more than the talks themselves.  This could serve as a driver for VR, as that will take us away from “attending events from our living room” to “being in our living room, but actually attending events in a dedicated space.”  There is a big difference. Another reason people meet up is to play games.  Online games are nothing new, and they’ve even gained some mainstream street cred thanks to casual gaming.  Expect to see improved coordination, such that you can play with people of your choosing (a feature lacking in a number of iOS Game Center offerings).  People playing more video games may also lead to greater participation in esports leagues, and even taking business meetings over a gaming session. In-person interaction is our most risky form of socializing at the moment, but it’s also the one people want the most. Goods and services that help us to (safely) meet face-to-face will not just help us on an emotional level, but they could play a key role in helping the economy get back on its feet. We have masks and face shields, which are good for being in public.  What about protective overgarments, reminiscent of 1950s interpretations of outer-space wear?  We could wear them to protect our entire body in public transit or airplanes, and then shed them before entering a friend’s home.  There’s also the down-to-earth business of designing and installing plastic shields between restaurant tables.  Maybe someone will create transparent, oversized cabins that allow you and a few trusted friends to be “on the beach” but still be indoors and away from others. Meeting in person also counts for office space.  In a work-from-home world, some teams still prefer the in-person experience.  What can we do to make it safer to be in the office, beyond standing several feet apart at all times?  An effective but low-tech offering could involve installing protective shields around conference tables (not unlike what we see in some restaurants) or modifying office layouts to discourage crowding.  The next step up would increase touch-free actions, such as choosing your elevator floor through a smartphone app.  Larger and higher-tech offerings would go deep into the guts of the building to install virus purifiers in building HVAC systems and the accompanying ductwork. Where do we go from here?  That depends how long we go without treatment or improved preventative measures.  One thing’s for sure: Covid-19 is a driver of change.  There is no more “normal” in terms of how we shop for groceries, attend events, or even lay out our homes.  It’s up to us to adapt to our present, even as that present continues to change, and that will influence how we decide what to buy and sell. How much we change, as people, depends on how long the pandemic lasts.  It’s possible that it will carve deep grooves in our collective social memory, similar to the Great Depression, and its impact will influence how people behave long after the disease is a threat. It also depends on how much we are willing to adapt.  That is a function of how soon we’re willing to let go of “normal,” which is really a euphemism for “the past.”  Especially since the past is heavily mythologized.\n",
            "----------------\n",
            "It is primarily written for the survey respondents, and anyone dealing with burn-out and resilience issues either in themselves, family members and employees. If you’re only interested in how to address burn-out skip to section seven. Ethics committees have at least three roles to play. The first is education. […] The second role of ethics committees is policy formation and review. […] The third role of ethics committees is to provide ethical consultation. [T]echnological decisions are not only about facts (for example, about what is more efficient), but also about the kind of life we want and the kind of society we strive to build. Simple, yet powerful ORM for modeling and querying data. (a) Schema As Code – model any database schema as Go objects. (b) Easily Traverse Any Graph – run queries, aggregations and traverse any graph structure easily. (c) Statically Typed And Explicit API – 100% statically typed and explicit API using code generation. (d) Multi Storage Driver – supports MySQL, PostgreSQL, SQLite and Gremlin.\n",
            "----------------\n",
            "Individuals typically “hire” communities to accomplish transitions that require human connection. Why do people join communities?; Member quality determines community success; Design your community to spark quality interactions; The two levels of group cohesion; Recognizing and retaining key members; Growing your ranks; A Time to Build.\n",
            "----------------\n",
            "grouped 10 design space dimensions into four major stages of a data science workflow: importing data into notebooks (data sources), editing code and prose (editor style, supported programming languages, versioning, collaboration), running code to generate outputs (cell execution order, liveness [6], execution environment, and cell outputs), and publishing notebook outputs. an open library for computational embroidery with Processing. Neuromorphic chips are packed with artificial neurons and artificial synapses that mimic the activity spikes that occur within the human brain—and they handle all this processing on the chip. This results in smarter, far more energy-efficient computing systems.\n",
            "----------------\n",
            "So you need to redesign your company’s data infrastructure. Do you buy a solution from a big integration company like IBM, Cloudera, or Amazon?  Do you engage many small startups, each focused on one part of the problem?  A little of both?  We see trends shifting towards focused best-of-breed platforms. That is, products that are laser-focused on one aspect of the data science and machine learning workflows, in contrast to all-in-one platforms that attempt to solve the entire space of data workflows. This article, which examines this shift in more depth, is an opinionated result of countless conversations with data scientists about their needs in modern data science workflows. Today we see two different kinds of offerings in the marketplace: Integrated all-in-one platforms assemble many tools together, and can therefore provide a full solution to common workflows. They’re reliable and steady, but they tend not to be exceptional at any part of that workflow and they tend to move slowly. For this reason, such platforms may be a good choice for companies that don’t have the culture or skills to assemble their own platform. In contrast, best-of-breed products take a more craftsman approach: they do one thing well and move quickly (often they are the ones driving technological change). They usually meet the needs of end users more effectively, are cheaper, and easier to work with.  However some assembly is required because they need to be used alongside other products to create full solutions.  Best-of-breed products require a DIY spirit that may not be appropriate for slow-moving companies. Which path is best? This is an open question, but we’re putting our money on best-of-breed products. We’ll share why in a moment, but first, we want to look at a historical perspective with what happened to data warehouses and data engineering platforms. Historically, companies bought Oracle, SAS, Teradata or other data all-in-one data warehousing solutions. These were rock solid at what they did–and “what they did” includes offering packages that are valuable to other parts of the company, such as accounting–but it was difficult for customers to adapt to new workloads over time. Next came data engineering platforms like Cloudera, Hortonworks, and MapR, which broke open the Oracle/SAS hegemony with open source tooling. These provided a greater level of flexibility with Hadoop, Hive, and Spark. However, while Cloudera, Hortonworks, and MapR worked well for a set of common data engineering workloads, they didn’t generalize well to workloads that didn’t fit the MapReduce paradigm, including deep learning and new natural language models. As companies moved to cloud, embraced interactive Python, integrated GPUs, or moved to a greater diversity of data science and machine learning use cases, these data engineering platforms weren’t ideal. Data scientists rejected these platforms and went back to working on their laptops where they had full control to play around and experiment with new libraries and hardware. While data engineering platforms provided a great place for companies to start building data assets, their rigidity becomes especially challenging when companies embrace data science and machine learning, both of which are highly dynamic fields with heavy churn that require much more flexibility in order to stay relevant. An all-in-one platform makes it easy to get started, but can become a problem when your data science practice outgrows it. So if data engineering platforms like Cloudera displaced data warehousing platforms like SAS/Oracle, what will displace Cloudera as we move into the data science/machine learning age? The worlds of data science and machine learning move at a much faster pace than data warehousing and much of data engineering.  All-in-one platforms are too large and rigid to keep up.  Additionally, the benefits of integration are less relevant today with technologies like Kubernetes.  Let’s dive into these reasons in more depth. “Data science” is an incredibly broad term that encompasses dozens of activities like ETL, machine learning, model management, and user interfaces, each of which have many rapidly evolving choices. Only part of a data scientist’s workflow is typically supported by even the most mature data science platforms. Any attempt to build a one-size-fits-all integrated platform would have to include such a wide range of features, and such a wide range of choices within each feature, that it would be extremely difficult to maintain and keep up to date.  What happens when you want to incorporate real-time data feeds? What happens when you want to start analyzing time series data?  Yes, the all-in-one platforms will have tools to meet these needs; but will they be the tools you want, or the tools you’d choose if you had the opportunity? Consider user interfaces. Data scientists use many tools like Jupyter notebooks, IDEs, custom dashboards, text editors, and others throughout their day. Platforms offering only “Jupyter notebooks in the cloud” cover only a small fraction of what actual data scientists use in a given day. This leaves data scientists spending half of their time in the platform, half outside the platform, and a new third half migrating between the two environments. Consider also the computational libraries that all-in-one platforms support, and the speed at which they go out of date quickly. Famously, Cloudera ran Spark 1.6 for years after Spark 2.0 was released–even though (and perhaps because) Spark 2.0 was released only 6 months after 1.6. It’s quite hard for a platform to stay on top of all of the rapid changes that are happening today. They’re too broad and numerous to keep up with. While the variety of data science has made all-in-one platforms harder, at the same time advances in infrastructure have made integrating best-of-breed products easier. Cloudera, Hortonworks, and MapR were necessary at the time because Hadoop, Hive, and Spark were notoriously difficult to set up and coordinate. Companies that lacked technical skills needed to buy an integrated solution. But today things are different. Modern data technologies are simpler to set up and configure. Also, technologies like Kubernetes and the cloud help to commoditize configuration and reduce integration pains with many narrowly-scoped products. Kubernetes lowers the barrier to integrating new products, which allows modern companies to assimilate and retire best-of-breed products on an as-needed basis without a painful onboarding process. For example, Kubernetes helps data scientists deploy APIs that serve models (machine learning or otherwise), build machine learning workflow systems, and is an increasingly common substrate for web applications that allows data scientists to integrate OSS technologies, as reported here by Hamel Hussain, Staff Machine Learning Engineer at Github. Kubernetes provides a common framework in which most deployment concerns can be specified programmatically.  This puts more control into the hands of library authors, rather than individual integrators.  As a result the work of integration is greatly reduced, often just specifying some configuration values and hitting deploy.  A good example here is the Zero to JupyterHub guide.  Anyone with modest computer skills can deploy JupyterHub on Kubernetes without knowing too much in about an hour.  Previously this would have taken a trained professional with pretty deep expertise several days. We believe that companies that adopt a best-of-breed data platform will be more able to adapt to technology shifts that we know are coming. Rather than being tied into a monolithic data science platform on a multi-year time scale, they will be able to adopt, use, and swap out products as their needs change.  Best of breed platforms enable companies to evolve and respond to today’s rapidly changing environment. The rise of the data analyst, data scientist, machine learning engineer and all the satellite roles that tie the decision function of organizations to data, along with increasing amounts of automation and machine intelligence, require tooling that meet these end users’ needs. These needs are rapidly evolving and tied to open source tooling that is also evolving rapidly. Our strong opinion (strongly held) is that best-of-breed platforms are better positioned to serve these rapidly evolving needs by building on these OSS tools than all-in-platforms. We look forward to finding out.  1 Note that we’re discussing data platforms that are built on top of OSS technologies, rather than the OSS technologies themselves. This is not another Dask vs Spark post, but a piece weighing up the utility of two distinct types of modern data platforms.\n",
            "----------------\n",
            "Sinter uses the user-mode EndpointSecurity API to subscribe to and receive authorization callbacks from the macOS kernel, for a set of security-relevant event types. The current version of Sinter supports allowing/denying process executions; in future versions we intend to support other types of events such as file, socket, and kernel events. We hate being wronged, and it makes us vengeful. On the other hand, we don’t necessarily love being “done right by,” and we don’t have a particular motivation that comes from it. There’s no “positive” version of revenge. These are useful steps but will not stop QAnon from spreading in social media comments or private chat groups or unmoderated forums. It’s not something we can reasonably hope for, and I don’t think there’s any technological solution (e.g. browser extensions) either. The only way to stop people from mistaking speculation from fact is for them to want to stop.\n",
            "----------------\n",
            "StackOverflow’s 2020 developer survey included a table showing the  “most loved, dreaded, and wanted languages.” Loved and wanted languages are, well, sort of boring. The programming languages we dread are a lot more interesting. As Tolstoy said, “All happy families are alike; each unhappy family is unhappy in its own way.” So what are these unhappy, unloved languages, and why do programmers dread using them? Given the chance it’s, well, hard to resist jumping in with some theories, and perhaps even saying something impolitic.  Or defending some languages that are disliked for the wrong reasons. More precisely, StackOverflow tabulated the “% of developers who are developing with the language or technology but have not expressed interest in continuing to do so.” That doesn’t sound quite as dire as “dreaded”; “not expressing an interest in working with a language again” is a rather vague indication of dread. There are lots of things I’ve done that I’d rather not do again, including writing troff macros that spawned shell scripts. But we won’t worry about that, will we? The list of least liked languages is similar to the lists of the most widely used languages, as indicated by Redmonk, Tiobe and, for that matter, searches on O’Reilly Learning. That’s no surprise; Bjarne Stroustrup said that “there are only two kinds of languages: the ones people complain about and the ones nobody uses.” And that makes a lot of sense, at least in relation to this survey. If you’ve got millions of users, it’s not hard to get a lot of people to dislike you. So seeing perennials like C alongside relative newcomers like Java on the list of disliked languages isn’t surprising. Kevlin Henney and I thought that the list of least liked languages also reflected the opinions of programmers who were working on large and legacy projects, as opposed to short programs. Dislike of a language may be “guilt by association”: dislike of a large, antiquated codebase with minimal documentation, and an architectural style in which every bug fixed breaks something else. Therefore, it’s not surprising to see languages that used to be widely used but have fallen from popularity on the list. And it’s also easy to fall in love with a quirky language that was perfect for one project, but that you’ll never see again.  (In my case, that’s Icon. Try it; you might like it. It’s not on anyone’s list.) What’s most surprising is when a language is out of place: when it’s significantly more or less disliked than you expect. That’s what I’d like to think about. So, having disposed of the preliminaries, here are a few observations: There’s lots more that could be said.  There’s no surprise that VBA is #1 disliked language.  I’ll admit to complete ignorance on Objective C (#2), which I’ve never had any reason to play with. Although I’m a Perl-hater from way back, I’m surprised that Perl is so widely disliked (#3), but some wounds never heal. It will be interesting to see what happens after Perl 7 has been out for a few years. Assembly (#4) is an acquired taste (and isn’t a single language).  If you don’t learn to love it, you pretty much have to hate it. And if you don’t love it, you really shouldn’t be using it. You can almost always avoid assembly, but when you need to work directly with the hardware, there’s no alternative.  C and C++ (#5 and #8, respectively) give you a lot of rope to hang yourself, but get you as close enough to the hardware for almost any project, without the pain of assembly.  Are they fading into the past, or will they be with us forever?  My guess is the latter; there are too many projects that demand C’s performance and ubiquity. It’s the foundation for just about everything important in modern computing. Speculating about languages and why they’re liked or disliked is fun.  It may or may not be useful.  Take it for what it’s worth.\n",
            "----------------\n",
            "lightweight, high-speed immutable database for systems and applications. Open Source and easy to integrate into any existing application. Japanese startup Donut Robotics […] created a smart mask — a high-tech upgrade to standard face coverings, designed to make communication and social distancing easier. In conjunction with an app, the C-Face Smart mask can transcribe dictation, amplify the wearer’s voice, and translate speech into eight different languages. a Cyber Punk inspired, Text Based MMORPG Browser Game where gameplay interfaces are ‘Stealthily’ mimicking the VSCode interface. Pysa performs iterative rounds of analysis to build summaries to determine which functions return data from a source and which functions have parameters that eventually reach a sink. If Pysa finds that a source eventually connects to a sink, it reports an issue.\n",
            "----------------\n",
            "I have a system with c servers, each of which can only handle a single concurrent request, and has no internal queuing. The servers sit behind a load balancer, which contains an infinite queue. An unlimited number of clients offer c * 0.8 requests per second to the load balancer on average. In other words, we increase the offered load linearly with c to keep the per-server load constant. Once a request arrives at a server, it takes one second to process, on average. How does the client-observed mean request team vary with c? Crush is an attempt to make a traditional command line shell that is also a modern programming language. It has the features one would expect from a modern programming language like a type system, closures and lexical scoping, but with a syntax geared toward both batch and interactive shell usage. DGL-KE is a high performance, easy-to-use, and scalable package for learning large-scale knowledge graph embeddings. The package is implemented on the top of Deep Graph Library (DGL) and developers can run DGL-KE on CPU machine, GPU machine, as well as clusters\n",
            "----------------\n",
            "With one of the devices up and running, you can point NyanSat’s antenna to specific coordinates in the sky and listen for the radio frequency transmissions coming from a satellite that’s out there.\n",
            "----------------\n",
            "I thought July was going to be a dull month, but I’m wrong again. COVID-specific technology seems to be drying up, though there’s a fascinating report about a DIY vaccine. (Developed by serious scientists, so don’t try this at home.) There’s a lot of news about AI, and specifically, about the GPT-2 and GPT-3 language models. And a few things that are just fun, like Festo’s Bionic Swifts.\n",
            "----------------\n",
            "It’s typical that a log will be accessed zero times. Collecting, aggregating, and indexing logs is usually a mistake made by people who aren’t clear on the use case for the logs. an ultra-lightweight Virtual DOM, highly-optimized diff algorithm, and state management library obsessed with minimalism. Update the database; scale vertically; leverage application code; use efficient data types; data normalization and denormalization; precompute data; leverage materialized views; use proper indexes; leverage the execution plan for query optimization; choose correct transaction isolation level; bulk INSERTs and UPDATEs; compress data for storage; make ALTER TABLEs work; manage concurrent connections; add read replicas; disk partitioning; use specialized extensions; sharding; don’t store everything in one table; process data outside the SQL database; be aware of the limitations of managed SQL databases.  We conclude that it appears next to impossible to find secret questions that are both secure and memorable. Secret questions continue have some use when combined with other signals, but they should not be used alone and best practice should favor more reliable alternatives.\n",
            "----------------\n",
            "tl;dr: I made a prototype IDE in which language semantics are specified in datalog, powered by a datalog interpreter written in TypeScript, running the browser. Demo here. Model Cards […] provide a structured framework for reporting on ML model provenance, usage, and ethics-informed evaluation and give a detailed overview of a model’s suggested uses and limitations. […] To streamline the creation of Model Cards for all ML practitioners, we are sharing the Model Card Toolkit (MCT), a collection of tools that support developers in compiling the information that goes into a Model Card and that aid in the creation of interfaces that will be useful for different audiences. Dafny is a language that is designed to make it easy to write correct code. This means correct in the sense of not having any runtime errors, but also correct in actually doing what the programmer intended it to do. To accomplish this, Dafny relies on high-level annotations to reason about and prove correctness of code. The effect of a piece of code can be given abstractly, using a natural, high-level expression of the desired behavior, which is easier and less error prone to write. Dafny then generates a proof that the code matches the annotations (assuming they are correct, of course!). Dafny lifts the burden of writing bug-free code into that of writing bug-free annotations. This is often easier than writing the code, because annotations are shorter and more direct.\n",
            "----------------\n",
            "Prefer to push fixes upstream instead of working around problems downstream. In October, 1953, I coined the word ‘software.’ In this article, we turn our attention to the process itself: how do you bring a product to market?\n",
            "----------------\n",
            "   Product Managers are responsible for the successful development, testing, release, and adoption of a product, and for leading the team that implements those milestones. Product managers for AI must satisfy these same responsibilities, tuned for the AI lifecycle. In the first two articles in this series, we suggest that AI Product Managers (AI PMs) are responsible for: If you’re an AI product manager (or about to become one), that’s what you’re signing up for.  In this article, we turn our attention to the process itself: how do you bring a product to market? The first step in building an AI solution is identifying the problem you want to solve, which includes defining the metrics that will demonstrate whether you’ve succeeded. It sounds simplistic to state that AI product managers should develop and ship products that improve metrics the business cares about. Though these concepts may be simple to understand, they aren’t as easy in practice.  It’s often difficult for businesses without a mature data or machine learning practice to define and agree on metrics. Politics, personalities, and the tradeoff between short-term and long-term outcomes can all contribute to a lack of alignment. Many companies face a problem that’s even worse: no one knows which levers contribute to the metrics that impact business outcomes, or which metrics are important to the company (such as those reported to Wall Street by publicly-traded companies). Rachel Thomas writes about these challenges in “The problem with metrics is a big problem for AI.” There isn’t a simple fix for these problems, but for new companies, investing early in understanding the company’s metrics ecosystem will pay dividends in the future.  The worst case scenario is when a business doesn’t have any metrics. In this case, the business probably got caught up in the hype about AI, but hasn’t done any of the preparation. (Fair warning: if the business lacks metrics, it probably also lacks discipline about data infrastructure, collection, governance, and much more.) Work with senior management to design and align on appropriate metrics, and make sure that executive leadership agrees and consents to using them before starting your experiments and developing your AI products in earnest. Getting this kind of agreement is much easier said than done, particularly because a company that doesn’t have metrics may never have thought seriously about what makes their business successful. It may require intense negotiation between different divisions, each of which has its own procedures and its own political interests. As Jez Humble said in a Velocity Conference training session, “Metrics should be painful: metrics should be able to make you change what you’re doing.” Don’t expect agreement to come simply. Lack of clarity about metrics is technical debt worth paying down. Without clarity in metrics, it’s impossible to do meaningful experimentation. A product manager needs to think about ethics–and encourage the product team to think about ethics–throughout the whole product development process, but it’s particularly important when you’re defining the problem.  Is it a problem that should be solved?  How can the solution be abused? Those are questions that every product team needs to think about. There’s a substantial literature about ethics, data, and AI, so rather than repeat that discussion, we’ll leave you with a few resources.  Ethics and Data Science is a short book that helps developers think through data problems, and includes a checklist that team members should revisit throughout the process. The Markkula Institute at the University of Santa Clara has an excellent list of resources, including an app to aid ethical decision-making. The Ethical OS also provides excellent tools for thinking through the impact of technologies.  And finally–build a team that includes people of different backgrounds, and who will be affected by your products in different ways. It’s surprising (and saddening) how many ethical problems could have been avoided if more people thought about how the products would be used. AI is a powerful tool: use it for good. Once you know which metrics are most important, and which levers affect them, you need to run experiments to be sure that the AI products you want to develop actually map to those business metrics.  Experiments allow AI PMs not only to test assumptions about the relevance and functionality of AI Products, but also to understand the effect (if any) of AI products on the business. AI PMs must ensure that experimentation occurs during three phases of the product lifecycle: AI product managers need to understand how sensitive their project is to error. This isn’t always simple, since it doesn’t just take into account technical risk; it also has to account for social risk and reputational damage. As we mentioned in the first article of this series, an AI application for product recommendations can make a lot of mistakes before anyone notices (ignoring concerns about bias); this has business impact, of course, but doesn’t cause life-threatening harm. On the other hand, an autonomous vehicle really can’t afford to make any mistakes; even if the autonomous vehicle is safer than a human driver, you (and your company) will take the blame for any accidents. AI PMs have to make tough choices when deciding where to apply limited resources. It’s the old “choose two” rule, where the parameters are Speed, Quality, and Features. For example, for a mobile phone app that uses object detection to identify pets, speed is a requirement. A product manager may sacrifice either a more diverse set of animals, or the accuracy of detection algorithms.  These decisions have dramatic implications on project length, resources, and goals. Similarly, AI product managers often need to choose whether to prioritize the scale and impact of a product over the difficulty of product development.  Years ago a health and fitness technology company realized that its content moderators, used to manually detect and remediate offensive content on its platform, were experiencing extreme fatigue and very poor mental health outcomes.  Even beyond the humane considerations, moderator burnout was a serious product issue, in that the company’s platform was rapidly growing, thus exposing the average user to more potentially offensive or illegal content.  The difficulty of content moderation work was exacerbated by its repetitive nature, making it a candidate for automation via AI.  However, the difficulty of developing a robust content moderation system at the time was significant, and would have required years of development time and research. Ultimately, the company decided to simply drop the most social components of the platform, a decision which limited overall growth. This tradeoff between impact and development difficulty is particularly relevant for products based on deep learning: breakthroughs often lead to unique, defensible, and highly lucrative products, but investing in products with a high chance of failure is an obvious risk. Products based on deep learning can be difficult (or even impossible) to develop; it’s a classic “high return versus high risk” situation, in which it is inherently difficult to calculate return on investment. The final major tradeoff that AI product managers must evaluate is how much time to spend during the R&D and design phases.  With no restrictrictions on release dates, PMs and engineers alike would choose to spend as much time as necessary to nail the product goals. But in the real world, products need to ship, and there’s rarely enough time to do the research necessary to ship the best possible product. Therefore, product managers must make a judgment call about when to ship, and that call is usually based on incomplete experimental results. It’s a balancing act, and admittedly, one that can be very tricky: achieving the product’s goals versus getting the product out there. As with traditional software, the best way to achieve your goals is to put something out there and iterate. This is particularly true for AI products. Microsoft, LinkedIn, and Airbnb have been especially candid about their journeys towards building an experiment-driven culture and the technology required to support it. Some of the best lessons are captured in Ron Kohavi, Diane Tang, and Ya Xu’s book: Trustworthy Online Controlled Experiments : A Practical Guide to A/B Testing. The development phases for an AI project map nearly 1:1 to the AI Product Pipeline we described in the second article of this series.  AI projects require a “feedback loop” in both the product development process and the AI products themselves. Because AI products are inherently research-based, experimentation and iterative development are necessary. Unlike traditional software development, in which the inputs and results are often deterministic, the AI development cycle is probabilistic. This requires several important modifications to how projects are set up and executed, regardless of the project management framework. Product managers must ensure that AI projects gather qualitative information about customer behavior. Because it might not be intuitive, it’s important to note that traditional data measurement tools are more effective at measuring magnitude than sentiment. For most AI products, the product manager will be less interested in the click-through rate (CTR) and other quantitative metrics than they are in the utility of the AI product to the user. Therefore, traditional product research teams must engage with the AI team to ensure that the correct intuition is applied to AI product development, as AI practitioners are likely to lack the appropriate skills and experience. CTRs are easy to measure, but if you build a system designed to optimize these kinds of metrics, you might find that the system sacrifices actual usefulness and user satisfaction. In this case, no matter how well the AI product contributes to such metrics, it’s output won’t ultimately serve the goals of the company. It’s easy to focus on the wrong metric if you haven’t done the proper research. One mid-sized digital media company we interviewed reported that their Marketing, Advertising, Strategy, and Product teams once wanted to build an AI-driven user traffic forecast tool. The Marketing team built the first model, but because it was from marketing, the model optimized for CTR and lead conversion. The Advertising team was more interested in cost per lead (CPL) and lifetime value (LTV), while the Strategy team was aligned to corporate metrics (revenue impact and total active users).  As a result, many of the tool’s users were dissatisfied, even though the AI functioned perfectly. The ultimate result was the development of multiple models that optimize for different metrics, and the redesign of the tool so that it could present those outputs clearly and intuitively to different kinds of users. Internally, AI PMs must engage stakeholders to ensure alignment with the most important decision-makers and top-line business metrics. Put simply, no AI product will be successful if it never launches, and no AI product will launch unless the project is sponsored, funded, and connected to important business objectives. This phase of an AI project is laborious and time consuming, but completing it is one of the strongest indicators of future success. A product needs to balance the investment of resources against the risks of moving forward without a full understanding of the data landscape. Acquiring data is often difficult, especially in regulated industries. Once relevant data has been obtained, understanding what is valuable and what is simply noise requires statistical and scientific rigor.  AI product managers probably won’t do the research themselves; their role is to guide data scientists, analysts, and domain experts towards a product-centric evaluation of the data, and to inform meaningful experiment design. The goal is to have a measurable signal for what data exists, solid insights into that data’s relevance, and a clear vision of where to concentrate efforts in designing features. Data wrangling and feature engineering is the most difficult and important phase of every AI project.  It’s generally accepted that, during a typical product development cycle, 80% of a data scientist’s time is spent in feature engineering. Trends and tools in AutoML and Deep Learning have certainly reduced the time, skills, and effort required to build a prototype, if not an actual product.  Nonetheless, building a superior feature pipeline or model architecture will always be worthwhile. AI product managers should make sure project plans account for the time, effort, and people needed. The modeling phase of an AI project is frustrating and difficult to predict. The process is inherently iterative, and some AI projects fail (for good reason) at this point. It’s easy to understand what makes this step difficult: there is rarely a sense of steady progress towards a goal. You experiment until something works; that might happen on the first day, or the hundredth day. An AI product manager must motivate the team members and stakeholders when there is no tangible “product” to show for everyone’s labor and investment.  One strategy for maintaining motivation is to push for short-term bursts to beat a performance baseline. Another would be to start multiple threads (possibly even multiple projects), so that some will be able to demonstrate progress. Unlike traditional software engineering projects, AI product managers must be heavily involved in the build process. Engineering managers are usually responsible for making sure all the components of a software product are properly compiled into binaries, and for organizing build scripts meticulously by version to ensure reproducibility. Many mature DevOps processes and tools, honed over years of successful software product releases, make these processes more manageable, but they were developed for traditional software products. The equivalent tools and processes simply do not exist in the ML/AI ecosystem; when they do, they are rarely mature enough to use at scale. As a result, AI PMs must take a high-touch, customized approach to guiding AI products through production, deployment, and release. Like any other production software system, after an AI product is live it must be monitored. However, for an AI product, both model performance and application performance must be monitored simultaneously. Alerts that are triggered when the AI product performs out of specification may need to be routed differently; the in-place SRE team may not be able to diagnose technical issues with the model or data pipelines without support from the AI team. Though it’s difficult to create the “perfect” project plan for monitoring, it’s important for AI PMs to ensure that project resources (especially engineering talent) aren’t immediately released when the product has been deployed.  Unlike a traditional software product, it’s hard to define when an AI product has been deployed successfully.  The development process is iterative, and it’s not over after the product has been deployed–though, post-deployment, the stakes are higher, and your options for dealing with issues are more limited.  Therefore, members of the development team must remain on the maintenance team to ensure that there is proper instrumentation for logging and monitoring the product’s health, and to ensure that there are resources available to deal with the inevitable problems that show up after deployment. (We call this “debugging” to distinguish it from the evaluation and testing that takes place during product development. The final article in this series will be devoted to debugging.) Among operations engineers, the idea of observability is gradually replacing monitoring.  Monitoring requires you to predict the metrics you need to watch in advance.  That ability is certainly important for AI products–we’ve talked all along about the importance of metrics.  Observability is critically different.  Observability is the ability to get the information you need to understand why the system behaved the way it does; it’s less about measuring known quantities, and more about the ability to diagnose “unknown unknowns.” We’ve spent a lot of time talking about planning.  Now let’s shift gears and discuss what’s needed to build a product.  After all, that’s the point. AI Product Interface Design The AI product manager must be a member of the design team from the start, ensuring that the product provides the desired outcomes. It’s important to account for the ways a product will be used. In the best AI products, users can’t tell how the underlying models impact their experience. They neither know or care that there is AI in the application. Take Stitch Fix, which uses a multitude of algorithmic approaches to provide customized style recommendations. When a Stitch Fix user interacts with its AI products, they interface with the prediction and recommendation engines. The information they interact with during that experience is an AI product–but they neither know, nor care, that AI is behind everything they see. If the algorithm makes a perfect prediction, but the user can’t imagine wearing the items they’re shown, the product is still a failure. In reality, ML models are far from perfect, so it is even more imperative to nail the user experience. To do so, product managers must ensure that design gets an equal seat at the table with engineering.  Designers are more attuned to qualitative research about user behavior.  What signals show user satisfaction?  How do you build products that delight users? Apple’s sense of design, making things that “just work,” pioneered through the iPod, iPhone, and iPad products is the foundation of their business.  That’s what you need, and you need that input from the beginning.  Interface design isn’t an after-the-fact add-on. Picking the Right Scope “Creeping featurism” is a problem with any software product, but it’s a particularly dangerous problem for AI. Focus your product development effort on problems that are relevant to the business and consumer. A successful AI product measurably (and positively) impacts metrics that matter to the business. Therefore, limit the scope of an AI product to features that can create this impact.  To do so, start with a well-framed hypothesis that, upon validation through experimentation, will produce meaningful outcomes. Doing this effectively means that AI PMs must learn to translate business intuitions into product development tools and processes. For example, if the business seeks to understand more about its customer base in order to maximize lifetime value for a subscription product, an AI PM would do well to understand the tools available for customer and product-mix segmentation, recommendation engines, and time-series forecasting. Then, when it comes to developing the AI product roadmap, the AI PM can focus engineering and AI teams on the right experiments, the correct outcomes,andthe smoothest path to production. It is tempting to over-value the performance gains achieved through the use of more complex modeling techniques, leading to the dreaded “black box” problem: models for which it’s difficult (if not impossible) to understand the relationship between the input and the output. Black box models are seldom useful in business environments for several reasons. First, being able to explain how the model works is often a prerequisite for executive approval. Ethical and regulatory considerations often require a detailed understanding of the data, derived features, pipelines and scoring mechanisms involved in the AI system. Solving problems with the simplest model possible is always preferable, and not just because it leads to models that are interpretable. In addition, simpler modeling approaches are more likely to be supported by a wide variety of frameworks, data platforms, and languages, increasing interoperability and decreasing technical debt. Another scoping consideration concerns the processing engine that will power the product. Problems that are real-time (or near real-time) in nature can only be addressed by highly performant stream processing architectures. Examples of this include product recommendations in e-commerce systems or AI-enabled messaging. Stream processing requires significant engineering effort, and it’s important to account for that effort at the beginning of development. Some machine learning approaches (and many software engineering practices) are simply not appropriate for near-real time applications. If the problem at hand is more flexible and less interactive (such as offline churn probability prediction), batch processing is probably a good approach, and is typically easier to integrate with the average data stack. Prototypes and Data Product MVPs Entrepreneurial product managers are often associated with the phrase “Move Fast and Break Things.”  AI product mangers live and die by “Experiment Fast So You Don’t Break Things Later.”  Take any social media company that sells advertisements. The timing, quantity, and type of ads displayed to segments of a company’s user population are overwhelmingly determined by algorithms. Customers contract with the social media company for a certain fixed budget, expecting to achieve certain audience exposure thresholds that can be measured by relevant business metrics. The budget that is actually spent successfully is referred to as fulfillment, and is directly related to the revenue that each customer generates. Any change to the underlying models or data ecosystem, such as how certain demographic features are weighted, can have a dramatic impact on the social media company’s revenue. Experimenting with new models is essential–but so is yanking an underperforming model out of production. This is only one example of why rapid prototyping is important for teams building AI products. AI PMs must create an environment in which continuous experimentation and failure are allowed (even celebrated), along with supporting the processes and tools that enable experimentation and learning through failure. In a previous section, we introduced the importance of user research and interface design.  Qualitative data collection tools (such as SurveyMonkey, Qualtrics, and Google Forms) should be joined with interface prototyping tools (such as Invision and Balsamiq), and with data prototyping tools (such as Jupyter Notebooks) to form an ecosystem for product development and testing. Once such an environment exists, it’s important for the product manager to codify what constitutes a “minimum viable” AI product (MVP). This product should be robust enough to be used for user research and quantitative (model evaluation) experimentation, but simple enough that it can be quickly discarded or adjusted in favor of new iterations. And, while the word “minimum” is important, don’t forget “viable.” An MVP needs to be a product that can stand on its own, something that customers will want and use. If the product isn’t “viable” (i.e., if a user wouldn’t want it) you won’t be able to conduct good user research. Again, it’s important to listen to data scientists, data engineers, software developers, and design team members when deciding on the MVP. Data Quality and Standardization In most organizations, Data Quality is either an engineering or IT problem; it is rarely addressed by the product team until it blocks a downstream process or project. This relationship is impossible for teams developing AI products. “Garbage in, garbage out” holds true for AI, so good AI PMs must concern themselves with data health. There are many excellent resources on data quality and data governance. The specifics are outside the scope of this article, but here are some core principles that should be incorporated into any product manager’s toolkit: Augmenting AI Product Management with Technical Leadership There is no intuitive way to predict what will work best in AI product development. AI PMs can build amazing things, but this often comes largely from the right frameworks rather than the correct tactical actions. Many new tech capabilities have the potential to enable software engineering using ML/AI techniques more quickly and accurately. AI PMs will need to leverage new and emerging AI techniques (image upscaling, synthetic text generation using adversarial networks, reinforcement learning, and more), and partner with expert technologists to put these tools to use. It’s unlikely that every AI PM will have world-class technical intuition in addition to excellent product sense, UI/X experience, customer knowledge, leadership skills, and so on. But don’t let that breed pessimism. Since one person can’t be an expert at everything, AI PMs need to form a partnership with a technology leader (e.g., a Technical Lead or Lead Scientist) who knows the state of the art and is familiar with current research, and trust that tech leader’s educated intuition. Finding this critical technical partner can be difficult, especially in today’s competitive talent market.  However, all is not lost:  there are many excellent technical product leaders out there masquerading as competent engineering managers. Product manager Matt Brandwein suggests observing what potential tech leads do in their idle time, and taking note of which domains they find attractive. Someone’s current role often doesn’t reveal where their interests and talent lie. Most importantly, the AI PM should look for a tech lead who can mitigate their own weaknesses. For example, if the AI PM is a visionary, picking a technical lead with operational experience is a good idea. When a product is ready to ship, the PM will work with user research and engineering teams to develop a release plan that collects both qualitative and quantitative user feedback. The bulk of this data will be concentrated on user interaction with the user interface and front end of the product. AI PMs must also plan to collect data about the “hidden” functionality of the AI product, the part no user ever sees directly: model performance. We’ve discussed the need for proper instrumentation at both the model and business levels to gauge the product’s effectiveness; this is where all of that planning and hard work pays off! On the model side, performance metrics that were validated during development (predictive power, model fit, precision) must be constantly re-evaluated as the model is exposed to more and more unseen data. A/B testing, which is frequently used in web-based software development, is useful for evaluating model performance in production. Most companies already have a framework for A/B testing in their release process, but some may need to invest in testing infrastructure. Such investments are well worth it. It’s inevitable that the model will require adjustments over time, so AI PMs must ensure that whoever is responsible for the product post-launch has access to the development team in order to investigate and resolve issues. Here, A/B testing has another benefit: the ability to run champion/challenger model evaluations. This framework allows for a deployed model to run uninterrupted, while a second model is evaluated against a sample of the total population. If the second model outperforms the original, it can simply be swapped out-often without any downtime! Overall, AI PMs should remain closely involved in the early release lifecycle for AI products, taking responsibility for coordinating and managing A/B tests and user data collection, and resolving issues with the product’s functionality.  In this article, we’ve focused primarily on the AI product development process, and mapping the AI product manager’s responsibilities to each stage of that process. As with many other digital product development cycles, AI PMs must first ensure that the problem to be solved is both a problem that ML/AI can solve and a problem that is vital to the business. Once this criteria has been met, the AI PM must consider whether the product should be developed, considering the myriad of technical and ethical considerations at play when developing and releasing a production AI system. We propose the AI Product Development Process as a blueprint for AI PMs of all industries, who may develop myriad different AI products. Though this process is by no means exhaustive, it emphasizes the kind of critical thinking and cross-departmental collaboration necessary to success at each stage of the AI product lifecycle. However, regardless of the process you use, experimentation is the key to success.  We’ve said that repeatedly, and we aren’t tired: the more experiments you can do, the more likely you are to build a product that works (i.e., positively impacts metrics the company cares about).  And don’t forget qualitative metrics that help you understand user behavior! Once an AI system is released and in use, however, the AI PM has a somewhat unique role in product maintenance. Unlike PMs for many other software products, AI PMs must ensure that robust testing frameworks are built and utilized not only during the development process, but also in post-production. Our next article focuses on perhaps the most important phase of the AI product lifecycle: maintenance and debugging.\n",
            "----------------\n",
            "A recent article in The Verge discussed PULSE, an algorithm for “upsampling” digital images. PULSE, when applied to a low-resolution image of Barack Obama, recreated a White man’s face; applied to Alexandria Ocasio-Cortez, it built a White woman’s face.  It had similar problems with other images of Black and Hispanic people, frequently giving them White skin and facial features.  PULSE could be used for applications like upsampling video for 8K ultra high-definition, but I’m less interested in the algorithm and its applications than in the discussion about ethics that it provoked. Is this just a problem with training data, as Yann LeCun said on Twitter?  Or is it a sign of larger systemic issues about bias and power, as Timnit Gebru argued? The claim that this is only a problem with the data is tempting, but it is important to step back and see the bigger issues: nothing is “just” a problem with data.  That shift to a wider perspective is badly needed. There’s no question that the training data was a problem.  If the algorithm were trained using a set of photos dominated by Black people, it would no doubt turn White faces into Black ones. With the right training set and training process, we could presumably minimize errors. When looked at this way, it’s largely a problem of mathematics and statistics. That’s the position that Timnit Gebru rejects, because it obscures the bigger issues hiding behind the training set. As organizations like Data For Black Lives, Black in AI, the Algorithmic Justice League, and others have been pointing out, it’s never just an issue of statistics. It’s an issue of harms and of power. Who stands to gain? Who stands to lose? That’s the point we really need to consider, particularly when we’re asking AI to create “information” where nothing existed before. Who controls the erasure, or the creation, of color? What are the assumptions that lie behind it? I do not believe there are many AI researchers giggling about turning Black people into Whites (though there are no doubt some). Nor do I believe there’s some kind of racist demon lurking in the mathematics implemented by neural networks. But errors like this nevertheless happen; they happen all too frequently; the results are often harmful; and none of us are surprised that the transition was Black->White rather than the other way around. We were not surprised when we found that products like COMPAS recommended tougher criminal sentences for Black people than for Whites; nor were we surprised when Timnit Gebru and Joy Buolamwini showed that facial recognition is much less accurate for Black people than White people, and particularly inaccurate for Black women. So, how do we think about the problem of power and race in AI? Timnit Gebru is right; saying that the problem is in the training data ignores the real problem.  As does being saddened and making vague promises about doing better in the future.  If we aren’t surprised, why?  What do we have to learn, and how do we put that learning into practice? We can start by considering what “biased training data” means. One of my favorite collections of essays about data is “Raw Data” is an Oxymoron. There is no such thing as “raw data,” and hence, no pure, unadulterated, unbiased data. Data is always historical and, as such, is the repository of historical bias. Data doesn’t just grow, like trees; data is collected, and the process of data collection often has its own agenda. Therefore, there are different ways of understanding data, different ways of telling stories about data–some of which account for its origin and relation to history, and some of which don’t. Take, for example, housing data. That data will show that, in most places in the US, Black people live in separate neighborhoods from White people. But there are a number of stories we can tell about that data.  Here are two very different stories: There are many variations on those stories, but those two are enough. Neither is entirely wrong—though the first story erases an important fact, that White people have generally had the power to prevent Black people from moving into their neighborhoods. The second story doesn’t treat that data as an intransigent given; it critiques the data, asks that data how and why it came to be.  As I’ve argued, AI is capable of revealing our biases, and showing us where they are hidden.  It gives us an opportunity to learn about and critique our own institutions.  If you don’t look critically at the data, its origins, and its stories (something that’s not a part of most computer science curricula), you’re likely to institutionalize the bias embedded in the data behind a wall of mathwashing. There are plenty of situations in which that critique is needed.  Here’s one: researchers looking at ride data from Chicago’s public data portal found that the dynamic pricing algorithms used by ride-hailing services (such as Uber and Lyft) charged more for rides to and from low-income, nonwhite areas. This effect might not have been discovered without machine learning.  It means that it’s time to audit the services themselves, and find out exactly why their algorithms behave this way. And it’s an opportunity to learn what stories the data is telling us. The real issue is which of those stories we choose to tell. I use the word “we” because the data doesn’t tell a story on its own, any more than a pixelated image of President Obama becomes a White man on its own. Someone chooses what story to tell; someone releases the software; and that someone is a person, not an algorithm. So if we really want to get to the bottom of the upsampling problem with PULSE, we need to be looking at people in addition to training data. If PULSE needed more images of Black people in its training set, why didn’t it have them? And why are we not surprised that these issues show up all the time, in applications ranging from COMPAS to the Google app that tagged Black people as gorillas? That’s really a question about the teams of people who are creating and testing this software. They are predominantly White and male. I admit that if I wrote a program that upsampled images, it might not occur to me to test a Black person’s face. Or to test whether jail sentences for White and Black people are comparable. Or to test whether a real estate application will recommend that Black people consider buying homes in largely White neighborhoods. These not-so-microaggressions are the substance from which greater abuses of power are made. And we’re more likely to discover those microaggressions in time to stop them if the teams developing the software include people with Black and Brown faces, as well as White ones. The problem isn’t limited to building teams that realize we need different training data, or that understand the need for testing against different kinds of bias.  We also need teams that can think about what applications should and shouldn’t be built. Machine learning is complicit in many power structures. Andrew Ng’s newsletter, The Batch, gives payday lending as an example. An application might compute the optimal interest rate to charge any customer, and that app might easily be “fair” by some mathematical standard–although even that is problematic. But the industry itself exists to take advantage of vulnerable, low-income people. In this situation, it is impossible for an algorithm—even a “fair” one—to be fair. Likewise, given the current power structures, along with the possibility for abuse, it is very difficult to imagine a face recognition application, no matter how accurate, that isn’t subject to abuse. Fairness isn’t a mathematical construct that can be embodied by an algorithm; it has everything to do with the systems in which the algorithm is embedded. The algorithms used to identify faces can also be used to identify bird species, detect diseased tomatoes on a farm, and the like. The ethical problem isn’t the algorithm, it’s the context and the power structures, and those are issues that most software teams aren’t used to thinking about. There’s an even better example close at hand: PULSE itself.  Obscuring faces by replacing them with low-resolution, pixelated images is a classic way of protecting the identity of the person in the photograph.  It’s something people do to protect themselves–a particularly important issue in these days of incognito armies. Software like PULSE, even (especially) if it is trained correctly, undoes individuals’ efforts to protect their own privacy.  It tips the power relationship even further in the direction of the empowered. And an application like Stanford’s #BlackLivesMatter PrivacyBot tips the balance back the other way. There are many ways to address these issues of bias, fairness, and power, but they all start with building inclusive teams, and with taking a step back to look at the bigger issues involved. You are more likely to detect bias if there are people on the team who have been victims of bias. You are more likely to think about the abuse of power if the team includes people who have been abused by power. And, as I’ve argued elsewhere, the job of “programming” is becoming less about writing code, and more about understanding the nature of the problem to be solved. In the future, machines will write a lot of code for us. Our task will be deciding what that software should do—not putting our heads down and grinding out lines of code. And that task isn’t going to go well if our teams are monochromatic.\n",
            "----------------\n",
            "‘Tech’, of course, has all of this complexity, but we’re having to work this out a lot more quickly. It took 75 years for seatbelts to become compulsory, but tech has gone from interesting to crucial only in the last five to ten years. That speed means we have to form opinions about things we didn’t grow up with and don’t always understand quite so well as, say, supermarkets. uses extreme ultraviolet (EUV) light at a wavelength of 13.5 nm to make silicon features down to a few nanometers in size. While quantum computing has garnered most of the recent headlines, quantum networking—especially with its promise of secure communication—actually is capturing the interest of a growing community across science, industry, and national security. Today, many people recognize that building and scaling quantum-protected and enhanced communication networks are among the most important technological frontiers of the 21st century. Although a general-purpose quantum computer still is many years away, the research community perceives a quantum Internet may be closer to realization.\n",
            "----------------\n",
            "Taiwan and Audrey Tang occupy a unique spot in a world, where the ascendance of the internet and digital technology is marked by the twin dystopias of “post-truth” information chaos in the United States and China’s totalitarian, technologically mediated surveillance-and-censorship regime. With Audrey Tang as the symbolic figurehead, the island nation is making the radical argument that digital tools can be effectively used to build stronger, more open, more accountable democracies. Whether the challenge is fighting disinformation campaigns orchestrated by hostile powers or the existential threat of a virus run amok or simply figuring out how to regulate Uber, Taiwan is demonstrating the best ways technology can be used to marry the energy and talents of civil society with the administrative powers of government bureaucracy. we found that the employees who averaged the most weekly one-on-one time with their managers experienced the smallest increase in working hours  if you have a fleet of healthy services written in a single application stack, then it’s a good idea to think twice before introducing a service mesh. By simply introducing or evolving a shared RPC library, you’ll get the exact same benefits and avoid dealing with the downsides of maintaining service meshes.\n",
            "----------------\n",
            "why aren’t we — ostensibly the people writing software — doing more with AI in our day-to-day? Why are things like TabNine and Kite so often seen as curiosities instead of game-changers? If you take seriously the idea that ai will fundamentally change the nature of many occupations in the coming decade, what reason do you have to believe that you’ll be immune from that because you work in software? Looking at the code you’ve been paid to write over the past few years, how much of that can you honestly say is truly novel? The truth is, I’ve come around to thinking that programming isn’t the most important thing for programmers to pay attention to right now. represent our Domain Model declaratively, as an in-program data structure (a ‘meta-database’); derive the ‘machine’ behaviour generically from this representation. We have developed a new methodology that retains the ease-of-use, familiarity, and (some of) the free-form nature of informal methods, while benefiting from the rigor, structure, and potential for automation characteristic of formal methods. Our approach aims to foster thoughtful and timely analysis through the introduction of structure, and collaboration through access to the corporate memory of current and past analytic results. This article covers the outcomes of research performed in 2019 on how engineers at Google debug production issues, including the types of tools, high-level strategies, and low-level tasks that engineers use in varying combinations to debug effectively. It examines the research approach used to capture data, summarizing the common engineering journeys for production investigations and sharing examples of how experts debug complex distributed systems. Finally, the article extends the Google specifics of this research to provide some practical strategies that you can apply in your organization.\n",
            "----------------\n",
            "Our diff solution involves embedding each line into a low dimensional vector and (optionally “fine-tuning” or updating the embedding model at the same time), assigning it to a cluster, and identifying lines in different clusters as “different”. Locality sensitive hashing is a probabilistic algorithm that permits constant time cluster assignment and near-constant time nearest neighbors search. Good leaders can walk into a situation where people have lost track of their goals and get everyone aligned on a clear path forward. They remove unimportant details, distill complex situations to their essence, and get the right decision-maker to make a call – even if it’s not them. They’re able to not only stop bad plans before it’s too late, but get them moving again in the right direction.\n",
            "----------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2F1pZJdXVwJK"
      },
      "source": [
        "PATH = '/content/rss_posts/'\n",
        "\n",
        "DOC_PATTERN = r'.*\\.txt'\n",
        "corpus = PlaintextCorpusReader(PATH, DOC_PATTERN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJI6AbGA4JIg"
      },
      "source": [
        "### Iterate through the fileids in the corpus, extract the raw text of each document, and store them in a list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbhHF74sdD8M"
      },
      "source": [
        "docs = [corpus.raw(fileid) for fileid in corpus.fileids()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOZU1dUoeKE-",
        "outputId": "27a880e1-08c6-4e21-842d-7798713c8a73"
      },
      "source": [
        "print(docs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['2020 has been a year of great challenges for so many, but it’s not all negative. Around the world, organizations and their workforces have risen to the occasion, recognizing the importance of expanding their knowledge, taking on new tasks, and bettering themselves both personally and professionally. With the uptick in virtual conferencing, remote work, and, for some, reentering the job market, new technology adoption was accelerated, driving the workforce to build new skills. While 2020 was the year of the global COVID-19 pandemic, it will also be commemorated as the year online learning prevailed. As vaccine development persists and life gets back to normal, with it will come a more future-proof workforce ready to share their new knowledge with the world.\\xa0 Since the onset of the pandemic, online courses and programs have seen dramatic spikes in consumption and enrollment, and O’Reilly has been no different. A big contributor to O’Reilly’s continued success during these unprecedented times has been its live virtual training courses. This year, more than 900,000 users have registered for live events through O’Reilly online learning—a 96% increase from last year. This functionality also allowed O’Reilly to introduce its Superstream Series, a new lineup of virtual conferences featuring expert speakers delivering talks and training sessions on the most important topics and emerging trends in technology.\\xa0 So what are the trends driving this uptick in learning? Companies are increasingly interested in understanding how to successfully adjust to remote work and effectively manage time. And individual O’Reilly members are looking to build and expand on their technical skills in everything from software architecture and microservices to AI and programming languages. But which topics are the brightest minds in technology most focused on? We’ve compiled the top 20 live online training courses of 2020 to shed some light on what those in the know want to know. Top 20 live online training courses of 2020 For a more in-depth analysis of the hot technology topics of 2020, based on data from O’Reilly online learning, stay tuned for our upcoming report, Wrapping Up 2020 (and What to Expect for 2021): Trends on O’Reilly online learning. ', 'It has long seemed to me that functional programming is, essentially, programming viewed as mathematics. Many ideas in functional programming came from Alonzo Church’s Lambda Calculus, which significantly predates anything that looks remotely like a modern computer. Though the actual history of computing runs differently: in the early days of computing, Von Neumann’s ideas were more important than Church’s, and had a tremendous influence on the design of early computers—an influence that continues to the present. Von Neumann’s thinking was essentially imperative: a program is a list of commands that run on a machine designed to execute those commands.\\xa0 So, what does it mean to say that functional programming is programming “viewed as mathematics”? Von Neumann was a “mathematician,” and programming of all kinds found its first home in Mathematics departments. So, if functional programming is mathematical, what does that mean? What kind of math? I’m not thinking of any specific branch of mathematics. Yes, the Lambda Calculus has significant ties to set theory, logic, category theory, and many other branches of mathematics. But let’s start with grade school mathematics and assignment statements; they’re basic to any programming language. We’re all familiar with code like this: Mathematically, this is nonsense. An equation is a statement about a relationship that holds true. i can equal i; it can’t equal i+1. And while i++ and i+=1 no longer look like equations, they are equally nonsensical; once you’ve said that i equals something, you can’t say it equals something else. “Variables” don’t change values; they’re immutable. Immutability is one of the most important principles of functional programming. Once you’ve defined a variable, you can’t change it. (You can create a new one in a different function scope, but that’s a different matter.) Variables, in functional programming, are invariant; and that’s important. You may be wondering “what about loops? How can I write a for loop?” Not only do you have to do without index variables, you can’t modify any of the variables in the loop body.\\xa0 Setting aside the (solvable) problem of iteration, there’s no reason you can’t write code in (almost) any non-functional language that has this same effect. Just declare all your variables final or const. In the long run, functional programming is more about a specific kind of discipline than about language features. Programming languages can enforce certain rules, but in just about any modern language it’s possible to follow those rules without language support. Another important principle of functional programming is that functions are “first class entities.” That is, there are minimal restrictions about where you can use a function. You can also have functions without names, often called “lambdas” (which refers directly to the Lambda Calculus, in which functions were unnamed).\\xa0 In Python, you can write code like this: The “key” is an anonymous function that returns a specific column of an array; that function is then used for sorting. Personally, I’m not overly fond of “anonymous functions”; it’s often clearer to write the anonymous function as a regular, named function. So I might write this: The ability to use functions as arguments to functions gives you a very nice way to implement the “strategy pattern”: I often get the sense that all programmers really want from functional programming is first-class functions and lambdas. Lambdas were added to Python very early on (1.0) but didn’t reach Java until Java 8.\\xa0 Another consequence of thinking mathematically (and possibly a more important one) is that functions can’t have side-effects and, given the same arguments, will always return the same value. If a mathematician (or a high school trig student) writes they don’t have to deal with the possibility that sin(x) sets some global variable to 42, or will return a different value every time it’s called. That just can’t happen; in math, the idea of a “side-effect” is meaningless. All the information that sin(x) provides is encapsulated in the return value. In most programming languages, side-effects happen all too easily, and in some, they’re almost an obsession. Again, creating functions that have no side-effects is a matter of exercising discipline. A programming language can enforce this rule, but you can follow it whether or not your language makes you do it. We don’t have cartoon devils looking over our shoulders saying “Go ahead; make a side effect. No one will notice.” Functional languages vary the degree to which they enforce the lack of side-effects. If you’re a purist, anything that interacts with the real world is a side-effect. Printing a document? Changing a row in a database? Displaying a value on the user’s screen? Those are all side-effects (they aren’t completely encapsulated in the value returned by the function), and they have to be “hidden” using a mechanism like monads in Haskell. And that’s the point at which many programmers get confused and throw up their hands in despair. (I’ll only point you to Real World Haskell.) In both Java and Python, lambda functions can have side-effects, which means that, strictly speaking, they aren’t really “functional.” Guido van Rossum’s discussion of the addition of Lambdas to Python is worth reading; among other things, he\\xa0says\\xa0“I have never considered Python to be heavily influenced by functional languages, no matter what people say or think.” Streams are often associated with functional languages; they’re essentially long (perhaps infinite) lists that are evaluated lazily—meaning that elements of the string are only evaluated as they’re needed. Maps apply a function to every element of a list, returning a new list—and that includes streams, which (for these purposes) are specialized lists. That’s an incredibly useful feature; it’s a great way to write a loop without having to write a loop—and without even knowing how much data you have. You can also create “filters” that choose whether to pass any element of the stream to the output, and you can chain maps and filters together. If you think this sounds like a Unix pipeline, you’re right. Streams, maps, filters, and the act of chaining them together really have as much to do with the Unix shell as they do with functional languages. Another way to avoid writing loops is to use “comprehensions,” a feature of Python. It’s easy to get very fond of list comprehensions; they’re compact, they eliminate off-by-one errors, and they’re very flexible. Although comprehensions look like a compact notation for a traditional loop, they really come from set theory—and their closest computational “relatives” are to be found in relational databases, rather than functional programming. Here’s a comprehension that applies a function to every element of a list: The most general way to avoid traditional loops is to use recursion: a function that calls itself. Here’s the recursive equivalent to the previous comprehension: Recursion is a mainstay of functional languages: you don’t have indices being modified, and you’re not even modifying the resulting list (assuming that append doesn’t count as modification).\\xa0 However, recursion has its own problems. It’s hard to wrap your mind around recursion; you still need to do a lot of your own bookkeeping (in this case, passing in a vector so a result can be returned); and except in one (common) special case, called “tail recursion,” it can be a performance nightmare. I started by saying that functional programming was programming considered as “math,” and that’s at least partially correct. But is that claim useful? There are many branches of mathematics that map onto programming concepts in different ways. Functional programming only represents one of them. If you’re a topologist, you may well like graph databases. But discussing which branch of mathematics corresponds to which programming practices isn’t really helpful. Remembering high school algebra may help when thinking about immutability, statelessness, and the absence of side-effects; but most programmers will never study the real mathematical origins of functional programing. Lambdas are great; functions as arguments in method calls is great; even recursion is (sometimes) great; but we’re fooling ourselves if we think programmers are going to start using Java as if it were Haskell. But that’s OK; for Java programmers, the value of Lambdas isn’t some mathematical notion of “functional,” but in providing a huge improvement over anonymous inner classes. The tools to be functional are there, should you choose to use them. In college, I learned that engineering was about making tradeoffs. Since then, I’ve heard very few programmers talk about tradeoffs—but those tradeoffs are still central to good engineering. And while engineering uses a lot of mathematics, engineering isn’t mathematics, in part because mathematics doesn’t deal in tradeoffs. Using “mathematics” as a way to think about a particular style of disciplined coding maybe be useful, particularly if that discipline leads to fewer bugs. It’s also useful to use the tools of mathematics to make good tradeoffs between rigor, performance, and practicality—which may lead you in an entirely different direction. Be as functional as you need to (but no more).\\xa0', '1. Basic Processor & Memory hierarchy; 2. Advanced Out-of-Order Processor; 3. Data-parallel processors; 4. Micro-controller introduction; 5. Multicore;  6. RISC-V core; 7. Advanced Multicore; 8. Multicore programming; 9. Graphics Processing Unit (GPU); 10. Heterogeneous SoC; 11. GPU Programming; 12. Application-Specific Instruction-Set Processor (ASIP); 13 PULP: Parallel Ultra-Low-Power Computing; 14. Architecture in the Future – Wrap-up Next-generation reliable, safe, concise, and functional-first programming language.\\nFlix is a principled and flexible functional-, logic-, and imperative- programming language that takes inspiration from F#, Go, OCaml, Haskell, Rust, and Scala. Flix looks like Scala, but its type system is closer to that of OCaml and Haskell. Its concurrency model is inspired by Go-style processes and channels. Flix compiles to JVM bytecode, runs on the Java Virtual Machine, and supports full tail call elimination. supports first-class Datalog constraints enriched with lattice semantics. Abundance, connectivity, healthspan, capital, AR and Spatial Web, smart devices, human-level AI, AI-Human collaboration, software shells, renewable energy, insurance industry switches to prevention, autonomous vehicles and flying cars, on-demand production and delivery, knowledge, advertising, cellular agriculture, brain-computer interfaces, VR, sustainability/environment, and CRISPR. Level -2: No Authentication; Level -1: All Passwords = “password”; Level 0: Hardcode Everywhere; Level +1: Move Secrets into a Config File; Level +2: Encrypt the Config File; Level +3: Use a Secret Manager; Level +4: Dynamic Ephemeral Credentials', 'The programming world used to be split into functional languages, object-oriented languages, and everything else (mostly procedural languages). One “was” a functional programmer (at least as a hobby) writing Lisp, Haskell, or Erlang; or one “was” an OO programmer (at least professionally), writing code in Java or C++.\\xa0 (One never called oneself a “procedural programmer”; when these names escaped from academia in the 1990s, calling yourself a “procedural programmer” would be akin to wearing wide ties and bell-bottom jeans.) But this world has been changing. Over the past two decades, we’ve seen the rise of hybrid programming languages that combine both functional and object-oriented features. Some of these languages (like Scala) were multi-paradigm from the beginning. Others, like Python (in the transition from Python 2 to 3) or Java (with the introduction of Lambdas in Java 8) are object-oriented or procedural languages to which functional features were added. Although we think of C++ as an object-oriented language, it has also been multi-paradigm from the beginning. It started with C, a procedural language, and added object-oriented features. Later, beginning with the Standard Template Library, C++ was influenced by many ideas from Scheme, a descendant of LISP.\\xa0 JavaScript was also heavily influenced by Scheme, and popularized the idea of anonymous functions and functions as first class objects. And JavaScript was object-oriented from the start, with a prototype-based object model and syntax (though not semantics) that gradually evolved to become similar to Java’s. We’ve also seen the rise of languages combining static and dynamic typing (TypeScript in the JavaScript world; the addition of optional type hinting in Python 3.5; Rust has some limited dynamic typing features). Typing is another dimension in paradigm space. Dynamic typing leads to languages that make programming fun and where it’s easy to be productive, while strict typing makes it significantly easier to build, understand, and debug large systems. It’s always been easy to find people praising dynamic languages, but, except for a few years in the late 00s, the dynamic-static paradigmatic hasn’t attracted as much attention. Why do we still see holy wars between advocates of functional and object-oriented programming? That strikes me as a huge missed opportunity. What might “multi-paradigm programming” mean? What would it mean to reject purity and use whatever set of features provide the best solution in any given context? Most significant software is substantial enough that it certainly has components where an object-oriented paradigm makes more sense, and components where a functional paradigm is superior.\\xa0 For example, look at a “functional” feature like recursion.\\xa0 There are certainly algorithms that make much more sense recursively (Towers of Hanoi, or printing a sorted binary tree in order); there are algorithms where it doesn’t make much of a difference whether you use loops or recursion (whenever tail recursion optimizations will work); and there are certainly cases where recursion will be slow and memory-hungry. How many programmers know which solution is best in any situation? These are the sort of questions we need to start asking. Design patterns have been associated with object-oriented programming from the beginning. What kinds of design patterns make sense in a multi-paradigm world? Remember that design patterns aren’t “invented”; they’re observed, they’re solutions to problems that show up again and again, and that should become part of your repertoire. It’s unfortunate that functional programmers tend not to talk about design patterns; when you realize that patterns are observed solutions, statements like “patterns aren’t needed in functional languages” cease to make sense. Functional programmers certainly solve problems, and certainly see the same solutions show up repeatedly. We shouldn’t expect those problems and solutions to be the same problems and solutions that OO programmers observe. What patterns yield the best of both paradigms? What patterns might help to determine which approach is most appropriate in a given situation? Programming languages represent ways of thinking about problems. Over the years, the paradigms have multiplied, along with the problems we’re interested in solving. We now talk about event-driven programming, and many software systems are event-driven, at least on the front end. Metaprogramming was popularized by JUnit, the first widely used tool to rely on this feature that’s more often associated with functional languages; since then, several drastically different versions of metaprogramming have made new things possible in Java, Ruby, and other languages. We’ve never really addressed the problem of how to make these paradigms play well together; so far, languages that support multiple paradigms have left it to the programmers to figure out how to use them. But simply mixing paradigms ad hoc probably isn’t the ideal way to build large systems–and we’re now building software at scales and speeds that were hard to imagine only a few years ago. Our tools have improved; now we need to learn how to use them well. And that will inevitably involve blending paradigms that we’ve long viewed as distinct, or even in conflict.  Thanks to Kevlin Henney for ideas and suggestions!', 'Would the mental focus on a specific hypothesis prevent us from making a discovery? To test this, we made up a dataset and asked students to analyze it. […] The most notable “discovery” in the dataset was that if you simply plotted the number of steps versus the BMI, you would see an image of a gorilla waving at you (Fig. 1b). my issue was the fact that the systems doing the flashing were running the yocto images and perl and the guy writing the perl was also responsible for writing the thing that actually updates the car. that thing (the car-side updater) is about ~100k lines of C in a single file. code reviews were always a laugh riot.', 'system security starts at the hardware layer free home server for your comics and ebooks library a relational database management system developed by the Carnegie Mellon Database Group. The research goal of the NoisePage project is to develop high-performance system components that support autonomous operation and optimization as a first-class design principle.', 'Second-tier Scottish football club Inverness Caledonian Thistle doesn’t have a camera operator for matches at their stadium so the club uses an AI-controlled camera that’s programmed to follow the ball for their broadcasts. But in a recent match against Ayr United, the AI controller kept moving the camera off the ball to focus on the bald head of the linesman, making the match all but unwatchable. No fans allowed in the stadium either, so the broadcast was the only way to watch. Basically at midnight at the end of 1927, the clocks went back 5 minutes and 52 seconds. On average, UX improvements have substantially decreased since 2006–2008: from 247% to 75% (a 69% decrease). This difference is statistically significant (p = 0.01) — we can be quite confident that average improvement scores are lower now than they were 12–14 years ago. This hands-on course explores a selection of techniques from Programming Languages and Human-Computer Interaction that can help us create useful, usable programming languages and programming tools. We will cover strategies for designing programming systems—e.g., need finding, formative studies, user-centered design broadly. We will also cover tools and techniques that help us build user-friendly programming systems—e.g., program synthesis, structure editors, abstraction design, program slicing. For the final project, individuals or teams will develop a usable abstraction, language, or programming tool of their own design.', 'Perhaps the most important event this month isn’t technical, but the start of the US Justice Dept.’s lawsuit against Google. That will certainly play out over years rather than months, but it’s significance is less about this particular case than the idea that legal and regulatory systems will play a large role in the evolution of technology in the US. In the short term, it’s worth watching the CPPA, GDPR, California’s Props 22 and 24, and FCC interference with social media’s enforcement of rules around community behavior.\\xa0 Long term, this is only the beginning.', 'in this paper, we semi-automatically learn error-inducing patterns from a corpus of common Java coding errors and from changes that caused operational anomalies at Facebook specifically. We combine the mutations with instrumentation that measures which tests exactly visited the mutated piece of code. Results on more than 15,000 generated mutants show that more than half of the generated mutants survive Facebook’s rigorous test suite of unit, integration, and system tests. is the companion tutorial for the paper “Algorithms for Causal Reasoning in Probability trees” by Genewein T. et al. (2020). Probability trees are one of the simplest models of causal generative processes.They possess clean semantics and are strictly more general than causal Bayesian networks, being able to e.g. represent causal relations that causal Bayesian networks can’t. Even so, they have received little attention from the AI and ML community. In this tutorial we present new algorithms for causal reasoning in discrete probability trees that cover the entire causal hierarchy (association, intervention, and counterfactuals), operating on arbitrary logical and causal events. a method in occupational safety for avoiding mistakes by pointing at important indicators and calling out the status. reduce the imperative shell and move code into the functional core', 'In this paper, we investigate “split-second phantom attacks,” a scientific gap that causes two commercial advanced driver-assistance systems (ADASs), Telsa Model X (HW 2.5 and HW 3) and Mobileye 630, to treat a depthless object that appears for a few milliseconds as a real obstacle/object. 1. Prefer running your system in the cloud over local emulation. 2. CI/CD Pipelines are not enough, local deployment automation is crucial. 3. For AWS Serverless & “Function-as-a-Service” – monolithic functions are OK. 4. Consider adopting a monorepo, with tooling appropriate for monorepos. 5. Implement all three pillars of observability. I reverse engineered mcdonald’s internal api and I’m currently placing an order worth $18,752 every minute at every mcdonald’s in the US to figure out which locations have a broken ice cream machine.', '“On peut interroger n’importe qui, dans n’importe quel état; ce sont rarement les réponses qui apportent la vérité, mais l’enchaînement des questions.““You can interrogate anyone, no matter what their state of being.\\xa0 It’s rarely their answers that unveil the truth, but the sequence of questions that you have to ask.“–\\xa0 Inspector Pastor in La Fée Carabine, by Daniel Pennac The authors’ jobs all involve asking questions.\\xa0 A lot of questions. We do so out of genuine curiosity as well as professional necessity: Q is an ML/AI consultant, Chris is a product manager in the AI space, and Shane is an attorney.\\xa0 While we approach our questions from different angles because of our different roles,\\xa0 we all have the same goal in mind: we want to elicit truth and get people working with us to dig deeper into an issue. Preferably before things get out of hand, but sometimes precisely because they have. A recent discussion led us down the path of our favorite questions: what they are, why they’re useful, and when they don’t work so well.\\xa0 We then each chose our top three questions, which we’ve detailed in this article. We hope you’re able to borrow questions you haven’t used before, and even cook up new questions that are more closely related to your personal and professional interests. Before we get too far, let’s explore what we mean by a “good question.” For one, it’s broad and open-ended.\\xa0 It’s a lot less “did this happen?” and more “what happened?”\\xa0 It encourages people to share their thoughts and go deep. There’s an implied “tell me more” in an open-ended question.\\xa0 Follow it with silence, and (as any professional interrogator will tell you) people will fill in extra details.\\xa0They will get to what happened, along with when and how and why.\\xa0 They will tell a full story, which may then lead to more questions, which branch into other stories. All of this fills in more pieces to the puzzle.\\xa0 Sometimes, it sheds light on parts of the puzzle you didn’t know existed. By comparison, yes/no questions implicitly demand nothing more than what was expressly asked.\\xa0 That makes them too easy to dodge. Two, a good question challenges the person asking it as much as (if not more than) the person who is expected to answer.\\xa0 Anyone can toss out questions at random, in an attempt to fill the silence. To pose useful questions requires that you first understand the present situation, know where you want to wind up, and map out stepping-stones between the two. Case in point: the Daniel Pennac line that opened this piece was uttered by a detective who was “interviewing” a person in a coma.\\xa0 As he inspected their wounds, he asked more questions to\\xa0 explore their backstory, and that helped him to piece together his next steps of the investigation.\\xa0 Perhaps Inspector Pennac was inspired by Georg Cantor, who once said: “To ask the right question is harder than to answer it.” Three, a good question doesn’t always have a right answer.\\xa0 Some of them don’t have any answer at all.\\xa0 And that’s fine. Sometimes the goal of asking a question is to break the ice on a topic, opening a discussion that paints a larger picture. Four, sometimes a question is effective precisely because it comes from an unexpected place or person.\\xa0While writing this piece, one author pointed out (spoiler alert) that the attorney asked all of the technical questions, which seems odd, until you realize that he’s had to ask those because other people did not. When questions seem to come out of nowhere—but they are really born of experience—they can shake people out of the fog of status quo and open their eyes to new thoughts. The opinions presented here are personal, do not reflect the view of our employers, and are not professional product, consulting, or legal advice. Source: Q The backstory: This is the kind of question you sometimes have to ask three times.\\xa0The first time, someone will try to hand you the company’s mission statement or slogan.\\xa0The second time, they’ll provide a description of the company: industry vertical, size, and revenue.\\xa0So you ask again, this time with an emphasis on the really.\\xa0And then you wait for the question to sink in, and for the person to work backwards from all of the company’s disparate activities to see what it’s all truly for.\\xa0Which will be somewhere between the raison d’etre and the sine qua non. Taking the time to work this out is like building a mathematical model: if you understand what a company truly does, you don’t just get a better understanding of the present, but you can also predict the future.\\xa0It guides decisions such as what projects to implement, what competitors to buy, and whom to hire into certain roles. As a concrete example, take Amazon.\\xa0Everyone thinks it’s a store. It has a store, but at its core, Amazon is a delivery/logistics powerhouse.\\xa0 Everything they do has to end with your purchases winding up in your hot little hands. Nothing else they do matters—not the slick website, not the voice-activated ordering, not the recommendation engine—unless they get delivery and logistics down. How I use it: I explore this early in a consulting relationship.\\xa0Sometimes even early in the sales cycle. And I don’t try to hide it; I’ll ask it, flat-out, and wait for people to fill the silence. Why it’s useful: My work focuses on helping companies to start, restart, and assess their ML/AI efforts. Understanding the company’s true purpose unlocks the business model and sheds light on what is useful to do with the data. As a bonus, it can also highlight cases of conflict.\\xa0Because sometimes key figures have very different ideas of what the company is and what it should do next. When it doesn’t work so well: This question can catch people off-guard.\\xa0 Since I work in the AI space, people sometimes have a preconceived notion that I’ll only talk about data and models.\\xa0 Hearing this question from an ostensibly technical person can be jarring… though, sometimes, that can actually help the conversation along.\\xa0 So it’s definitely a double-edged sword. Source: Chris The backstory: Ideation is about coming up with the “best” ideas. What is the best way to solve this problem? What is the most important? What is best for the business? The problem with “best” is that it is tied up with all of the biases and assumptions someone already has. To get to what really matters we have to understand the edge of what is good or bad. The gray area between those tells you the shape of the problem. Half the time this question will give you real, bad ideas.\\xa0 What has been surprising to me is that the other half of the time, the so-called “bad” idea is really a “good” idea in disguise.\\xa0 You just have to relax certain assumptions. Often these assumptions were just set at some point without a reason or much to back it up. How I use it: I like to ask this after going through a lot of the “best” questions in an ideation session. It can be adapted to focus on different types of “bad,” like “stupid,” “wasteful,” and “unethical.”\\xa0 Ask follow up questions about why they believe the idea is “bad” and why it might actually be “good.” Why it’s useful: How can you truly know what is good without also knowing what is bad? When it doesn’t work so well: When I was a design consultant working for clients in highly regulated industries (.e.g banking, insurance, etc.), I found this can be a difficult question to ask. In those cases you will need to get your legal team to either grant the attorney/client privilege to ask the questions, or ask the prompt/response in such a way that it protects people in the conversation. Source: Shane The backstory: In the early days of ML training data, companies and research teams frequently used “some stuff we found on the Internet” as a source for training data. This approach has two problems: (1) there may not be an appropriate license attached to the data, and (2) the data may not be a good representative sample for the intended use. It’s worth noting that the first issue is not just limited to images collected from the Internet. In recent years a number of research datasets (including Stanford’s Brainwash, Microsoft’s MS Celeb, and Duke’s MTMC) were withdrawn for reasons including a lack of clarity around the permission and rights granted by people appearing in the datasets. More recently, at least one company has earned itself significant PR and legal controversy for collecting training data sources from social media platforms under circumstances that were at least arguably a violation of both the platform’s terms of service and platform users’ legal rights.\\xa0 The safest course of action is also the slowest and most expensive: obtain your training data as part of a collection strategy that includes efforts to obtain the correct representative sample under an explicit license for use as training data. The next best approach is to use existing data collected under broad licensing rights that include use as training data even if that use was not the explicit purpose of the collection. How I use it: I like to ask this as early as possible.\\xa0 You don’t want to invest your time, effort, and money building models only to later realize that you can’t use them, or that using them will be much more expensive than anticipated because of unexpected licenses or royalty payments. It’s also a good indirect measure of training data quality: a team that does not know where their data originated is likely to not know other important details about the data as well. Why it’s useful: No matter how the data is collected, a review by legal counsel before starting a project—and allow me to emphasise the word before—can prevent significant downstream headaches. When it doesn’t work so well:\\xa0 This question is most useful when asked before the model goes into production. It loses value once the model is on sale or in service, particularly if it is embedded in a hardware device that can’t be easily updated. Source: Shane The backstory: One of the most interesting aspects of machine learning (ML) is its very broad applicability across a variety of industries and use cases. ML can be used to identify cats in photos as well as to guide autonomous vehicles. Understandably, the potential harm caused by showing a customer a dog when they expected to see a cat is significantly different from the potential harm caused by an autonomous driving model failing to properly recognize a stop sign.\\xa0 Determining the risk profile of a given model requires a case-by-case evaluation but it can be useful to think of the failure risk in three broad categories: How I use it: I use this question to determine both the potential risk from an individual failure and the potential aggregate risk from a systemic failure.\\xa0 It also feeds back into my question about training data: some relatively minor potential harms are worth additional investment in training data and testing if they could inconvenience millions, or billions, of users or create a significant negative PR cycle for a company. Why it’s useful: This is the sort of question that gets people thinking about the importance of their model in the overall business. It can also be a helpful guide that companies invest in such a model, and the kinds of business processes that are amenable to models.\\xa0 Remember that models that work nearly perfectly can still fail spectacularly in unusual situations. When it doesn’t work so well: We don’t always have the luxury of time or accurate foresight.\\xa0Sometimes a business does not know how a model will be used: a model is developed for Product X and repurposed for Product Y, a minor beta feature suddenly becomes an overnight success, or a business necessity unexpectedly forces a model into widespread production. Source: Q The backstory: A consultant is an agent of change.\\xa0When a prospect contacts me to discuss a project, I find it helpful to compare the cost of the desired change to the cost of another-change or even to the cost of the not-change. “What happens if you don’t do this?\\xa0What costs do you incur, what exposures do take on now? And six months from now?”\\xa0A high cost of doing nothing means that this is an urgent matter. Some consultants will tell you that a high cost of doing nothing is universally great (it means the prospect is ready to move) and a low cost is universally bad (the prospect isn’t really interested).\\xa0 I see it differently: we can use that cost of doing nothing as a guide to how we define the project’s timeline, fee structure, and approach. If the change is extremely urgent—a very high cost of doing nothing—it may warrant a quick fix now, soon followed by a more formal approach once the system is stable. A low cost of doing nothing, by comparison, means that we can define the project as “research” or “an experiment,” and move at a slower pace. How I use it: I will ask this one, flat-out, once a consulting prospect has outlined what they want to do. Why it’s useful: Besides helping to shape the structure of the project, understanding the cost of doing nothing can also shed light on the prospect’s motivations. That, in turn, can unlock additional information that can be relevant to the project.\\xa0(For example, maybe the services I provide will help them reach the desired change, but that change won’t really help the company.\\xa0Perhaps I can refer them to someone else in that case.) When it doesn’t work so well: Sometimes people don’t have a good handle on the risks and challenges they (don’t) face.\\xa0They may hastily answer that this is an urgent matter when it’s not; or they may try to convince you that everything is fine when you can clearly see that the proverbial house is on fire. When you detect that their words and the situation don’t align, you can ask them to shed light on their longer-term plans.\\xa0That may help them to see the situation more clearly. Source: Chris The backstory: This is something that was inspired from the intersection of an incredibly boring decision-science book and roadmap planning. Decision trees and roadmaps are very useful when building out the possible spaces of the future. However, for both decision trees and roadmaps we are usually overly optimistic in how we will proceed.\\xa0 We fail at properly considering failure.\\xa0 To appropriately plan for the future we must consider the different ways we can be wrong. Sometimes it will be at a certain decision point (“we didn’t get enough signups to move forward”) or an event trigger (“we see too many complaints”).\\xa0 If we consider this wrong-ness and the possible next step, we can start to normalize this failure and make better decisions. How I use it:\\xa0 It’s best to ask this when you find that certainty is at a high point for the project. More often than not, people don’t consider ways to detect that they need to change course. Why it’s useful: You build a map into the future based on what you can detect. This helps make hard decisions easier because you are effectively practicing the decision process before you are in the heat of the moment. When it doesn’t work so well: When things are currently going “wrong” it can be a sensitive subject for people. I’ve found it is easier to talk about how to get out of a current wrong situation than considering additional future situations. Source: Shane The backstory: Imagine you employ a vendor to provide or enrich your training data, or you pay for consulting services related to ML.\\xa0What happens to the information used by the vendors to build your product?\\xa0 Their downstream rights there run the gamut from “absolutely nothing” to “retaining a full copy of the training data, labels, trained models, and test results.” The median position, in my observation, tends to be that the vendor retains control of any new techniques and information derived from the work that would be useful in general, such as new methods of programmatically applying error correction to a trained model, but not the specific data used to train the model or the resulting trained model. From the customer perspective, downstream rights are tied to competition/cost tradeoffs and the rights associated with training data.\\xa0 A company that considers ML a competitive advantage likely will not want their models or derivative data available to competitors, and they must balance this against the business consideration that vendors which retain downstream rights typically charge lower fees (because reselling that data or models can be a source of revenue). In addition, training data usually comes with contractual limitations and customers of ML services need to ensure they are not granting downstream rights that they don’t have in their upstream agreements. Finally, some kinds of training data, such as medical records or classified government data, may forbid unauthorized access or use in systems that lack adequate safeguards and audit logs. How I use it: This question is less relevant to companies that have an entirely in-house workflow (they generate their own training data, train their own models, and use models with their own employees and tools).\\xa0 It is highly relevant to companies that buy or sell ML services, use external vendors for part of their workflow, or handle sensitive data. Why it’s useful:\\xa0 The notion of downstream rights is not a new question, nor is it specific to the ML world.\\xa0 Almost all vendor relationships involve delineating the intellectual property (IP) and tools that each party brings to the project, as well as the ownership of new IP developed during the project. Helping founders to recognize and establish those boundaries early on can save them a lot of trouble later. When it doesn’t work so well: This is a question a company definitely wants to answer before they’ve provided data or services to a counterparty.\\xa0 These issues can be very difficult to resolve once data has been shared or work has begun. Source: Q The backstory: A risk is a potential change that comes with consequences.\\xa0 To properly manage risk—to avoid those consequences—you need to identify those changes in advance (perform a risk assessment) and sort out what to do about them (devise your risk mitigation plans). That’s where this trio of questions comes in: “What if?” is the key to a risk assessment, as it opens the discussion on ways a project may deviate from its intended path.\\xa0 “Then?” explores the consequences of that deviation.\\xa0The “What next?” starts the discussion on how to handle them. “What if … our data vendor goes out of business? Then?\\xa0Our business is hamstrung.\\xa0What next?\\xa0We’d better have a backup data vendor in the wings.\\xa0 Or better yet, keep two vendors running concurrently so that we can switch over with minimal downtime.” “What if … something changes, and the model’s predictions are wrong most of the time?\\xa0Then? We’re in serious trouble, because that model is used to automate purchases.\\xa0What next? We should implement monitors around the model, so that we can note when it’s acting out of turn. We should also add a ‘big red button’ so that a person can quickly, easily, and completely shut it down if it starts to go haywire.” How I use it: \\xa0Once we’ve sorted out what the client wants to achieve, I’ll round out the picture by walking them through some “What if? Then? What next?” scenarios where things don’t work out. Why it’s useful: It’s too easy to pretend the not-intended outcomes don’t exist if you don’t bring them up. I want my clients to understand what they’re getting into, so they can make informed decisions on whether and how to proceed. Going through even a small-scale risk assessment like this can shed light on the possible downside loss that’s lurking alongside their desired path. All of that risk can weigh heavily on their investment, and possibly even wipe out any intended benefit. When it doesn’t work so well: The business world, especially Western business culture, has a strange relationship with positive attitudes. This energy can be infectious and it can help to motivate a team across the finish line. It can also convince people to pretend that the non-intended outcomes are too remote or otherwise not worth consideration.\\xa0That’s usually when they find out, the hard way, what can really go wrong. How to handle this varies based on your role in the company, internal company politics, your ability to bring about change, and your ability to weather a storm. Source: Chris The backstory: The most important question is one that isn’t expected. It is one that leads to unexpected answers. We don’t have dialog for dialog sake; we do it to learn something new. Sometimes the thing we learn is that we aren’t aligned. I’ve found that the most unexpected thing is something that we wouldn’t choose based on our current thought process. Randomly choosing a question from a collection appropriate for your domain is really valuable. If you are building something for the web, what kinds of questions could you ask about a web project? This is helpful when the checklists of things to do get too large to try all of them. Pick a few at random. You can take it a step further and pick questions from outside of your domain. This can simply be a list of provocations that require a high amount of interpretation by you to make sense. This is because randomness doesn’t work without the lens of human intuition.\\xa0 Randomness without this intuition is just garbage. We do the work to bridge from random questions to some new idea related to our problem. We build the analogies in our mind even when something is seemingly not connected at first. How I use it: When you find that you keep asking the same questions. I have decks of cards like Oblique Strategies for provocations, Triggers for domain-specific questions, and others that can provide randomness. Domain-specific random questions can also be very impactful. Eventually, I expect models like GPT-n to provide appropriate random questions to prompts. Why it’s useful: Even with all of the questions we ask to get out of bias, we are still biased. We still have assumptions we don’t realize. Randomness doesn’t care about your biases and assumptions. It will ask a question that you think on the surface is stupid, but when you think about it is important. When it doesn’t work so well: With teams that are high on certainty they may think of the random question as a toy or distraction. The people I’ve found to be incredibly confident in their world trivialize the need to question bias. They will even try to actively subvert the process sometimes. If you hide the fact that a question was randomly chosen, it can go over better. If you’re collecting facts—names, numbers, times—then narrow questions will suffice.\\xa0 But if you’re looking to understand the bigger picture, if you want to get a meeting out of a rut, if you want people to reflect before they speak, then open-ended questions will serve you well.\\xa0 Doubly so when they come from an unexpected source and at an unexpected time. The questions we’ve documented here have helped us in our roles as an AI consultant, a product manager, and an attorney.\\xa0(We also found it interesting that we use a lot of the same questions, which tells us how widely applicable they are.) We hope you’re able to put our favorite questions to use in your work. Perhaps they will even inspire you to devise and test a few of your own. One point we hope we’ve driven home is that your goal in asking good questions isn’t to make yourself look smarter.\\xa0Nor is it to get the answers you want to hear.\\xa0Instead, your goal is to explore a problem space, shed light on new options, and mitigate risk. With that new, deeper understanding, you’re more prepared to work on the wicked problems that face us in the workplace and in the world at large.', 'Differential dataflow programs look like many standard “big data” computations, borrowing idioms from frameworks like MapReduce and SQL. However, once you write and run your program, you can change the data inputs to the computation, and differential dataflow will promptly show you the corresponding changes in its output. Promptly meaning in as little as milliseconds. In this work, we create a true Many-to-Many multilingual translation model that can translate directly between any pair of 100 languages. Our focus on non-English-Centric models brings gains of more than 10 BLEU when directly translating between non-English directions while performing competitively with the best single systems of WMT.', 'You start with a single component, the nand gate. Using this as the fundamental building block, you will build all other components necessary. today we are unveiling Recursive Belief-based Learning (ReBeL), a general RL+Search algorithm that can work in all two-player zero-sum games, including imperfect-information games. ReBeL builds on the RL+Search algorithms like AlphaZero that have proved successful in perfect-information games. Unlike those previous AIs, however, ReBeL makes decisions by factoring in the probability distribution of different beliefs each player might have about the current state of the game, which we call a public belief state (PBS). In other words, ReBeL can assess the chances that its poker opponent thinks it has, for example, a pair of aces. We demonstrate our claim by implementing tensor algebra and stochastic gradient descent using lambda expressions for loss functions as a pipelined operator in a main memory database system. Our approach enables common machine learning tasks to be performed faster than by extended disk-based database systems or as well as dedicated tools by eliminating the time needed for data extraction. This work aims to incorporate gradient descent and tensor data types into database systems, allowing them to handle a wider range of computational tasks.', 'Automerge is designed for creating local-first software, i.e. software that treats a user’s local copy of their data (on their own device) as primary, rather than centralising data in a cloud service.  In this paper, we present HangFix, a software hang bug fixing framework which can automatically fix a hang bug that is triggered and detected in production cloud environments. HangFix first leverages stack trace analysis to localize the hang function and then performs root cause pattern matching to classify hang bugs into different types based on likely root causes. Next, HangFix generates effective code patches based on the identified root cause patterns. We have implemented a prototype of HangFix and evaluated the system on 42 real-world software hang bugs in 10 commonly used cloud server applications. Our results show that HangFix can successfully fix 40 out of 42 hang bugs in seconds.', 'Focusing on the data entry and storage aspects, this article offers practical recommendations for organizing spreadsheet data to reduce errors and ease later analyses. The basic principles are: be consistent, write dates like YYYY-MM-DD, do not leave any cells empty, put just one thing in a cell, organize the data as a single rectangle (with subjects as rows and variables as columns, and with a single header row), create a data dictionary, do not include calculations in the raw data files, do not use font color or highlighting as data, choose good names for things, make backups, use data validation to avoid data entry errors, and save the data in plain text files. To our knowledge, this is the first exploration of a practical general purpose real number type that both reflects the mathematical laws of the real numbers, and also supports exact comparisons in situations in which that’s normally expected. Here, we report a universal fabrication scheme to enable printing and room-temperature sintering of the metal nanoparticle on paper/fabric for FPCBs and directly on the human skin for on-body sensors with a novel sintering aid layer. Consisting of polyvinyl alcohol (PVA) paste and nanoadditives in the water, the sintering aid layer reduces the sintering temperature. Together with the significantly decreased surface roughness, it allows for the integration of a submicron-thick conductive pattern with enhanced electromechanical performance. Various on-body sensors integrated with an FPCB to detect health conditions illustrate a system-level example. A presentation of several novel ways to visualize 25 years of the Gartner Hype Cycle.', 'The field of AI product management continues to gain momentum. As the AI product management role advances in maturity, more and more information and advice has become available. Our previous articles in this series introduce our own take on AI product management, discuss the skills that AI product managers need, and detail how to bring an AI product to market. One area that has received less attention is the role of an AI product manager after the product is deployed. In traditional software engineering, precedent has been established for the transition of responsibility from development teams to maintenance, user operations, and site reliability teams. New features in an existing product often follow a similar progression. For traditional software, the domain knowledge and skills required to develop new features differ from those necessary to ensure that the product works as intended. Because product development and product operations are distinct, it’s logical for different teams and processes to be responsible for them. In contrast, many production AI systems rely on feedback loops that require the same technical skills used during initial development. Similarly, in “Building Machine Learning Powered Applications: Going from Idea to Product,” Emmanuel Ameisen states: “Indeed, exposing a model to users in production comes with a set of challenges that mirrors the ones that come with debugging a model.” As a result, at the stage when product managers for other types of products might shift to developing new features (or to other projects altogether), an AI product manager and the rest of the original development team should remain heavily involved. One reason for this is to tackle the (likely) lengthy backlog of ML/AI model improvements that will be discovered after the product engages with the real world. Another, of course, is to ensure that the product functions as expected and desired over time. We describe the final responsibility of the AI PM as coordinating with the engineering, infrastructure, and site reliability teams to ensure all shipped features can be supported at scale. This article offers our perspective into the practical details of the AI PM’s responsibilities in the latter parts of the AI product cycle, as well as some insight into best practices in execution of those responsibilities. In Bringing an AI Product to Market, we distinguished the debugging phase of product development from pre-deployment evaluation and testing. This distinction assumes a slightly different definition of debugging than is often used in software development. We define debugging as the process of using logging and monitoring tools to detect and resolve the inevitable problems that show up in a production environment. Emmanuel Ameisen again offers a useful framework for defining errors in AI/ML applications: “…three areas in particular are most important to verify: inputs to a pipeline, the confidence of a model and the outputs it produces.” To support verification in these areas, a product manager must first ensure that the AI system is capable of reporting back to the product team about its performance and usefulness over time.\\xa0 This may manifest in several ways, including the collection of explicit user feedback or comments via channels outside of the product team, and the provision of mechanisms to dispute the output of the AI system where applicable. Proper AI product monitoring is essential to this outcome. From a technical perspective, it is entirely possible for ML systems to function on wildly different data. For example, you can ask an ML model to make an inference on data taken from a distribution very different from what it was trained on—but that, of course, results in unpredictable and often undesired performance. Therefore, deployed AI products should include validation steps to ensure that model inputs and outputs are within generally expected limits, before a model training or inference task is accepted as successful. Ideally, AI PMs would steer development teams to incorporate I/O validation into the initial build of the production system, along with the instrumentation needed to monitor model accuracy and other technical performance metrics. But in practice, it is common for model I/O validation steps to be added later, when scaling an AI product. Therefore, the PM should consider the team that will reconvene whenever it is necessary to build out or modify product features that: The composition of these teams will vary between companies and products, but a typical cross-functional team would likely include representatives from Data Science (for product-level experimentation and inference task validation), Applied Science (for model performance and evaluation), ML Engineering (for data and feature engineering, as well as model pipeline support) and Software/Feature Engineering (for integration with the full stack of the AI product—such as UI/UX, cloud services, and dev ops tools). Working together, this post-production development team should embrace continuous delivery principles, and prioritize the integration of any additional necessary instrumentation that was not already implemented during the model development process. Finally, the AI PM must work with production engineering teams to design and implement the alerting and remediation framework. Considerations include where to set thresholds for each persona, alert frequency, and the degree of remediation automation (both what’s possible and desired). During testing and evaluation, application performance is important, but not critical to success. In the production environment, when the outputs of an ML model are often a central (yet hidden) component of a greater application, speed and reliability are critically important. It is entirely possible for an AI product’s output to be absolutely correct from the perspective of accuracy and data quality, but too slow to be even remotely useful. Consider the case of autonomous vehicles: if the outputs from even one of the many critical ML models that comprise the vehicle’s AI-powered “vision” are delivered after a crash, who cares if they were correct? In engineering for production, AI PMs must take into account the speed at which information from ML/AI models must be delivered (to validation tasks, to other systems in the product, and to users). Technologies and techniques—such as engineering specifically for GPU/TPU performance and caching—are important tools in the deployment process, but they are also additional components that can fail, and thus be responsible for the failure of an AI product’s core functionality. An AI PM’s responsibility is to ensure that the development team implements proper checks prior to release, and—in the case of failure—to support the incident response teams, until they are proficient in resolving issues independently. AI product managers must also consider availability: the degree to which the service that an AI product provides is available to other systems and users. Service Level Objectives (SLOs) provide a useful framework for encapsulating this kind of decision. In an incident management blog post, Atlassian defines SLOs as: “the individual promises you’re making to that customer… SLOs are what set customer expectations and tell IT and DevOps teams what goals they need to hit and measure themselves against. SLOs can be useful for both paid and unpaid accounts, as well as internal and external customers.” Service Level Indicators, Objectives, and Agreements (SLIs, SLOs, and SLAs) are well-known, frequently used, and well-documented tools for defining the availability of digital services.\\xa0 For cloud infrastructure some of the most common SLO types are concerned with availability, reliability and scalability. For AI products, these same concepts must be expanded to cover not just infrastructure, but also data and the system’s overall performance at a given task. While useful, these constructs are not beyond criticism. Chief among the challenges are: choosing the correct metrics to begin with, measuring and reporting once metrics are selected, and the lack of incentive for a service provider to update the service’s capabilities (which leads to outdated expectations). Despite these concerns, service level frameworks can be quite useful, and should be in the AI PM’s toolkit when designing the kind of experience that an AI product should provide. You must also take durability into account when building a post-production product plan. Even if well-designed, multi-layer fault detection and model retraining systems are carefully planned and implemented, every AI-powered system must be robust to the ever-changing and naturally stochastic environment that we (humans) all live in. Product managers should assume that any probabilistic component of an AI product will break at some point. A good AI product will be able to self-detect and alert experts upon such a failure; a great AI product will be able to detect the most common problems and adjust itself automatically—without significant interruption of services for users, or high-touch intervention by human experts. There are many ways to improve AI product durability, including: It’s worth noting that model durability and retraining can raise legal and policy issues. For example, in many regulated industries, changing any core functionality of an AI system’s decision-making capability (i.e., objective functions, major changes to hyperparameters, etc.) require not only disclosure, but also monitored testing.\\xa0 As such, an AI Product Manager’s responsibility here extends to releasing not only a usable product, but one that can be ethically and legally consumed. It’s also important to remember that no matter what the approach to developing and maintaining a highly durable AI system, the product team must have access to high quality, relevant metrics on both model performance and functionality. Proper monitoring (and the software instrumentation necessary to perform it) is essential to the success of an AI product. However, monitoring is a loaded term. The reasons for monitoring AI systems are often conflated, as are the different types of monitoring and alerting provided by off-the-shelf tools. Emmanuel Ameisen once again provides a useful and concise definition of model monitoring as a way to “track the health of a system. For models, this means monitoring their performance and the equity of their predictions.” The simplest case of model monitoring is to compute key performance metrics (related to both model fit and inference accuracy) regularly. These metrics can be combined with human-determined thresholds and automated alerting systems to inform when a model has “drifted” beyond normal operating parameters. While ML monitoring is a relatively new product area, standalone commercial products (including Fiddler and superwise.ai) are available, and monitoring tools are incorporated into all the major machine learning platforms. Separate from monitoring for model freshness, Ameisen also mentions the need to apply technical domain experience in designing monitoring systems that detect fraud, abuse, and attack from external actors. AI PMs should consult with Trust & Safety and Security teams to combine the best principles and technical solutions with existing AI product functionality. In some specific domains—such as financial services or medicine—no easy technical solutions exist. In this case, it is the responsibility of the AI product team to build tools to detect and mitigate fraud and abuse in the system. As we’ve mentioned previously, it’s not enough to simply monitor an AI system’s performance characteristics. It is even more important to consistently ensure that the AI product’s user-facing and business purposes are being fulfilled. This responsibility is shared by the development team with Design, UX Research, SRE, Legal, PR, and Customer Support teams. The AI PM’s responsibility is again to orchestrate reasonable and easily repeatable mitigations to any problems. It is crucial to design and implement specific alerting capabilities for these functions and teams. If you simply wait for complaints, they will arise far too late in the cycle for your team to react properly. No matter how well you research, design, and test an AI system, once it is released, people are going to complain about it. Some of those complaints will likely have merit, and responsible stewardship of AI products requires that users are given the ability to disagree with the system’s outputs and escalate issues to the product team. It is also entirely possible for this feedback to show you that the system is underserving a particular segment of the population, and that you may need a portfolio of models to serve more of the user base. As an AI PM, you have the responsibility to build a safe product for everyone in the population who might use it. This includes consideration of the complexities that come into play with intersectionality. For example, an AI product might produce great outcomes for wealthy, American, cisgender, heterosexual, White women—and although it might be tempting to assume those outcomes would apply to all women, such an assumption would be incorrect. Returning to previous anti-bias and AI transparency tools such as Model Cards for Model Reporting (Timnit Gebru, et al.) is a great option at this point. It is important not to pass this development task off to researchers or engineers alone; it is an integral part of the AI product cycle. If done right, users will never be aware of all the product monitoring and alerting that is in place, but don’t let that trick you. It’s essential to success. One question that an AI PM might ask when pondering these post-production requirements is: “This seems hard; can’t I just buy these capabilities from someone else?” This is a fair question, but—as with all things related to machine learning and artificial intelligence—the answer is far from a binary yes or no. There are many tools available to help with this process, from traditional vendors and bleeding edge startups alike. Deciding what investment to make in MLOps tooling is an inherently complex task. However, careful consideration and proactive actions often lead to defendable competitive advantages over time. Uber (the developer of Michelangelo), Airbnb (developer of zipline), and Google have all taken advantage of superior tooling and operations skills to build market-leading AI products. Nearly every ML/AI library touts full end-to-end capabilities, from enterprise-ready stacks (such as H20.ai, MLFlow, and Kubeflow) to the highly specialized and engineer-friendly (such as Seldon.io) and everything in-between (like Dask). Enterprise level-frameworks often provide deep and well-supported integration with many common production systems; smaller companies might find this integration unnecessary or overly cumbersome. Regardless, it’s a safe bet that getting these off-the-shelf tools to work with your AI product in the exact ways you need them to will be costly (if not financially, then at least in time and human labor). That said—from a scale, security and features perspective—such capabilities may be required in many mature AI product environments. On the other hand, building and scaling a software tool stack from scratch requires a significant sustained investment in both developer time and technology. Facebook, Uber, AirBnB, Google, Netflix, and other behemoths have all spent millions of dollars to build their ML development platforms; they also employ dozens to hundreds of employees, each tasked with building and scaling their internal capabilities. The upside here is that such end-to-end development to deployment frameworks and tools eventually become a competitive advantage, in and of themselves. However, it’s worth noting that in such environments, employing a single AI PM is not feasible. Instead, a cadre of PMs focused on different components of the AI product value chain are needed. Building great AI products is a significant, cross-disciplinary, and time-consuming undertaking, even for the most mature and well-resourced companies. However, what ML and AI can accomplish at scale can be well worth the investment.\\xa0 Although a return on investment is never guaranteed, our goal is to provide AI PMs with the tools and techniques needed to build highly engaging and impactful AI products in a wide variety of contexts. In this article, we focused on the importance of collaboration between product and engineering teams, to ensure that your product not only functions as intended, but is also robust to both the degradation of its effectiveness and the uncertainties of its operating environment. In the world of machine learning and artificial intelligence, a product release is just the beginning. Product managers have a unique place in the development ecosystem of ML/AI products, because they cannot simply guide the product to release and then turn it over to IT, SRE, or other post-production teams. AI product managers have a responsibility to oversee not only the design and build of the system’s capabilities, but also to coordinate the team during incidents, until the development team has completed enough knowledge transfer for independent post-production operation. The evolution of AI-enabled product experiences is accelerating at breakneck speed. In parallel, the emerging role of AI product management continues to evolve at a similar pace, to ensure that the tools and products delivered to the market provide true utility and value to both customers and businesses. Our goal in this four-part series on AI product management is to increase community awareness and empower individuals and teams to improve their skill sets in order to effectively steer AI product development toward successful outcomes. The best ML/AI products that exist today were brought to market by teams of PhD ML/AI scientists and developers who worked in tandem with resourceful and skilled product teams.\\xa0 All were essential to their success. As the field of AI continues to mature, so will the exciting field of AI product management. We can’t wait to see what you build!   We would like to thank the many people who have\\xa0 contributed their expertise to the early drafts of the articles in this series, including: Emmanuel Ameisen, Chris Albon, Chris Butler, Ashton Chevalier, Hilary Mason, Monica Rogati, Danielle Thorp, and Matthew Wise.', 'CG/SQL is a code generation system for the popular SQLite library that allows developers to write stored procedures in a variant of Transact-SQL (T-SQL) and compile them into C code that uses SQLite’s C API to do the coded operations. CG/SQL enables engineers to create highly complex stored procedures with very large queries, without the manual code checking that existing methods require. (1) Social media is addictive, and we are powerless to resist it. The concept of addiction does not encompass the full range of pleasures, risks, and uses that people create with technology. (2) Technology companies can fix the problems they create with better technology. Some technology cannot be fixed by more design, and some technology should not be built at all. (3) Growth and engagement metrics are the best drivers of decision making at tech companies. Many of the most important parts of digital well-being cannot be captured by quantitative metrics. (4) Our health and well-being depend on spending less time with screens and social media platforms. Health and well-being cannot be reduced to the single variable of screen time. a web-based multi-emulator (RetroArch/libretro) designed to recreate the experience of playing console games with a single controller in a room full of friends.', 'The release of GPT-3 has reinvigorated a discussion of creativity and artificial intelligence. That’s a good discussion to have, primarily because it forces us to think carefully about what we mean when we use words like “creativity” and “art.” As I’ve argued in the past, each time we have this discussion, we end up raising the bar. Each time an AI system does something that looks “intelligent” or creative, we end up deciding that’s not what intelligence really is.  And that’s a good thing. AI is likely to teach us more about what intelligence and creativity are not than about what they are. I’m not terribly interested in whether AI can imitate human creativity. “Can an AI create a ‘new’ poem that reads as if it were written by Keats, or a new piano sonata that sounds like Beethoven” isn’t a question that’s worth asking. Of course it can—if not now, it will be able to in the near future. We really don’t need a new Beethoven sonata; the 32 he wrote are enough. Nor do we need a new Keats ode, limited though his output was. Or a new Rembrandt. Imitation is ultimately a party trick: clever and amusing, but not really important. Sure, if you want texts for greeting cards or elevator music (or maybe even commercial pop), algorithms may do the trick. What’s really important is the transition between different forms of creativity. How do you get something that’s qualitatively new, and not just imitation? Creativity isn’t about the artifacts as much as it’s about the transitions. How do you get from Bach to Haydn? How do you get from Haydn to Beethoven? And, even within the career of a single artist: how do you get from the beginning to the end? How do you get from Beethoven’s first piano sonata, which sounds like Haydn, to the last, which at some points anticipates jazz? Artists aren’t stagnant. But I have no idea how even to ask whether an AI system can “mature” or “grow” in its output. Great artists (and yes, I’m presuming a lot with the word “great”) frequently work by defining themselves against what came before. This is particularly clear with artists of the Romantic period in Germany, England, and France. The term Romanticism didn’t come around until some years later, but they left behind several manifestos describing what they were trying to do, and how it was different from what came before. That is still how artists work: Nnedi Okorafor’s Africanfuturism is important as a way of defining and directing her own work. The importance of these defining statements isn’t so much about “rightness” (in the sense of “this is what art is or should be”) but in setting a direction for their project. Can a machine do that? Can it decide how its work will be different from what came before? It’s not clear to me that it can’t, but that’s a significant step beyond any machine learning projects we currently have. Although artists work by projecting their work into the future, by defining something “new,” their work is also derived from what came before—possibly as misinterpretation, but almost always as revision. Listen to the Beatles, and you hear something that was really built on the backbone of blues, filtered through some British pop-cultural trends. At the end of the “His Dark Materials” trilogy, Phillip Pullman thanks all the authors he stole from. Or, as T. S. Eliot said, “Immature poets imitate; mature poets steal; bad poets deface what they take, and good poets make it into something better, or at least something different.” Artists break from the past by reinterpreting that past.\\n For AI-generated art–where does that sense of “different” come from? Can artificial intelligence learn to steal and reinterpret? Where does that engagement with history, current events and even selfhood come from?  Without that, there’s no basis for reinterpretation aside from random perturbation. It’s not clear that a sense of history couldn’t come from a big model trained on a gigantic corpus (although the best we can do now is build models that have no idea what they are saying). What kind of model would take that corpus and make something that was different, something that hadn’t been seen before? Would we care if Hamlet wasn’t written by Shakespeare in a specific historical context, but in 2025 by a computer trained on an archive of Elizabethan politics, drama, and history? (Never mind that an archive of Elizabethan drama would be very skimpy; most plays from that period were never published.) Would anyone care about the opera Nixon in China if it didn’t reflect a composer’s, and a librettist’s, thinking about historical events?\\n There’s another way in which I find computed-generated artworks unsatisfactory, particularly in music. AI-generated music is often interesting over the short term, but fails at larger-scale structure. Much of music history, from four-bar blues to Beethoven’s massive experiments in sonata form, is about building structures that can be interesting over the long term, whether “long term” means a few minutes of a blues song to several hours of opera. That was Beethoven’s project; more recently, it was the project of rock groups like Pink Floyd. It seems conceivable that a model could learn to generate such longer-form structures, but I’ve yet to see one that does it satisfactorily. Is this where collaboration between humans and machines comes in, and if so, what does that say about creativity? A machine could conceivably do the pattern-matching and combinatorics, assembling a creative work out of news clippings and stylistic mimicry. But a human still needs to supply the sense of history that makes the work something we care about. A human still needs to provide the structure that makes art works more than brief curiosities. Is it possible for a human could tweak a model like GPT-3 to give it that sense of direction and context? What kinds of user interfaces would facilitate this kind of interaction? I can’t answer those questions, but that sounds like a much more interesting form of digital collaboration than having an algorithm write hundreds of dull poems or songs and “curating” a few that aren’t boring. That’s just a recipe for greeting-card sentiment and elevator music. I mean no disrespect to Hallmark—mass-produced poetry for greeting cards serves a purpose—but when we think about what kinds of creativity we want, and how that creativity will be mediated by AI, we should demand more.', ' To analyze the possible consequences, we study experimentally the behavior of algorithms powered by Artificial Intelligence (Q-learning) in a workhorse oligopoly model of repeated price competition. We find that the algorithms consistently learn to charge supracompetitive prices, without communicating with one another. The high prices are sustained by collusive strategies with a finite phase of punishment followed by a gradual return to cooperation. This finding is robust to asymmetries in cost or demand, changes in the number of players, and various forms of uncertainty. RAND researchers developed Hedgemony, a wargame designed to teach U.S. defense professionals how different strategies could affect key planning factors in the trade space at the intersection of force development, force management, force posture, and force employment. The game presents players, representing the United States and its key strategic partners and competitors, with a global situation, competing national incentives, constraints, and objectives; a set of military forces with defined capacities and capabilities; and a pool of periodically renewable resources. The players are asked to outline their strategies and are then challenged to make difficult choices by managing the allocation of resources and forces in alignment with their strategies to accomplish their objectives within resource and time constraints. Run Android applications on any GNU/Linux operating system.', 'On its own, using a simple DC voltage as the input, the device outputs not just simple spikes, as some other devices can manage, but the whole array of neural activity—bursts of spikes, self-sustained oscillations, and other stuff that goes on in your brain connected communities; the Truth Sandwich; pre-bunking; distributed debunking; localize the context; humor over rumor.', 'This month, the big surprise is that there’s no significant technology news about COVID. And there is more news than ever about legislation and regulation. I suspect that the legal system will be a big driver for technology over the next year. Another trend that doesn’t quite count as technology news but that definitely bears watching is that college enrollment in the US is down. Grad schools are up, 4 year colleges are down slightly; the big hit is in 2 year colleges. COVID is probably the biggest contributing factor, but regardless of the cause, this is an inauspicious trend.', ' So he then examined the mechanism the coffee maker used to receive firmware updates. It turned out they were received from the phone with—you guessed it—no encryption, no authentication, and no code signing. Big machines are sometimes more efficient. But they cost more, so fewer can be produced with a finite budget. Small machines are cheaper and may benefit from improvement over time, driven by experience in building more units. When does this experience lead to greater overall efficiency? We derive an approximation which, given a learning rate, tells how much smaller a machine must be to overcome an initial efficiency disadvantage. The first work that specifically addressed the detection of automated accounts in online social networks dates back to January 2010.', 'A program P is incremental if repeating P with a changed input is faster than from-scratch computation. Adapton offers programming language abstractions for incremental computation. Keep your migration scripts away from your production code; Keep it low-tech, don’t deserialize; Write tests to exercise each migration script individually; Consider running long migrations online; Consider versioning your documents. However, we do not look at image formats from a general point of view, but rather think of ways to glitch them. When we look at PNG from the point of view of glitch, what kind of peculiarity does it have?', 'Figuring out what shapes proteins fold into is known as the “protein folding problem”, and has stood as a grand challenge in biology for the past 50 years. In a major scientific advance, the latest version of our AI system AlphaFold has been recognised as a solution to this grand challenge by the organisers of the biennial Critical Assessment of protein Structure Prediction (CASP). The organizers even worried DeepMind may have been cheating somehow. So Lupas set a special challenge: a membrane protein from a species of archaea, an ancient group of microbes. For 10 years, his research team tried every trick in the book to get an x-ray crystal structure of the protein. “We couldn’t solve it.” But AlphaFold had no trouble. It returned a detailed image of a three-part protein with two long helical arms in the middle. The model enabled Lupas and his colleagues to make sense of their x-ray data; within half an hour, they had fit their experimental results to AlphaFold’s predicted structure. “It’s almost perfect,” Lupas says. “They could not possibly have cheated on this. I don’t know how they do it.” Far more useful (and to me, more impressive) than AlphaGo. Purpose-First Programming — Some students resist the cognitively-heavy tasks of simulating program execution. The secret to teaching those folks to program may be “purpose-first programming”: She used Github repositories and expert interviews to identify a few programming plans (just like Elliot Soloway and Jim Spohrer studied years ago) that were in common use in a domain that her participants cared about. She then taught those plans. Students modified and combined the plans to create programs that the students found useful. Rather than start with syntax or semantics, she started with the program’s purpose. Very reminiscent of the late 90s Perl and PHP copy-and-change coding boom that got orders of magnitude more people programming than were coming through CS courses at the time. She used Github repositories and expert interviews to identify a few programming plans (just like Elliot Soloway and Jim Spohrer studied years ago) that were in common use in a domain that her participants cared about. She then taught those plans. Students modified and combined the plans to create programs that the students found useful. Rather than start with syntax or semantics, she started with the program’s purpose.', 'This class examines ethical frameworks, modern ethical concerns related to computer science and technology, and clear oral and written communication. Topics we will explore include policy vacuums created by new technology, copyright and patent, software bugs and liability, freedom of speech, privacy, security, employment and job markets, warfare and state-building, wealth discrepancy and consumerism, environmental impact, and changing cultural norms and social contracts. What follows is part position paper and part ahistorical review. This essay introduces the term hardware lottery to describe when a research idea wins because it is compatible with available software and hardware and not because the idea is superior to alternative research directions. We argue that choices about software and hardware have often played a decisive role in deciding the winners and losers in early computer science history. a Bitsy game about learning to rely on others and fighting against hopelessness, together. If you are writing down “rules” and insisting that developers abide by them, it’s probably because your developers are continuously doing things you wish they wouldn’t. Usually, this isn’t because your developers don’t understand “the rules” and/or don’t like you—it’s because they know what the organization values, and those values are in conflict with your “rules,” and they’re trying to deliver that value.', 'a higher level taxonomy that I use to think about concurrent performance. We’ll group the performance of concurrent operations into six broad levels running from fast to slow, with each level differing from its neighbors by roughly an order of magnitude in performance. without more geographic representation, they’ll produce a global vision for AI ethics that reflects the perspectives of people in only a few regions of the world, particularly North America and northwestern Europe. […] This lack of regional diversity reflects the current concentration of AI research (pdf): 86% of papers published at AI conferences in 2018 were attributed to authors in East Asia, North America, or Europe. And fewer than 10% of references listed in AI papers published in these regions are to papers from another region. Patents are also highly concentrated: 51% of AI patents published in 2018 were attributed to North America. As a result, the local model is only useful for queries with a very strong “signal.” Apple’s system, for example, uses the local model to estimate the popularity of emojis, but the results are only useful for the most popular emojis (i.e. where the “signal” is strongest). The local model is typically not used for more complex queries, like those used in the U.S. Census [3] or applications like machine learning.', 'AI is a field where value, in the form of outcomes and their resulting benefits, is created by machines exhibiting the ability to learn and “understand,” and to use the knowledge learned to carry out tasks or achieve goals. AI-generated benefits can be realized by defining and achieving appropriate goals. These goals depend on who the stakeholder is; in other words, the person or company receiving the benefits. There are three potential stakeholders for AI applications, with a single application often involving all three. They are business stakeholders, customers, and users. Each type of stakeholder has different and unique goals; each group is most interested in having their specific objectives met, or problems solved. My book, AI for People and Business, introduces a framework that highlights the fact that both people and businesses can benefit from AI in unique and different ways. A typical social media platform needs to satisfy all three stakeholders. In the case of Twitter, the business stakeholder’s top goals are likely centered around profits and revenue growth. Customer stakeholders are the people and companies that advertise on the platform, and are most concerned with ROI on their ad spend. User stakeholders are interested in benefiting from the platform’s functionality: staying up-to-date, quickly finding new people and topics to follow, and engaging with family and friends. Goals should be defined specifically and at a granular level for each stakeholder and relevant use case. Twitter has no doubt went through this exercise long ago; but if we imagine Twitter taking its first steps towards AI, some specific and granular goals could be to build a recommendation engine that helps users find the most relevant people to follow (a goal for users), while also building an AI-powered advertising targeting engine that best matches ads with those most likely to be interested in the product or service being advertised (for customers). This in turn would increase the platform’s value for users and thus increase engagement, which would result in more eyes to see and interact with ads, which would mean better ROI on ad spend for customers, which would then achieve the goal of increased revenue and customer retention (for business stakeholders). The key is to start with small and easily identifiable AI projects that will trickle value upwards towards a company’s highest priority goals. For companies early in their AI journey, setting appropriate goals helps create a foundation from which to build AI maturity. It also helps companies learn how to translate existing AI capabilities into solving specific real-world problems and use cases. In my book, I introduce the Technical Maturity Model: I define technical maturity as a combination of three factors at a given point of time. These factors are: There’s a lot of overlap between these factors.\\xa0 Defining them precisely isn’t as important as the fact that you need all three. Higher levels of experience, technical sophistication, and technical competence increase technical maturity. Increased AI technical maturity boosts certainty and confidence, which in turn, results in better and more efficient AI-powered outcomes and success. Technical maturity is a major factor behind why some companies are very successful with AI, while other companies struggle to get started and/or achieve success. Turning an AI idea into actual benefits is difficult and requires the “right” goals, leadership, expertise, and approach. It also requires buy-in and alignment at the C-level. Identifying, prioritizing, and goal-setting for AI opportunities is a multi-functional team effort that should include business folks, domain experts, and AI practitioners and researchers. This helps ensure alignment with company goals, while also including necessary business and domain expertise. AI initiatives may also require significant considerations for governance, compliance, ethics, cost, and risk. Further, while the technical details of AI are complex, the outputs of AI techniques are relatively simple. In most cases, AI solutions are built to map a set of inputs to one or more outputs, where the outputs fall into a small group of possibilities. Outputs from trained AI models include numbers (continuous or discrete), categories or classes (e.g., spam or not-spam), probabilities, groups/segments, or a sequence (e.g., characters, words, or sentences). Therefore, AI techniques don’t just solve real-world problems out of the box. They don’t automatically generate revenue and growth, maximize ROI, or keep users engaged and loyal. Likewise, AI doesn’t inherently optimize supply chains, detect diseases, drive cars, augment human intelligence, or tailor promotions to different market segments. Setting a company-wide goal of reducing customer churn by 25% is great, but, unfortunately, is far too broad for most AI applications. That’s why customer churn reduction is not a natural output of AI techniques. The mismatch between goals like reducing customer churn and actual AI outputs must be properly handled and mapped. AI goals should be appropriate for a given company’s technical maturity, and should be chosen to maximize the likelihood of success, prove value, and build a foundation from which to create increasingly sophisticated AI solutions that achieve higher-level business goals. A crawl, walk, run approach is a good analogy for this. Goals should be well-formed, meaning they are stakeholder-specific, map actual AI outputs to applications and use cases that achieve business goals, and are appropriately sized. For companies early in their AI maturity, appropriately-sized goals mean that they should be small and specific enough to experiment with, and prove potential value from, relatively quickly (think lean methodologies and incremental). As AI maturity increases, a non-incremental, holistic, and organization-wide AI vision and strategy should be created to achieve hierarchically-aligned AI goals of varying granularity—goals that drive all AI initiatives and development. This should be accompanied by a transition from incremental thinking to big vision, “applied AI transformation” thinking. Let’s consider the overall goal of reducing customer churn. In an early stage of AI maturity, we can build AI solutions that reduce search friction (e.g., Netflix and Amazon recommendation engines), increase stickiness through personalized promotions and content that is more relevant and engaging, create a predictive model to identify customers most likely to churn and take appropriate preventative actions, or automate and optimize results in areas that are outside of a person’s primary area of expertise (e.g., automated retirement portfolio rebalancing and maximized ROI). When transitioning to developing a bigger AI vision and strategy, we may create a prioritized product roadmap consisting of a suite of recommendation engines and an AI-based personalized loyalty program, for example. At the individual goal level, and for each well-formed goal, the same multi-functional team mentioned earlier must work collaboratively to determine what AI opportunities are available, select and prioritize the ones to pursue, and determine the technical feasibility of each. There are frameworks like SMART to help characterize well-formed goals, but since AI is a field that I characterize as scientific innovation (like R&D), characteristics like being achievable and time-bound may not be the best goals. Results are typically achieved through a scientific process of discovery, exploration, and experimentation, and these processes are not always predictable. Given the scientific nature of AI, goals are better expressed as well-posed questions and hypotheses around a specific and intended benefit or outcome for a certain stakeholder. With well-formed goals, data scientists and machine learning engineers can then apply the scientific method to test different approaches in order to determine the validity of the hypothesis, and assess whether a given approach is feasible and can achieve the goal. For example, by introducing the “Frequently bought together” recommendations (and other recommendations), Amazon was able to increase average customer shopping cart size and order amount (i.e., up-sell and cross-sell), which in turn increases average revenue per customer, which in turn increases Amazon’s e-commerce generated revenue per quarter. McKinsey estimates that up to 35% of Amazon’s revenue and 75% of everything watched on Netflix comes from AI-powered recommendations. But when defining an AI project, the goal or hypothesis in this case isn’t to increase top-line revenue for the company, but rather to posit that building an application that groups products by likelihood to be purchased together will increase average customer order size, which in turn will have an upward impact on top level goals like increasing average revenue per customer and top-line revenue. Another example would be setting a goal around building a well-performing AI model that can predict demand (number of units likely to be purchased) for a specific product for a given day, time, and weather conditions. If accurate, this prediction can help a retailer ensure that they do not run out of stock, which means that there is no lost revenue because a product is out of stock. An added benefit is improved customer experience, which results in happier and more loyal customers who are able to buy the products they want whenever they want to buy it. This same approach can be applied to virtually any other application of AI. AI and machine learning technologies have come a long way in terms of capabilities and accessibility, but off-the-shelf AI solutions aren’t yet available for specific industries or business domains, companies, sets of data, applications, and use cases. The key to success with AI is assembling a multi-functional team that defines appropriate goals, then letting these goals drive the AI initiatives and projects.', 'In our paper, A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild, ACM Multimedia 2020, we aim to lip-sync unconstrained videos in the wild to any desired target speech.', '“There are lot of definitions of what a developer is […] It’s not just people who write code.” […] Microsoft has even given these “civilian” programmers a persona: Mort. […] The fictional “Mort” is a skilled professional, anyone from a business analyst to a construction site cost estimator, who needs computers to perform specific functions without mastering the intricacies of full-blown programming. We don’t need complete, perfect solutions; we need partial solutions in situations where we don’t have all the information, and we need the ability to explore those solutions with an (artificially) intelligent partner.', 'In a conversation with Kevlin Henney, we started talking about the kinds of user interfaces that might work for AI-assisted programming. This is a significant problem: neither of us were aware of any significant work on user interfaces that support collaboration. However, as software developers, many of us have been practicing effective collaboration for years. It’s called pair programming, and it’s not at all like the models we’ve seen for interaction between an AI system and a human. Most AI systems we’ve seen envision AI as an oracle: you give it the input, it pops out the answer. It’s a unidirectional flow from the source to the destination. This model has many problems; for example, one reason medical doctors have been slow to accept AI may be that it’s good at giving you the obvious solution (“that rash is poison ivy”), at which point the doctor says “I knew that…” Or it gives you a different solution, to which the doctor says “That’s wrong.” Doctors worry that AI will “derail clinicians’ conversations with patients,” hinting that oracles are unwelcome in the exam room (unless they’re human).  Shortly after IBM’s Watson beat the world Jeopardy champions, IBM invited me to see a presentation about it. For me, the most interesting part wasn’t the Jeopardy game Watson played against some IBM employees; it was when they showed the set of answers Watson considered before selecting its answer, weighted with their probabilities. That level was the real gold.\\xa0 We don’t need an AI system to tell us something obvious, or something we can Google in a matter of seconds. We need AI when the obvious answer isn’t the right one, and one of the possible but rejected answers is. What we really need is the ability to have a dialog with the machine. We still don’t have the user interface for that. We don’t need complete, perfect solutions; we need partial solutions in situations where we don’t have all the information, and we need the ability to explore those solutions with an (artificially) intelligent partner. What is the logic behind the second, third, fourth, and fifth solutions? If we know the most likely solution is wrong, what’s next? Life is not like a game of Chess or Go—or, for that matter, Jeopardy.  What would this look like? One of the most important contributions of Extreme Programming and other Agile approaches was that they weren’t unidirectional. These methodologies stressed iteration: building something useful, demo-ing it to the customer, taking feedback, and then improving. Compared to Waterfall, Agile gave up on the master plan and specification that governed the project’s shape from start to finish, in favor of many mid-course corrections. That cyclic process, which is about collaboration between software developers and customers, may be exactly what we need to get beyond the “AI as Oracle” interaction. We don’t need a prescriptive AI writing code; we need a round trip, in which the AI makes suggestions, the programmer refines those suggestions, and together, they work towards a solution. That solution is probably embedded in an IDE. Programmers might start with a rough description of what they want to do, in an imprecise, ambiguous language like English. The AI could respond with a sketch of what the solution might look like, possibly in pseudo-code. The programmer could then continue by filling in the actual code, possibly with extensive code completion (and yes, based on a model trained on all the code in GitHub or whatever). At this point, the IDE could translate the programmer’s code back into pseudo-code, using a tool like \\xa0Pseudogen (a promising new tool, though still experimental). Any writer, whether of prose or of code, knows that having someone tell you what they think you meant does wonders for revealing your own lapses in understanding. MISIM is another research project that envisions a collaborative role for AI.\\xa0 It watches the code that a developer is writing, extracting its meaning and comparing it with similar code.\\xa0 It then makes suggestions about rewriting code that looks buggy or inefficient, based on the structure of similar programs. Although its creators suggest that MISIM could eventually lead to machines that program themselves, that’s not what interests me; I’m more interested in the idea that MISIM is helping a human to write better code. AI is still not very good at detecting and fixing bugs; but it is very good at asking a programmer to think carefully when it looks like something is wrong. Is this pair programming with a machine? Maybe.\\xa0 It is definitely enlisting the machine as a collaborator, rather than as a surrogate. The goal isn’t to replace the programmers, but to make programmers better, to work in ways that are faster and more effective. Will it work? I don’t know; we haven’t built anything like it yet. It’s time to try.', 'Don’t push the responsibility of maintaining invariants required by your class on to its callers.  a scalable open-source multi-model database natively supporting graph, document and search. All supported data models & access patterns can be comnbined in queries allowing for maximal flexibility.', 'A VS Code extension for visualizing data structures while debugging. Like the VS Code’s watch view, but with rich visualizations of the watched value. an integrated dataflow environment for end-users. It allows users to interact with modules that implement functionality for different domains from a single user interface and combine these modules in creative ways. a library that allows you to write declarative logic programs in Rust, with a Datalog-like syntax From initial testing, the generated code is very fast. Variants of transitive closure for large graphs (~1000 nodes) run at comparable speed to compiled Souffle, and at a fraction of the compilation time.', 'Compared to the last few months, there are relatively few items about COVID. And almost no items about Blockchains, though the one item I’ve listed, about China’s Blockchain Services Network, may be the most important item here. I’m seeing a steady stream of articles about various forms of no-code/low-code programming. While many programmers scoff at the idea of programming-without-programming, spreadsheets are an early example of low-code programing. Excel is hardly insignificant. On the AI front, the most significant change is discussion (see the thread below) of a “Deep Learning Recession,” as companies under pressure from COVID look for results and can’t find them.', 'app that automatically tracks how you spend time on your devices I’ve done some previous digging into natural language SQL queries — there’s a good amount of research around this. But the error rate is always too high for useful production deployment and the systems I’ve looked at never handled ambiguity well. The target user for this is someone who knows nothing about SQL so ambiguity is guaranteed. I worked for Cleargraph, a startup which built a natural language query layer on top of RDBMSes and which we sold to Tableau. We reached the same conclusions as you: that such a system must properly handle ambiguous queries, and users need to explicitly understand the results they’re viewing. Futuristic Sci-Fi and Cyberpunk Graphical User Interface Framework for Web Apps.', 'This month’s collection of interesting articles that point to important trends is dominated by AI. That’s not surprising; AI has probably been the biggest single category all year. But its dominance over other topics seems to be increasing. That’s partly because there’s more research into why AI fails; partly because we’re beginning to see AI in embedded systems, ranging from giant gas and oil wells to the tiny devices that Pete Warden is working with.', 'The Covid-19 pandemic has changed how people and businesses spend and operate.\\xa0 Over the coming pages we’ll explore ways in which our current world is already very different from the one we knew just a few months ago, as well as predictions of our “new normal” once the proverbial boat stops rocking.\\xa0 Specifically, we’ll see this through the lens of decision-making: how has Covid-19 changed the way we think? And what does this mean for our purchase patterns and business models? You’re used to a certain level of uncertainty in your life, sure.\\xa0 But the pandemic has quickly turned up the uncertainty on even basic planning. Your dishwasher, piano, or clothes dryer is making an odd sound. Do you proactively call a repair service to check it out?\\xa0 Your ounce of prevention will also cost you two weeks’ wondering whether the repair technician was an asymptomatic carrier.\\xa0 If you hold off, you’re placing a bet that the appliance lasts long enough for treatment to become widely available, because you certainly don’t want it to break down just as infection rates spike. Stresses on a system reveal that some of our constants were really variables in disguise.\\xa0 “I can always leave my house.”\\xa0 “I can get to the gym on Friday.”\\xa0 “If I don’t go grocery shopping tonight, I can always do it tomorrow.\\xa0 It’s not like they’ll run out of food.”\\xa0 These weren’t exactly bold statements in January. \\xa0 But by March, many cities’ shelter-in-place orders had turned those periods into question marks. Even as cities are starting to relax those restrictions, there’s the worry that they may suddenly return as the virus continues to spread. As this reality sets in, some of us are even weighing what we call “acceptance purchases”: items which show that we’re in this for the long haul.\\xa0 Your gym isn’t closed, but it’s as good as closed since the city can quickly order it to shut down if local case counts climb again.\\xa0 So maybe it’s time to buy that fancy exercise bike.\\xa0 And ride-hailing services were appealing until using them increased your exposure to the virus.\\xa0 Maybe now you’ll buy that car you sometimes think about?\\xa0 You had considered downsizing your home, but you’ll appreciate the extra space if you’re spending more time indoors. Those sorts of purchases are meant to last you for years, though, which means they’re only wise investments if the pandemic (and its impact on the local economy) continues for a long time.\\xa0 What if we see improved prevention or widespread treatment within a few months?\\xa0 Do we want to try to offload an exercise bike or a car that we no longer need? The longer you hold off on making those decisions, the greater the chances that you’ll make those purchases too late. It’s tough to make a decision when you can’t rely on your near-term world falling within some certain, predictable scope.\\xa0 You try to keep all of your options open at all times so that you can be ready for any possibility.\\xa0 But that’s a lot of extra strain on your brain.\\xa0 And it’s tiring. The pandemic has made a number of businesses less desirable or outright inaccessible.\\xa0 Doing that work yourself reduces the impact around the uncertainty of when they’ll return.\\xa0 It’s also a lot more responsibility for you. Congratulations on running a restaurant, cafe, bar, cinema, gym, school, daycare, office, and storage facility.\\xa0 You get to buy workout equipment, cooking gear, hair care tools, teaching supplies, and anything else needed to backfill services to which you used to outsource. You’re responsible for the decisions on which models of equipment to buy, as well as the upkeep thereof. You suddenly need to know a lot of things about a lot of things, but you don’t have the time to become an expert in any one of them. Welcome to the diseconomies of non-scale: being small and self-sufficient is expensive. Like a factory, hotel event space, or a fast-food kitchen, you find yourself constantly partitioning or re-tooling rooms to compensate for your limited space.\\xa0 The gym becomes the reading room becomes the video meeting space becomes the work area.\\xa0 (You and your spouse flip a coin to see who gets the real office and who gets that basement corner.\\xa0 Hint: if you want the nicer space, make sure you’re on more video calls. And pretend that you can’t get Zoom backgrounds to work.) The kitchen table flips between a dining area and a school, three times a day.\\xa0 The bathroom becomes the hair salon every week and quickly switches back again.\\xa0 All of these swaps take time, effort, and mental energy, what economists collectively refer to as switching costs.\\xa0 They add up. Quickly. Nor is this just about the size of the home.\\xa0 It’s a matter of how much you were using it before the pandemic started, the number of spaces present, and the ratio of people to square feet.\\xa0 If you live in a sprawling, suburban house, but every room was already dedicated to some function or someone’s personal space, then you’re just marginally better off than the person occupying a small, urban apartment. This isn’t just about “working from home,” either.\\xa0 That usually means that you have a space set aside in your house to work and to take calls, and you have the house to yourself during working hours. Experienced remote-work professionals will tell you that we’re living a very different scenario.\\xa0 This increasing need to run a standalone, be-everything home means that we are suffering from the curse of generalism: we’re our children’s teachers, our cooks, our housekeepers, our barbers, our IT department, and our fix-it crew.\\xa0 We’re becoming more self-sufficient, but at the expense of having less time to specialize in our main jobs. All of this means that we’re spending a lot of time just getting by, and not much time advancing. In most places, you don’t need to be “connected” to get by day-to-day.\\xa0 Whom you know is less important because, with even a modest income, you can get most of what you need.\\xa0 You mostly care about diversifying your professional network, because that helps you to find a new job, which is what provides you the money that allows you to compensate for not knowing anyone else. When a pandemic triggers stresses in our supply chains, that idea breaks down.\\xa0 Whom you\\xa0 know in your personal sphere suddenly counts a lot more.\\xa0 Instead of money, it’s your social network that gets you through the day. Do you know someone whose job gives them leading indicators on the spread of the virus?\\xa0 Early in the pandemic, friends of medical professionals got some advance warning of what was to come.\\xa0 They could gather information from their professional spheres to let their personal networks know that something nasty was brewing.\\xa0 The same holds for anyone whose business sells protective gear or cleaning supplies.\\xa0 Over casual drinks, they might mention: “It’s weird … we’re getting a lot of new orders, and not from our usual customers.\\xa0 Something’s up.\\xa0 You may want to buy some extras, just in case.”\\xa0 (This, by the by, shows the value of keeping an eye on your company’s data.\\xa0 If you don’t have systems to tell you when your sales numbers are abnormal, you may miss information that you already had in-hand.\\xa0 And in this case, it would have been time-sensitive info.) Communities of shared experience are home to these socially-strong yet professionally-diverse networks.\\xa0 Family and close friendships top the list, with religious and ethnic ties running a close second.\\xa0 (People who were part of the same wave of immigration from the same country often forge ties that are as strong as family.)\\xa0 Neighbors and people who share a hobby are also in there, though to a lesser degree. Within these groups there’s always somebody who has a quick tip, someone who “knows a guy,” someone you can pull aside for a quick “Hey can I ask you about …”\\xa0 Maybe your niece works at a big grocery chain, and she can tell you when the shipments of hand sanitizer arrive.\\xa0 In December, this would have been a trivial mention.\\xa0 Today, when goods are scarce, this is timely information and it can make a difference. Personal networks often have the benefit of being geographically dispersed.\\xa0Your best friend can ship you cleaning supplies, since they are plentiful in his part of the country.\\xa0 Your extended family, which stretches from Paris to Singapore, can tell you how their cities are handling shelter-in-place rules.\\xa0 Chatting with those far-flung aunts and uncles gives you several weeks’ advance notice on how your city’s rules may turn out.\\xa0 That reduces your uncertainty, which makes it easier for you to prepare, which reduces your stress and decision fatigue. Your ability to forge new relationships can compensate for a smaller social network.\\xa0 If you don’t have a relative who works at Target, you can ask someone who works there, so long as you have the skill to spot whom to ask.\\xa0 You have to be able to read people, to see who would be receptive to that question. And you also need to tell whether this would be a simple favor, or something that merits monetary compensation.\\xa0 The value on that information just increased by a wide margin; shouldn’t the price follow? Relationship-building also counts in the B2B setting.\\xa0 Such was the case with grocery chain Trader Joe’s.\\xa0 They’ve managed to avoid shortages during this pandemic, most notably in toilet paper.\\xa0 When other stores seemed to run out, Trader Joe’s always magically had some in stock.\\xa0 That’s because they were able to strike a deal with an unnamed hotel chain to buy supplies that were going unused due to dramatic cuts in travel. Granted, Trader Joe’s very business model—white-labeling manufacturers’ goods—smoothed this road.\\xa0 But their ability to forge that relationship counted just as much as their ability to execute on selling the goods. We can reduce our pandemic-driven stresses by reducing the uncertainty.\\xa0 To do that, we can trace chains of knock-on effects to determine what changes are coming, and plan accordingly. \\xa0 For example:\\xa0 “many restaurants have closed up,” therefore, “there’s less waste from restaurants,” therefore, “there’s less food for rats,” therefore, “expect rats to get more bold.”\\xa0 So be careful when taking out your trash.\\xa0 “The pandemic has drastically cut air travel,” therefore “airlines will have less revenue,” therefore “airlines will furlough employees,” therefore “businesses those employees patronized—from in-airport restaurants to hotel shuttle services to their at-home economies—will suffer.” Though we can trace just one chain of effects at a time, multiple paths spin out of every “what next?” and spread out like a spider-crack in a window.\\xa0 They connect down the line to weave a fabric of impacts. Case in point: WSJ’s Scott McCartney points out that the sudden drop in air travel has upset airlines’ ability to set prices, since they take such a data-driven approach. People who work in the ML/AI field will tell you that this is not just an airline problem: a sudden shift will upend any predictive models built on past behaviors, regardless of industry.\\xa0 That will affect other fields’ dynamic pricing, yes, but also fraud detection (your credit card reflects a lot of outlier purchases, times, and locations since February) and demand forecasts (a knock-on effect of our collective outlier purchases).\\xa0 That, in turn, ties to inventory management, which is tied to supply chains, which involve all of the players in the shipping industry, which is tied to fuel consumption and vehicle maintenance… As with any tightly-coupled, complex system, all of these connections work in our favor until they suddenly don’t.\\xa0 Expect pandemic-related changes to cascade, revealing both endogenous and systemic problems that are related in unexpected ways. One problem with tight coupling, Charles Perrow notes in Normal Accidents, is that materials only have one path to take through the system.\\xa0 If a component in the middle breaks, everything backs up so the entire system is as good as broken.\\xa0 You can repair or re-create the old paths (when possible) or create new connections between components.\\xa0 In Covid-speak, that means our long-term solutions fall into “partially restoring and re-thinking the pre-pandemic life” and “creating new ways to handle the day-to-day when there’s a highly infectious disease running around.”\\xa0 There are business opportunities in both camps. We mostly assume the phrases “contactless” and “touch-free” refer to electronic payments.\\xa0 Those are very much in demand right now, but the touch-free space now extends to the wider notion of strangers not interacting in-person, and not handling the same objects at the same time.\\xa0 That opens the door to online learning, telemedicine, tele-anything.\\xa0 If you can provide your service at a distance, you have a lot of new prospects. Entertainment already had a firm footing in the online world thanks to video streaming services. The pandemic, and its dramatically reduced cinema attendance, has provided them even more leverage as some movies will have a shorter time on the big screen before they shift to online video.\\xa0 (As a side note, there’s another chain of knock-on effects to explore: since studios have been known to time releases to coincide with certain seasons and to have a better shot at industry awards, how will that change when films head into living rooms that much sooner?)\\xa0 Other groups, like Chicago’s Lyric Opera and New York’s Met Opera, are hosting performances online as their subscribers can no longer attend in-person. Still, it’s become more difficult for performances that rely on people being in the same space. Stand-up comedians from Nimesh Patel to Dave Chappelle have recently been able to pull off outdoor gigs with live-but-socially-distanced audiences. Fire-spinning and belly-dance performer Dawn Xiana Moon, of Raks Inferno and Raks Geek, combined multiple streaming services to simulate all of her performers being “on-stage” at the same time. This required her to leverage her technology background, a skill set that is admittedly rare in the live-act world, and she’d still prefer a single platform that just works. By comparison, TV and movie studios have yet to explain how they will manage to film while keeping cast and crew socially distant. The bottom line is that companies that create tools to improve filming multiple, simultaneous, geographically-dispersed teams will have a lot of customers.\\xa0 (And for movies, will we see an increase in animation to fill the gap?) People are also demanding more of their home internet infrastructure to support increased school- and work-related loads.\\xa0 (The authors know several people who have shelled out to their ISPs for greater bandwidth.)\\xa0 That also means a greater load on mid-tier services like social media sites, videoconference services, and the aforementioned streaming video platforms.\\xa0 If you sell networking hardware or provide network operations services to those companies, you will have no shortage of work. If the pandemic continues long enough, we expect to see a deeper penetration of home broadband service, especially wireless broadband.\\xa0 This is another touch-free offering, as it permits your provider to establish and troubleshoot internet connectivity issues without sending a technician into your home.\\xa0 (As another knock-on effect, this means providers will be able to limit field technicians’ service radius to their towers and datacenters, which should let them cover more territory on the same number of staff.) Traditional, multi-year lease commercial real estate was already experiencing disruption due to coworking spaces.\\xa0 They’ll now both suffer as companies rethink their post-pandemic office needs.\\xa0 Doubly so since some newly-remote workers are taking the opportunity to move out of state. (Yet another knock-on effect: without office workers, what will happen to the lunch spots and bars that lined the dense urban-business landscape?)\\xa0 You also have retailers abandoning spaces since there are far too few customers to browse stores. Two types of consumers may pick up that inventory, though.\\xa0 The first, in the short term, Amazon may convert some old mall spaces into distribution centers. Other businesses will undoubtedly find ways to repurpose empty urban office spaces at deep-discount prices. Second, and in the longer term, we’ll accept that our homes are simply not large enough to be our Everything Place.\\xa0 People who choose to remain in dense urban environments will want their apartments to be more like standalone houses, which means having space for in-unit washer/dryer, multiple bathrooms, and multiple rooms to serve as offices. Perhaps cities will divvy up old office buildings into large apartments to meet that need.\\xa0 That’s admittedly more of a stretch, if for no other reason than the time scale involved for the construction effort and the zoning law changes.\\xa0 For now, some percentage of urban residents will simply pack up for the suburbs, or even more rural areas out of state. Let’s face it: if the restaurant scene has dwindled and public transit feels like too much of a coronavirus risk, then urban living has lost a lot of its luster. Wherever we choose to live, we’ll need more support for running our homes.\\xa0 This will include ways to make the most of our limited space, such as smaller-scale workout equipment and compact storage, and increased support for DIY repairs, like video tutorials from manufacturers on how to service their products.\\xa0 This is another stretch, but if the pandemic lasts long enough, manufacturers will modify their products to make them easier to service.\\xa0 That, or cheaper to just throw away and replace when they encounter a problem. The difficulties of schooling don’t end with table space and bandwidth needs.\\xa0 There are also the socioemotional concerns such as college students learning how to live away from home, and how the K-12 set learns to socialize when they don’t interact in person.\\xa0 Not to mention, who will do the teaching?\\xa0 In March, when stay-at-home orders started to hit US cities, many parents suddenly had to balance their full-time jobs with being full-time teachers.\\xa0 (Technology consultants Sarah Aslanifar and Bobby Norton jokingly refer to their new roles as, “working from home-school.”)\\xa0 Businesses took a double hit as they had to scramble to find a way for people to work from home, and then those same people spent the next several weeks distracted during the workday. Some parents have since formed social “pods” with neighbors whom they trust to perform compatible pandemic hygiene.\\xa0 Some of those have evolved into educational pods, wherein parents spring for someone to teach their group of kids.\\xa0 An article in MIT Tech Review mentions a price tag of $10,000 per student, per semester.\\xa0 This isn’t accessible to everyone; but for high-earning parents, it’s a simple economic decision: the cost to outsource schooling is smaller than the amount of money they’ll earn when they can perform their day jobs at full capacity. Higher education was already experiencing some disruption—boot camps and certificate programs on one end, and students questioning their post-college job prospects on the other—and the onset of the pandemic has increased the pressure.\\xa0 This goes beyond the last few months of sorting out whether and how to open campuses for autumn 2020.\\xa0 Parents and students alike also question the price tag of a fancy four-year college when students will be attending classes from their kitchen table.\\xa0 (One SNL sketch framed the experience as “University of Phoenix Online, with worse tech support.”)\\xa0 For the time being, colleges can busy themselves by shoring up courseware and videoconferencing platforms in order to set autumn 2020 classes in motion.\\xa0 They’ll quickly need to sort out other near-term concerns (shoring up lost profits from empty student housing) as well as their future prospects (demonstrating their value compared to vocational programs, especially if the job market suffers over the long term). \\xa0 If schools can’t sort this out on their own, they’ll likely pay someone to sort it out for them.\\xa0 There’s also a business opportunity in providing a centralized, one-stop SaaS platform such that colleges won’t have to cobble together their own with a mix of one-off tools. One silver lining of working from home is that your job prospects just opened up.\\xa0 Covid-19 has forced a lot of companies to admit that the old “this work can’t be done from home” excuse doesn’t hold up.\\xa0 Some of them are even starting to like it: they see how much money they were burning on an office for people who already knew they’d be more effective working from home.\\xa0 Many of them will scratch that line item from next year’s budget. This means we’ll see more remote hiring in the sectors that can support it.\\xa0 That will establish a clear boundary between the companies that see the benefits (“we’re now able to hire across the country for these hard-to-fill roles”) and those that do not (“we’re only hiring people who live in this city, for when we go back to the office”).\\xa0 Big tech-sector names like Google and Facebook have already announced plans to extend work-from-home support, while Twitter and Atlassian have flat-out said that their crews can work from home indefinitely.\\xa0 In some fields, failing to provide a remote-work option may limit your talent pool.\\xa0 It will be the equivalent of running an office space in the suburbs when most companies, and their prospective employees, exist in the dense urban center and have no desire to commute. Just as we’ll pay for help adapting to the current state of things, we’ll also pay for some semblance of “the old normal.” People generally like meeting up, whether one-on-one for a tea or in larger groups for a party.\\xa0 We’re already using videoconferencing tools to hang out with friends and family, and to attend events.\\xa0 But we’re adapting to the tools more than the other way around.\\xa0 Right now services like Hangouts, Meet, and\\xa0 Zoom are still very much designed for, well, video versions of office conference calls: one person speaks at a time, and you get a “Brady Bunch” grid view of attendees.\\xa0 Expect the incumbent vendors as well as new upstarts to create tools that are better suited for [specific interaction]-over-video, like conferences, classroom teaching, or music lessons. We’re really feeling this in online conferences.\\xa0 While webinar tools fulfill the mission of letting a person deliver a talk to a large number of attendees, they don’t support other aspects of an in-person event.\\xa0 Randomly bumping into people and “hallway track” sessions have forged long-term bonds between conference attendees, far more than the talks themselves.\\xa0 This could serve as a driver for VR, as that will take us away from “attending events from our living room” to “being in our living room, but actually attending events in a dedicated space.”\\xa0 There is a big difference. Another reason people meet up is to play games.\\xa0 Online games are nothing new, and they’ve even gained some mainstream street cred thanks to casual gaming.\\xa0 Expect to see improved coordination, such that you can play with people of your choosing (a feature lacking in a number of iOS Game Center offerings).\\xa0 People playing more video games may also lead to greater participation in esports leagues, and even taking business meetings over a gaming session. In-person interaction is our most risky form of socializing at the moment, but it’s also the one people want the most. Goods and services that help us to (safely) meet face-to-face will not just help us on an emotional level, but they could play a key role in helping the economy get back on its feet. We have masks and face shields, which are good for being in public.\\xa0 What about protective overgarments, reminiscent of 1950s interpretations of outer-space wear?\\xa0 We could wear them to protect our entire body in public transit or airplanes, and then shed them before entering a friend’s home.\\xa0 There’s also the down-to-earth business of designing and installing plastic shields between restaurant tables.\\xa0 Maybe someone will create transparent, oversized cabins that allow you and a few trusted friends to be “on the beach” but still be indoors and away from others. Meeting in person also counts for office space.\\xa0 In a work-from-home world, some teams still prefer the in-person experience.\\xa0 What can we do to make it safer to be in the office, beyond standing several feet apart at all times?\\xa0 An effective but low-tech offering could involve installing protective shields around conference tables (not unlike what we see in some restaurants) or modifying office layouts to discourage crowding.\\xa0 The next step up would increase touch-free actions, such as choosing your elevator floor through a smartphone app.\\xa0 Larger and higher-tech offerings would go deep into the guts of the building to install virus purifiers in building HVAC systems and the accompanying ductwork. Where do we go from here?\\xa0 That depends how long we go without treatment or improved preventative measures.\\xa0 One thing’s for sure: Covid-19 is a driver of change.\\xa0 There is no more “normal” in terms of how we shop for groceries, attend events, or even lay out our homes.\\xa0 It’s up to us to adapt to our present, even as that present continues to change, and that will influence how we decide what to buy and sell. How much we change, as people, depends on how long the pandemic lasts.\\xa0 It’s possible that it will carve deep grooves in our collective social memory, similar to the Great Depression, and its impact will influence how people behave long after the disease is a threat. It also depends on how much we are willing to adapt.\\xa0 That is a function of how soon we’re willing to let go of “normal,” which is really a euphemism for “the past.”\\xa0 Especially since the past is heavily mythologized.', 'It is primarily written for the survey respondents, and anyone dealing with burn-out and resilience issues either in themselves, family members and employees. If you’re only interested in how to address burn-out skip to section seven. Ethics committees have at least three roles to play. The first is education. […] The second role of ethics committees is policy formation and review. […] The third role of ethics committees is to provide ethical consultation. [T]echnological decisions are not only about facts (for example, about what is more efficient), but also about the kind of life we want and the kind of society we strive to build. Simple, yet powerful ORM for modeling and querying data. (a) Schema As Code – model any database schema as Go objects. (b) Easily Traverse Any Graph – run queries, aggregations and traverse any graph structure easily. (c) Statically Typed And Explicit API – 100% statically typed and explicit API using code generation. (d) Multi Storage Driver – supports MySQL, PostgreSQL, SQLite and Gremlin.', 'Individuals typically “hire” communities to accomplish transitions that require human connection. Why do people join communities?; Member quality determines community success; Design your community to spark quality interactions; The two levels of group cohesion; Recognizing and retaining key members; Growing your ranks; A Time to Build.', 'grouped 10 design space dimensions into four major stages of a data science workflow: importing data into notebooks (data sources), editing code and prose (editor style, supported programming languages, versioning, collaboration), running code to generate outputs (cell execution order, liveness [6], execution environment, and cell outputs), and publishing notebook outputs. an open library for computational embroidery with Processing. Neuromorphic chips are packed with artificial neurons and artificial synapses that mimic the activity spikes that occur within the human brain—and they handle all this processing on the chip. This results in smarter, far more energy-efficient computing systems.', 'So you need to redesign your company’s data infrastructure. Do you buy a solution from a big integration company like IBM, Cloudera, or Amazon?\\xa0 Do you engage many small startups, each focused on one part of the problem?\\xa0 A little of both?\\xa0 We see trends shifting towards focused best-of-breed platforms. That is, products that are laser-focused on one aspect of the data science and machine learning workflows, in contrast to all-in-one platforms that attempt to solve the entire space of data workflows. This article, which examines this shift in more depth, is an opinionated result of countless conversations with data scientists about their needs in modern data science workflows. Today we see two different kinds of offerings in the marketplace: Integrated all-in-one platforms assemble many tools together, and can therefore provide a full solution to common workflows. They’re reliable and steady, but they tend not to be exceptional at any part of that workflow and they tend to move slowly. For this reason, such platforms may be a good choice for companies that don’t have the culture or skills to assemble their own platform. In contrast, best-of-breed products take a more craftsman approach: they do one thing well and move quickly (often they are the ones driving technological change). They usually meet the needs of end users more effectively, are cheaper, and easier to work with.\\xa0 However some assembly is required because they need to be used alongside other products to create full solutions.\\xa0 Best-of-breed products require a DIY spirit that may not be appropriate for slow-moving companies. Which path is best? This is an open question, but we’re putting our money on best-of-breed products. We’ll share why in a moment, but first, we want to look at a historical perspective with what happened to data warehouses and data engineering platforms. Historically, companies bought Oracle, SAS, Teradata or other data all-in-one data warehousing solutions. These were rock solid at what they did–and “what they did” includes offering packages that are valuable to other parts of the company, such as accounting–but it was difficult for customers to adapt to new workloads over time. Next came data engineering platforms like Cloudera, Hortonworks, and MapR, which broke open the Oracle/SAS hegemony with open source tooling. These provided a greater level of flexibility with Hadoop, Hive, and Spark. However, while Cloudera, Hortonworks, and MapR worked well for a set of common data engineering workloads, they didn’t generalize well to workloads that didn’t fit the MapReduce paradigm, including deep learning and new natural language models. As companies moved to cloud, embraced interactive Python, integrated GPUs, or moved to a greater diversity of data science and machine learning use cases, these data engineering platforms weren’t ideal. Data scientists rejected these platforms and went back to working on their laptops where they had full control to play around and experiment with new libraries and hardware. While data engineering platforms provided a great place for companies to start building data assets, their rigidity becomes especially challenging when companies embrace data science and machine learning, both of which are highly dynamic fields with heavy churn that require much more flexibility in order to stay relevant. An all-in-one platform makes it easy to get started, but can become a problem when your data science practice outgrows it. So if data engineering platforms like Cloudera displaced data warehousing platforms like SAS/Oracle, what will displace Cloudera as we move into the data science/machine learning age? The worlds of data science and machine learning move at a much faster pace than data warehousing and much of data engineering.\\xa0 All-in-one platforms are too large and rigid to keep up.\\xa0 Additionally, the benefits of integration are less relevant today with technologies like Kubernetes.\\xa0 Let’s dive into these reasons in more depth. “Data science” is an incredibly broad term that encompasses dozens of activities like ETL, machine learning, model management, and user interfaces, each of which have many rapidly evolving choices. Only part of a data scientist’s workflow is typically supported by even the most mature data science platforms. Any attempt to build a one-size-fits-all integrated platform would have to include such a wide range of features, and such a wide range of choices within each feature, that it would be extremely difficult to maintain and keep up to date.\\xa0 What happens when you want to incorporate real-time data feeds? What happens when you want to start analyzing time series data?\\xa0 Yes, the all-in-one platforms will have tools to meet these needs; but will they be the tools you want, or the tools you’d choose if you had the opportunity? Consider user interfaces. Data scientists use many tools like Jupyter notebooks, IDEs, custom dashboards, text editors, and others throughout their day. Platforms offering only “Jupyter notebooks in the cloud” cover only a small fraction of what actual data scientists use in a given day. This leaves data scientists spending half of their time in the platform, half outside the platform, and a new third half migrating between the two environments. Consider also the computational libraries that all-in-one platforms support, and the speed at which they go out of date quickly. Famously, Cloudera ran Spark 1.6 for years after Spark 2.0 was released–even though (and perhaps because) Spark 2.0 was released only 6 months after 1.6. It’s quite hard for a platform to stay on top of all of the rapid changes that are happening today. They’re too broad and numerous to keep up with. While the variety of data science has made all-in-one platforms harder, at the same time advances in infrastructure have made integrating best-of-breed products easier. Cloudera, Hortonworks, and MapR were necessary at the time because Hadoop, Hive, and Spark were notoriously difficult to set up and coordinate. Companies that lacked technical skills needed to buy an integrated solution. But today things are different. Modern data technologies are simpler to set up and configure. Also, technologies like Kubernetes and the cloud help to commoditize configuration and reduce integration pains with many narrowly-scoped products. Kubernetes lowers the barrier to integrating new products, which allows modern companies to assimilate and retire best-of-breed products on an as-needed basis without a painful onboarding process. For example, Kubernetes helps data scientists deploy APIs that serve models (machine learning or otherwise), build machine learning workflow systems, and is an increasingly common substrate for web applications that allows data scientists to integrate OSS technologies, as reported here by Hamel Hussain, Staff Machine Learning Engineer at Github. Kubernetes provides a common framework in which most deployment concerns can be specified programmatically.\\xa0 This puts more control into the hands of library authors, rather than individual integrators.\\xa0 As a result the work of integration is greatly reduced, often just specifying some configuration values and hitting deploy.\\xa0 A good example here is the Zero to JupyterHub guide.\\xa0 Anyone with modest computer skills can deploy JupyterHub on Kubernetes without knowing too much in about an hour.\\xa0 Previously this would have taken a trained professional with pretty deep expertise several days. We believe that companies that adopt a best-of-breed data platform will be more able to adapt to technology shifts that we know are coming. Rather than being tied into a monolithic data science platform on a multi-year time scale, they will be able to adopt, use, and swap out products as their needs change.\\xa0 Best of breed platforms enable companies to evolve and respond to today’s rapidly changing environment. The rise of the data analyst, data scientist, machine learning engineer and all the satellite roles that tie the decision function of organizations to data, along with increasing amounts of automation and machine intelligence, require tooling that meet these end users’ needs. These needs are rapidly evolving and tied to open source tooling that is also evolving rapidly. Our strong opinion (strongly held) is that best-of-breed platforms are better positioned to serve these rapidly evolving needs by building on these OSS tools than all-in-platforms. We look forward to finding out.  1 Note that we’re discussing data platforms that are built on top of OSS technologies, rather than the OSS technologies themselves. This is not another Dask vs Spark post, but a piece weighing up the utility of two distinct types of modern data platforms.', 'Sinter uses the user-mode EndpointSecurity API to subscribe to and receive authorization callbacks from the macOS kernel, for a set of security-relevant event types. The current version of Sinter supports allowing/denying process executions; in future versions we intend to support other types of events such as file, socket, and kernel events. We hate being wronged, and it makes us vengeful. On the other hand, we don’t necessarily love being “done right by,” and we don’t have a particular motivation that comes from it. There’s no “positive” version of revenge. These are useful steps but will not stop QAnon from spreading in social media comments or private chat groups or unmoderated forums. It’s not something we can reasonably hope for, and I don’t think there’s any technological solution (e.g. browser extensions) either. The only way to stop people from mistaking speculation from fact is for them to want to stop.', 'StackOverflow’s 2020 developer survey included a table showing the\\xa0 “most loved, dreaded, and wanted languages.” Loved and wanted languages are, well, sort of boring. The programming languages we dread are a lot more interesting. As Tolstoy said, “All happy families are alike; each unhappy family is unhappy in its own way.” So what are these unhappy, unloved languages, and why do programmers dread using them? Given the chance it’s, well, hard to resist jumping in with some theories, and perhaps even saying something impolitic.\\xa0 Or defending some languages that are disliked for the wrong reasons. More precisely, StackOverflow tabulated the “% of developers who are developing with the language or technology but have not expressed interest in continuing to do so.” That doesn’t sound quite as dire as “dreaded”; “not expressing an interest in working with a language again” is a rather vague indication of dread. There are lots of things I’ve done that I’d rather not do again, including writing troff macros that spawned shell scripts. But we won’t worry about that, will we? The list of least liked languages is similar to the lists of the most widely used languages, as indicated by Redmonk, Tiobe and, for that matter, searches on O’Reilly Learning. That’s no surprise; Bjarne Stroustrup said that “there are only two kinds of languages: the ones people complain about and the ones nobody uses.” And that makes a lot of sense, at least in relation to this survey. If you’ve got millions of users, it’s not hard to get a lot of people to dislike you. So seeing perennials like C alongside relative newcomers like Java on the list of disliked languages isn’t surprising. Kevlin Henney and I thought that the list of least liked languages also reflected the opinions of programmers who were working on large and legacy projects, as opposed to short programs. Dislike of a language may be “guilt by association”: dislike of a large, antiquated codebase with minimal documentation, and an architectural style in which every bug fixed breaks something else. Therefore, it’s not surprising to see languages that used to be widely used but have fallen from popularity on the list. And it’s also easy to fall in love with a quirky language that was perfect for one project, but that you’ll never see again.\\xa0 (In my case, that’s Icon. Try it; you might like it. It’s not on anyone’s list.) What’s most surprising is when a language is out of place: when it’s significantly more or less disliked than you expect. That’s what I’d like to think about. So, having disposed of the preliminaries, here are a few observations: There’s lots more that could be said.\\xa0 There’s no surprise that VBA is #1 disliked language.\\xa0 I’ll admit to complete ignorance on Objective C (#2), which I’ve never had any reason to play with. Although I’m a Perl-hater from way back, I’m surprised that Perl is so widely disliked (#3), but some wounds never heal. It will be interesting to see what happens after Perl 7 has been out for a few years. Assembly (#4) is an acquired taste (and isn’t a single language).\\xa0 If you don’t learn to love it, you pretty much have to hate it. And if you don’t love it, you really shouldn’t be using it. You can almost always avoid assembly, but when you need to work directly with the hardware, there’s no alternative.\\xa0 C and C++ (#5 and #8, respectively) give you a lot of rope to hang yourself, but get you as close enough to the hardware for almost any project, without the pain of assembly.\\xa0 Are they fading into the past, or will they be with us forever?\\xa0 My guess is the latter; there are too many projects that demand C’s performance and ubiquity. It’s the foundation for just about everything important in modern computing. Speculating about languages and why they’re liked or disliked is fun.\\xa0 It may or may not be useful.\\xa0 Take it for what it’s worth.', 'lightweight, high-speed immutable database for systems and applications. Open Source and easy to integrate into any existing application. Japanese startup Donut Robotics […] created a smart mask — a high-tech upgrade to standard face coverings, designed to make communication and social distancing easier. In conjunction with an app, the C-Face Smart mask can transcribe dictation, amplify the wearer’s voice, and translate speech into eight different languages. a Cyber Punk inspired, Text Based MMORPG Browser Game where gameplay interfaces are ‘Stealthily’ mimicking the VSCode interface. Pysa performs iterative rounds of analysis to build summaries to determine which functions return data from a source and which functions have parameters that eventually reach a sink. If Pysa finds that a source eventually connects to a sink, it reports an issue.', 'I have a system with c servers, each of which can only handle a single concurrent request, and has no internal queuing. The servers sit behind a load balancer, which contains an infinite queue. An unlimited number of clients offer c * 0.8 requests per second to the load balancer on average. In other words, we increase the offered load linearly with c to keep the per-server load constant. Once a request arrives at a server, it takes one second to process, on average. How does the client-observed mean request team vary with c? Crush is an attempt to make a traditional command line shell that is also a modern programming language. It has the features one would expect from a modern programming language like a type system, closures and lexical scoping, but with a syntax geared toward both batch and interactive shell usage. DGL-KE is a high performance, easy-to-use, and scalable package for learning large-scale knowledge graph embeddings. The package is implemented on the top of Deep Graph Library (DGL) and developers can run DGL-KE on CPU machine, GPU machine, as well as clusters', 'With one of the devices up and running, you can point NyanSat’s antenna to specific coordinates in the sky and listen for the radio frequency transmissions coming from a satellite that’s out there.', ' McIlroy keeps coming up. He’s the smartest of all of us and the least remembered (or written down)… McIlroy sat there and wrote —on a piece of paper, now, not on a computer— TMG [a proprietary yacc-like program] written in TMG… And then! He now has TMG written in TMG, he decided to give his piece of paper to his piece of paper and write down what came out (the code). Which he did. And then he came over to my editor and he typed in his code, assembled it, and (I won’t say without error, but with so few errors you’d be astonished) he came up with a TMG compiler, on the PDP-7, written in TMG. And it’s the most basic, bare, impressive self-compilation I’ve ever seen in my life. all of the ROS World videos, including all the lightning talks we propose a simple approach called Language Shaped Learning (LSL): if we have access to explanations at training time, we encourage the model to learn representations that are not only helpful for classification, but are predictive of the language explanations. Mondays: Algorithms; Wednesdays: Theory of Computation; Fridays: Theory of Computation; Sundays: Livestream/bonus', 'I thought July was going to be a dull month, but I’m wrong again. COVID-specific technology seems to be drying up, though there’s a fascinating report about a DIY vaccine. (Developed by serious scientists, so don’t try this at home.) There’s a lot of news about AI, and specifically, about the GPT-2 and GPT-3 language models. And a few things that are just fun, like Festo’s Bionic Swifts.', 'It’s typical that a log will be accessed zero times. Collecting, aggregating, and indexing logs is usually a mistake made by people who aren’t clear on the use case for the logs. an ultra-lightweight Virtual DOM, highly-optimized diff algorithm, and state management library obsessed with minimalism. Update the database; scale vertically; leverage application code; use efficient data types; data normalization and denormalization; precompute data; leverage materialized views; use proper indexes; leverage the execution plan for query optimization; choose correct transaction isolation level; bulk INSERTs and UPDATEs; compress data for storage; make ALTER TABLEs work; manage concurrent connections; add read replicas; disk partitioning; use specialized extensions; sharding; don’t store everything in one table; process data outside the SQL database; be aware of the limitations of managed SQL databases.  We conclude that it appears next to impossible to find secret questions that are both secure and memorable. Secret questions continue have some use when combined with other signals, but they should not be used alone and best practice should favor more reliable alternatives.', 'tl;dr: I made a prototype IDE in which language semantics are specified in datalog, powered by a datalog interpreter written in TypeScript, running the browser. Demo here. Model Cards […] provide a structured framework for reporting on ML model provenance, usage, and ethics-informed evaluation and give a detailed overview of a model’s suggested uses and limitations. […] To streamline the creation of Model Cards for all ML practitioners, we are sharing the Model Card Toolkit (MCT), a collection of tools that support developers in compiling the information that goes into a Model Card and that aid in the creation of interfaces that will be useful for different audiences. Dafny is a language that is designed to make it easy to write correct code. This means correct in the sense of not having any runtime errors, but also correct in actually doing what the programmer intended it to do. To accomplish this, Dafny relies on high-level annotations to reason about and prove correctness of code. The effect of a piece of code can be given abstractly, using a natural, high-level expression of the desired behavior, which is easier and less error prone to write. Dafny then generates a proof that the code matches the annotations (assuming they are correct, of course!). Dafny lifts the burden of writing bug-free code into that of writing bug-free annotations. This is often easier than writing the code, because annotations are shorter and more direct.', 'Prefer to push fixes upstream instead of working around problems downstream. In October, 1953, I coined the word ‘software.’ In this article, we turn our attention to the process itself: how do you bring a product to market?', '   Product Managers are responsible for the successful development, testing, release, and adoption of a product, and for leading the team that implements those milestones. Product managers for AI must satisfy these same responsibilities, tuned for the AI lifecycle. In the first two articles in this series, we suggest that AI Product Managers (AI PMs) are responsible for: If you’re an AI product manager (or about to become one), that’s what you’re signing up for.\\xa0 In this article, we turn our attention to the process itself: how do you bring a product to market? The first step in building an AI solution is identifying the problem you want to solve, which includes defining the metrics that will demonstrate whether you’ve succeeded. It sounds simplistic to state that AI product managers should develop and ship products that improve metrics the business cares about. Though these concepts may be simple to understand, they aren’t as easy in practice.  It’s often difficult for businesses without a mature data or machine learning practice to define and agree on metrics. Politics, personalities, and the tradeoff between short-term and long-term outcomes can all contribute to a lack of alignment. Many companies face a problem that’s even worse: no one knows which levers contribute to the metrics that impact business outcomes, or which metrics are important to the company (such as those reported to Wall Street by publicly-traded companies). Rachel Thomas writes about these challenges in “The problem with metrics is a big problem for AI.” There isn’t a simple fix for these problems, but for new companies, investing early in understanding the company’s metrics ecosystem will pay dividends in the future.\\xa0 The worst case scenario is when a business doesn’t have any metrics. In this case, the business probably got caught up in the hype about AI, but hasn’t done any of the preparation. (Fair warning: if the business lacks metrics, it probably also lacks discipline about data infrastructure, collection, governance, and much more.) Work with senior management to design and align on appropriate metrics, and make sure that executive leadership agrees and consents to using them before starting your experiments and developing your AI products in earnest. Getting this kind of agreement is much easier said than done, particularly because a company that doesn’t have metrics may never have thought seriously about what makes their business successful. It may require intense negotiation between different divisions, each of which has its own procedures and its own political interests. As Jez Humble said in a Velocity Conference training session, “Metrics should be painful: metrics should be able to make you change what you’re doing.” Don’t expect agreement to come simply. Lack of clarity about metrics is technical debt worth paying down. Without clarity in metrics, it’s impossible to do meaningful experimentation. A product manager needs to think about ethics–and encourage the product team to think about ethics–throughout the whole product development process, but it’s particularly important when you’re defining the problem.\\xa0 Is it a problem that should be solved?\\xa0 How can the solution be abused? Those are questions that every product team needs to think about. There’s a substantial literature about ethics, data, and AI, so rather than repeat that discussion, we’ll leave you with a few resources.\\xa0 Ethics and Data Science is a short book that helps developers think through data problems, and includes a checklist that team members should revisit throughout the process. The Markkula Institute at the University of Santa Clara has an excellent list of resources, including an app to aid ethical decision-making. The Ethical OS also provides excellent tools for thinking through the impact of technologies.\\xa0 And finally–build a team that includes people of different backgrounds, and who will be affected by your products in different ways. It’s surprising (and saddening) how many ethical problems could have been avoided if more people thought about how the products would be used. AI is a powerful tool: use it for good. Once you know which metrics are most important, and which levers affect them, you need to run experiments to be sure that the AI products you want to develop actually map to those business metrics.  Experiments allow AI PMs not only to test assumptions about the relevance and functionality of AI Products, but also to understand the effect (if any) of AI products on the business. AI PMs must ensure that experimentation occurs during three phases of the product lifecycle: AI product managers need to understand how sensitive their project is to error. This isn’t always simple, since it doesn’t just take into account technical risk; it also has to account for social risk and reputational damage. As we mentioned in the first article of this series, an AI application for product recommendations can make a lot of mistakes before anyone notices (ignoring concerns about bias); this has business impact, of course, but doesn’t cause life-threatening harm. On the other hand, an autonomous vehicle really can’t afford to make any mistakes; even if the autonomous vehicle is safer than a human driver, you (and your company) will take the blame for any accidents. AI PMs have to make tough choices when deciding where to apply limited resources. It’s the old “choose two” rule, where the parameters are Speed, Quality, and Features. For example, for a mobile phone app that uses object detection to identify pets, speed is a requirement. A product manager may sacrifice either a more diverse set of animals, or the accuracy of detection algorithms.\\xa0 These decisions have dramatic implications on project length, resources, and goals. Similarly, AI product managers often need to choose whether to prioritize the scale and impact of a product over the difficulty of product development.\\xa0 Years ago a health and fitness technology company realized that its content moderators, used to manually detect and remediate offensive content on its platform, were experiencing extreme fatigue and very poor mental health outcomes.\\xa0 Even beyond the humane considerations, moderator burnout was a serious product issue, in that the company’s platform was rapidly growing, thus exposing the average user to more potentially offensive or illegal content.\\xa0 The difficulty of content moderation work was exacerbated by its repetitive nature, making it a candidate for automation via AI.\\xa0 However, the difficulty of developing a robust content moderation system at the time was significant, and would have required years of development time and research. Ultimately, the company decided to simply drop the most social components of the platform, a decision which limited overall growth. This tradeoff between impact and development difficulty is particularly relevant for products based on deep learning: breakthroughs often lead to unique, defensible, and highly lucrative products, but investing in products with a high chance of failure is an obvious risk. Products based on deep learning can be difficult (or even impossible) to develop; it’s a classic “high return versus high risk” situation, in which it is inherently difficult to calculate return on investment. The final major tradeoff that AI product managers must evaluate is how much time to spend during the R&D and design phases.\\xa0 With no restrictrictions on release dates, PMs and engineers alike would choose to spend as much time as necessary to nail the product goals. But in the real world, products need to ship, and there’s rarely enough time to do the research necessary to ship the best possible product. Therefore, product managers must make a judgment call about when to ship, and that call is usually based on incomplete experimental results. It’s a balancing act, and admittedly, one that can be very tricky: achieving the product’s goals versus getting the product out there. As with traditional software, the best way to achieve your goals is to put something out there and iterate. This is particularly true for AI products. Microsoft, LinkedIn, and Airbnb have been especially candid about their journeys towards building an experiment-driven culture and the technology required to support it. Some of the best lessons are captured in Ron Kohavi, Diane Tang, and Ya Xu’s book: Trustworthy Online Controlled Experiments : A Practical Guide to A/B Testing. The development phases for an AI project map nearly 1:1 to the AI Product Pipeline we described in the second article of this series.  AI projects require a “feedback loop” in both the product development process and the AI products themselves. Because AI products are inherently research-based, experimentation and iterative development are necessary. Unlike traditional software development, in which the inputs and results are often deterministic, the AI development cycle is probabilistic. This requires several important modifications to how projects are set up and executed, regardless of the project management framework. Product managers must ensure that AI projects gather qualitative information about customer behavior. Because it might not be intuitive, it’s important to note that traditional data measurement tools are more effective at measuring magnitude than sentiment. For most AI products, the product manager will be less interested in the click-through rate (CTR) and other quantitative metrics than they are in the utility of the AI product to the user. Therefore, traditional product research teams must engage with the AI team to ensure that the correct intuition is applied to AI product development, as AI practitioners are likely to lack the appropriate skills and experience. CTRs are easy to measure, but if you build a system designed to optimize these kinds of metrics, you might find that the system sacrifices actual usefulness and user satisfaction. In this case, no matter how well the AI product contributes to such metrics, it’s output won’t ultimately serve the goals of the company. It’s easy to focus on the wrong metric if you haven’t done the proper research. One mid-sized digital media company we interviewed reported that their Marketing, Advertising, Strategy, and Product teams once wanted to build an AI-driven user traffic forecast tool. The Marketing team built the first model, but because it was from marketing, the model optimized for CTR and lead conversion. The Advertising team was more interested in cost per lead (CPL) and lifetime value (LTV), while the Strategy team was aligned to corporate metrics (revenue impact and total active users).\\xa0 As a result, many of the tool’s users were dissatisfied, even though the AI functioned perfectly. The ultimate result was the development of multiple models that optimize for different metrics, and the redesign of the tool so that it could present those outputs clearly and intuitively to different kinds of users. Internally, AI PMs must engage stakeholders to ensure alignment with the most important decision-makers and top-line business metrics. Put simply, no AI product will be successful if it never launches, and no AI product will launch unless the project is sponsored, funded, and connected to important business objectives. This phase of an AI project is laborious and time consuming, but completing it is one of the strongest indicators of future success. A product needs to balance the investment of resources against the risks of moving forward without a full understanding of the data landscape. Acquiring data is often difficult, especially in regulated industries. Once relevant data has been obtained, understanding what is valuable and what is simply noise requires statistical and scientific rigor.\\xa0 AI product managers probably won’t do the research themselves; their role is to guide data scientists, analysts, and domain experts towards a product-centric evaluation of the data, and to inform meaningful experiment design. The goal is to have a measurable signal for what data exists, solid insights into that data’s relevance, and a clear vision of where to concentrate efforts in designing features. Data wrangling and feature engineering is the most difficult and important phase of every AI project.\\xa0 It’s generally accepted that, during a typical product development cycle, 80% of a data scientist’s time is spent in feature engineering. Trends and tools in AutoML and Deep Learning have certainly reduced the time, skills, and effort required to build a prototype, if not an actual product.\\xa0 Nonetheless, building a superior feature pipeline or model architecture will always be worthwhile. AI product managers should make sure project plans account for the time, effort, and people needed. The modeling phase of an AI project is frustrating and difficult to predict. The process is inherently iterative, and some AI projects fail (for good reason) at this point. It’s easy to understand what makes this step difficult: there is rarely a sense of steady progress towards a goal. You experiment until something works; that might happen on the first day, or the hundredth day. An AI product manager must motivate the team members and stakeholders when there is no tangible “product” to show for everyone’s labor and investment.\\xa0 One strategy for maintaining motivation is to push for short-term bursts to beat a performance baseline. Another would be to start multiple threads (possibly even multiple projects), so that some will be able to demonstrate progress. Unlike traditional software engineering projects, AI product managers must be heavily involved in the build process. Engineering managers are usually responsible for making sure all the components of a software product are properly compiled into binaries, and for organizing build scripts meticulously by version to ensure reproducibility. Many mature DevOps processes and tools, honed over years of successful software product releases, make these processes more manageable, but they were developed for traditional software products. The equivalent tools and processes simply do not exist in the ML/AI ecosystem; when they do, they are rarely mature enough to use at scale. As a result, AI PMs must take a high-touch, customized approach to guiding AI products through production, deployment, and release. Like any other production software system, after an AI product is live it must be monitored. However, for an AI product, both model performance and application performance must be monitored simultaneously. Alerts that are triggered when the AI product performs out of specification may need to be routed differently; the in-place SRE team may not be able to diagnose technical issues with the model or data pipelines without support from the AI team. Though it’s difficult to create the “perfect” project plan for monitoring, it’s important for AI PMs to ensure that project resources (especially engineering talent) aren’t immediately released when the product has been deployed.\\xa0 Unlike a traditional software product, it’s hard to define when an AI product has been deployed successfully.\\xa0 The development process is iterative, and it’s not over after the product has been deployed–though, post-deployment, the stakes are higher, and your options for dealing with issues are more limited.\\xa0 Therefore, members of the development team must remain on the maintenance team to ensure that there is proper instrumentation for logging and monitoring the product’s health, and to ensure that there are resources available to deal with the inevitable problems that show up after deployment. (We call this “debugging” to distinguish it from the evaluation and testing that takes place during product development. The final article in this series will be devoted to debugging.) Among operations engineers, the idea of observability is gradually replacing monitoring.\\xa0 Monitoring requires you to predict the metrics you need to watch in advance.\\xa0 That ability is certainly important for AI products–we’ve talked all along about the importance of metrics.\\xa0 Observability is critically different.\\xa0 Observability is the ability to get the information you need to understand why the system behaved the way it does; it’s less about measuring known quantities, and more about the ability to diagnose “unknown unknowns.” We’ve spent a lot of time talking about planning.\\xa0 Now let’s shift gears and discuss what’s needed to build a product.\\xa0 After all, that’s the point. AI Product Interface Design The AI product manager must be a member of the design team from the start, ensuring that the product provides the desired outcomes. It’s important to account for the ways a product will be used. In the best AI products, users can’t tell how the underlying models impact their experience. They neither know or care that there is AI in the application. Take Stitch Fix, which uses a multitude of algorithmic approaches to provide customized style recommendations. When a Stitch Fix user interacts with its AI products, they interface with the prediction and recommendation engines. The information they interact with during that experience is an AI product–but they neither know, nor care, that AI is behind everything they see. If the algorithm makes a perfect prediction, but the user can’t imagine wearing the items they’re shown, the product is still a failure. In reality, ML models are far from perfect, so it is even more imperative to nail the user experience. To do so, product managers must ensure that design gets an equal seat at the table with engineering.\\xa0 Designers are more attuned to qualitative research about user behavior.\\xa0 What signals show user satisfaction?\\xa0 How do you build products that delight users? Apple’s sense of design, making things that “just work,” pioneered through the iPod, iPhone, and iPad products is the foundation of their business.\\xa0 That’s what you need, and you need that input from the beginning.\\xa0 Interface design isn’t an after-the-fact add-on. Picking the Right Scope “Creeping featurism” is a problem with any software product, but it’s a particularly dangerous problem for AI. Focus your product development effort on problems that are relevant to the business and consumer. A successful AI product measurably (and positively) impacts metrics that matter to the business. Therefore, limit the scope of an AI product to features that can create this impact.  To do so, start with a well-framed hypothesis that, upon validation through experimentation, will produce meaningful outcomes. Doing this effectively means that AI PMs must learn to translate business intuitions into product development tools and processes. For example, if the business seeks to understand more about its customer base in order to maximize lifetime value for a subscription product, an AI PM would do well to understand the tools available for customer and product-mix segmentation, recommendation engines, and time-series forecasting. Then, when it comes to developing the AI product roadmap, the AI PM can focus engineering and AI teams on the right experiments, the correct outcomes,andthe smoothest path to production. It is tempting to over-value the performance gains achieved through the use of more complex modeling techniques, leading to the dreaded “black box” problem: models for which it’s difficult (if not impossible) to understand the relationship between the input and the output. Black box models are seldom useful in business environments for several reasons. First, being able to explain how the model works is often a prerequisite for executive approval. Ethical and regulatory considerations often require a detailed understanding of the data, derived features, pipelines and scoring mechanisms involved in the AI system. Solving problems with the simplest model possible is always preferable, and not just because it leads to models that are interpretable. In addition, simpler modeling approaches are more likely to be supported by a wide variety of frameworks, data platforms, and languages, increasing interoperability and decreasing technical debt. Another scoping consideration concerns the processing engine that will power the product. Problems that are real-time (or near real-time) in nature can only be addressed by highly performant stream processing architectures. Examples of this include product recommendations in e-commerce systems or AI-enabled messaging. Stream processing requires significant engineering effort, and it’s important to account for that effort at the beginning of development. Some machine learning approaches (and many software engineering practices) are simply not appropriate for near-real time applications. If the problem at hand is more flexible and less interactive (such as offline churn probability prediction), batch processing is probably a good approach, and is typically easier to integrate with the average data stack. Prototypes and Data Product MVPs Entrepreneurial product managers are often associated with the phrase “Move Fast and Break Things.”\\xa0 AI product mangers live and die by “Experiment Fast So You Don’t Break Things Later.”\\xa0 Take any social media company that sells advertisements. The timing, quantity, and type of ads displayed to segments of a company’s user population are overwhelmingly determined by algorithms. Customers contract with the social media company for a certain fixed budget, expecting to achieve certain audience exposure thresholds that can be measured by relevant business metrics. The budget that is actually spent successfully is referred to as fulfillment, and is directly related to the revenue that each customer generates. Any change to the underlying models or data ecosystem, such as how certain demographic features are weighted, can have a dramatic impact on the social media company’s revenue. Experimenting with new models is essential–but so is yanking an underperforming model out of production. This is only one example of why rapid prototyping is important for teams building AI products. AI PMs must create an environment in which continuous experimentation and failure are allowed (even celebrated), along with supporting the processes and tools that enable experimentation and learning through failure. In a previous section, we introduced the importance of user research and interface design.\\xa0 Qualitative data collection tools (such as SurveyMonkey, Qualtrics, and Google Forms) should be joined with interface prototyping tools (such as Invision and Balsamiq), and with data prototyping tools (such as Jupyter Notebooks) to form an ecosystem for product development and testing. Once such an environment exists, it’s important for the product manager to codify what constitutes a “minimum viable” AI product (MVP). This product should be robust enough to be used for user research and quantitative (model evaluation) experimentation, but simple enough that it can be quickly discarded or adjusted in favor of new iterations. And, while the word “minimum” is important, don’t forget “viable.” An MVP needs to be a product that can stand on its own, something that customers will want and use. If the product isn’t “viable” (i.e., if a user wouldn’t want it) you won’t be able to conduct good user research. Again, it’s important to listen to data scientists, data engineers, software developers, and design team members when deciding on the MVP. Data Quality and Standardization In most organizations, Data Quality is either an engineering or IT problem; it is rarely addressed by the product team until it blocks a downstream process or project. This relationship is impossible for teams developing AI products. “Garbage in, garbage out” holds true for AI, so good AI PMs must concern themselves with data health. There are many excellent resources on data quality and data governance. The specifics are outside the scope of this article, but here are some core principles that should be incorporated into any product manager’s toolkit: Augmenting AI Product Management with Technical Leadership There is no intuitive way to predict what will work best in AI product development. AI PMs can build amazing things, but this often comes largely from the right frameworks rather than the correct tactical actions. Many new tech capabilities have the potential to enable software engineering using ML/AI techniques more quickly and accurately. AI PMs will need to leverage new and emerging AI techniques (image upscaling, synthetic text generation using adversarial networks, reinforcement learning, and more), and partner with expert technologists to put these tools to use. It’s unlikely that every AI PM will have world-class technical intuition in addition to excellent product sense, UI/X experience, customer knowledge, leadership skills, and so on. But don’t let that breed pessimism. Since one person can’t be an expert at everything, AI PMs need to form a partnership with a technology leader (e.g., a Technical Lead or Lead Scientist) who knows the state of the art and is familiar with current research, and trust that tech leader’s educated intuition. Finding this critical technical partner can be difficult, especially in today’s competitive talent market.\\xa0 However, all is not lost:\\xa0 there are many excellent technical product leaders out there masquerading as competent engineering managers. Product manager Matt Brandwein suggests observing what potential tech leads do in their idle time, and taking note of which domains they find attractive. Someone’s current role often doesn’t reveal where their interests and talent lie. Most importantly, the AI PM should look for a tech lead who can mitigate their own weaknesses. For example, if the AI PM is a visionary, picking a technical lead with operational experience is a good idea. When a product is ready to ship, the PM will work with user research and engineering teams to develop a release plan that collects both qualitative and quantitative user feedback. The bulk of this data will be concentrated on user interaction with the user interface and front end of the product. AI PMs must also plan to collect data about the “hidden” functionality of the AI product, the part no user ever sees directly: model performance. We’ve discussed the need for proper instrumentation at both the model and business levels to gauge the product’s effectiveness; this is where all of that planning and hard work pays off! On the model side, performance metrics that were validated during development (predictive power, model fit, precision) must be constantly re-evaluated as the model is exposed to more and more unseen data. A/B testing, which is frequently used in web-based software development, is useful for evaluating model performance in production. Most companies already have a framework for A/B testing in their release process, but some may need to invest in testing infrastructure. Such investments are well worth it. It’s inevitable that the model will require adjustments over time, so AI PMs must ensure that whoever is responsible for the product post-launch has access to the development team in order to investigate and resolve issues. Here, A/B testing has another benefit: the ability to run champion/challenger model evaluations. This framework allows for a deployed model to run uninterrupted, while a second model is evaluated against a sample of the total population. If the second model outperforms the original, it can simply be swapped out-often without any downtime! Overall, AI PMs should remain closely involved in the early release lifecycle for AI products, taking responsibility for coordinating and managing A/B tests and user data collection, and resolving issues with the product’s functionality.  In this article, we’ve focused primarily on the AI product development process, and mapping the AI product manager’s responsibilities to each stage of that process. As with many other digital product development cycles, AI PMs must first ensure that the problem to be solved is both a problem that ML/AI can solve and a problem that is vital to the business. Once this criteria has been met, the AI PM must consider whether the product should be developed, considering the myriad of technical and ethical considerations at play when developing and releasing a production AI system. We propose the AI Product Development Process as a blueprint for AI PMs of all industries, who may develop myriad different AI products. Though this process is by no means exhaustive, it emphasizes the kind of critical thinking and cross-departmental collaboration necessary to success at each stage of the AI product lifecycle. However, regardless of the process you use, experimentation is the key to success.\\xa0 We’ve said that repeatedly, and we aren’t tired: the more experiments you can do, the more likely you are to build a product that works (i.e., positively impacts metrics the company cares about).\\xa0 And don’t forget qualitative metrics that help you understand user behavior! Once an AI system is released and in use, however, the AI PM has a somewhat unique role in product maintenance. Unlike PMs for many other software products, AI PMs must ensure that robust testing frameworks are built and utilized not only during the development process, but also in post-production. Our next article focuses on perhaps the most important phase of the AI product lifecycle: maintenance and debugging.', 'A recent article in The Verge discussed PULSE, an algorithm for “upsampling” digital images. PULSE, when applied to a low-resolution image of Barack Obama, recreated a White man’s face; applied to Alexandria Ocasio-Cortez, it built a White woman’s face.\\xa0 It had similar problems with other images of Black and Hispanic people, frequently giving them White skin and facial features.  PULSE could be used for applications like upsampling video for 8K ultra high-definition, but I’m less interested in the algorithm and its applications than in the discussion about ethics that it provoked. Is this just a problem with training data, as Yann LeCun said on Twitter?\\xa0 Or is it a sign of larger systemic issues about bias and power, as Timnit Gebru argued? The claim that this is only a problem with the data is tempting, but it is important to step back and see the bigger issues: nothing is “just” a problem with data.\\xa0 That shift to a wider perspective is badly needed. There’s no question that the training data was a problem.\\xa0 If the algorithm were trained using a set of photos dominated by Black people, it would no doubt turn White faces into Black ones. With the right training set and training process, we could presumably minimize errors. When looked at this way, it’s largely a problem of mathematics and statistics. That’s the position that Timnit Gebru rejects, because it obscures the bigger issues hiding behind the training set. As organizations like Data For Black Lives, Black in AI, the Algorithmic Justice League, and others have been pointing out, it’s never just an issue of statistics. It’s an issue of harms and of power. Who stands to gain? Who stands to lose? That’s the point we really need to consider, particularly when we’re asking AI to create “information” where nothing existed before. Who controls the erasure, or the creation, of color? What are the assumptions that lie behind it? I do not believe there are many AI researchers giggling about turning Black people into Whites (though there are no doubt some). Nor do I believe there’s some kind of racist demon lurking in the mathematics implemented by neural networks. But errors like this nevertheless happen; they happen all too frequently; the results are often harmful; and none of us are surprised that the transition was Black->White rather than the other way around. We were not surprised when we found that products like COMPAS recommended tougher criminal sentences for Black people than for Whites; nor were we surprised when Timnit Gebru and Joy Buolamwini showed that facial recognition is much less accurate for Black people than White people, and particularly inaccurate for Black women. So, how do we think about the problem of power and race in AI? Timnit Gebru is right; saying that the problem is in the training data ignores the real problem.\\xa0 As does being saddened and making vague promises about doing better in the future.\\xa0 If we aren’t surprised, why?\\xa0 What do we have to learn, and how do we put that learning into practice? We can start by considering what “biased training data” means. One of my favorite collections of essays about data is “Raw Data” is an Oxymoron. There is no such thing as “raw data,” and hence, no pure, unadulterated, unbiased data. Data is always historical and, as such, is the repository of historical bias. Data doesn’t just grow, like trees; data is collected, and the process of data collection often has its own agenda. Therefore, there are different ways of understanding data, different ways of telling stories about data–some of which account for its origin and relation to history, and some of which don’t. Take, for example, housing data. That data will show that, in most places in the US, Black people live in separate neighborhoods from White people. But there are a number of stories we can tell about that data.\\xa0 Here are two very different stories: There are many variations on those stories, but those two are enough. Neither is entirely wrong—though the first story erases an important fact, that White people have generally had the power to prevent Black people from moving into their neighborhoods. The second story doesn’t treat that data as an intransigent given; it critiques the data, asks that data how and why it came to be.\\xa0 As I’ve argued, AI is capable of revealing our biases, and showing us where they are hidden.\\xa0 It gives us an opportunity to learn about and critique our own institutions.\\xa0 If you don’t look critically at the data, its origins, and its stories (something that’s not a part of most computer science curricula), you’re likely to institutionalize the bias embedded in the data behind a wall of mathwashing. There are plenty of situations in which that critique is needed.\\xa0 Here’s one: researchers looking at ride data from Chicago’s public data portal found that the dynamic pricing algorithms used by ride-hailing services (such as Uber and Lyft) charged more for rides to and from low-income, nonwhite areas. This effect might not have been discovered without machine learning.\\xa0 It means that it’s time to audit the services themselves, and find out exactly why their algorithms behave this way. And it’s an opportunity to learn what stories the data is telling us. The real issue is which of those stories we choose to tell. I use the word “we” because the data doesn’t tell a story on its own, any more than a pixelated image of President Obama becomes a White man on its own. Someone chooses what story to tell; someone releases the software; and that someone is a person, not an algorithm. So if we really want to get to the bottom of the upsampling problem with PULSE, we need to be looking at people in addition to training data. If PULSE needed more images of Black people in its training set, why didn’t it have them? And why are we not surprised that these issues show up all the time, in applications ranging from COMPAS to the Google app that tagged Black people as gorillas? That’s really a question about the teams of people who are creating and testing this software. They are predominantly White and male. I admit that if I wrote a program that upsampled images, it might not occur to me to test a Black person’s face. Or to test whether jail sentences for White and Black people are comparable. Or to test whether a real estate application will recommend that Black people consider buying homes in largely White neighborhoods. These not-so-microaggressions are the substance from which greater abuses of power are made. And we’re more likely to discover those microaggressions in time to stop them if the teams developing the software include people with Black and Brown faces, as well as White ones. The problem isn’t limited to building teams that realize we need different training data, or that understand the need for testing against different kinds of bias.\\xa0 We also need teams that can think about what applications should and shouldn’t be built. Machine learning is complicit in many power structures. Andrew Ng’s newsletter, The Batch, gives payday lending as an example. An application might compute the optimal interest rate to charge any customer, and that app might easily be “fair” by some mathematical standard–although even that is problematic. But the industry itself exists to take advantage of vulnerable, low-income people. In this situation, it is impossible for an algorithm—even a “fair” one—to be fair. Likewise, given the current power structures, along with the possibility for abuse, it is very difficult to imagine a face recognition application, no matter how accurate, that isn’t subject to abuse. Fairness isn’t a mathematical construct that can be embodied by an algorithm; it has everything to do with the systems in which the algorithm is embedded. The algorithms used to identify faces can also be used to identify bird species, detect diseased tomatoes on a farm, and the like. The ethical problem isn’t the algorithm, it’s the context and the power structures, and those are issues that most software teams aren’t used to thinking about. There’s an even better example close at hand: PULSE itself.\\xa0 Obscuring faces by replacing them with low-resolution, pixelated images is a classic way of protecting the identity of the person in the photograph.\\xa0 It’s something people do to protect themselves–a particularly important issue in these days of incognito armies. Software like PULSE, even (especially) if it is trained correctly, undoes individuals’ efforts to protect their own privacy.\\xa0 It tips the power relationship even further in the direction of the empowered. And an application like Stanford’s #BlackLivesMatter PrivacyBot tips the balance back the other way. There are many ways to address these issues of bias, fairness, and power, but they all start with building inclusive teams, and with taking a step back to look at the bigger issues involved. You are more likely to detect bias if there are people on the team who have been victims of bias. You are more likely to think about the abuse of power if the team includes people who have been abused by power. And, as I’ve argued elsewhere, the job of “programming” is becoming less about writing code, and more about understanding the nature of the problem to be solved. In the future, machines will write a lot of code for us. Our task will be deciding what that software should do—not putting our heads down and grinding out lines of code. And that task isn’t going to go well if our teams are monochromatic.', '‘Tech’, of course, has all of this complexity, but we’re having to work this out a lot more quickly. It took 75 years for seatbelts to become compulsory, but tech has gone from interesting to crucial only in the last five to ten years. That speed means we have to form opinions about things we didn’t grow up with and don’t always understand quite so well as, say, supermarkets. uses extreme ultraviolet (EUV) light at a wavelength of 13.5 nm to make silicon features down to a few nanometers in size. While quantum computing has garnered most of the recent headlines, quantum networking—especially with its promise of secure communication—actually is capturing the interest of a growing community across science, industry, and national security. Today, many people recognize that building and scaling quantum-protected and enhanced communication networks are among the most important technological frontiers of the 21st century. Although a general-purpose quantum computer still is many years away, the research community perceives a quantum Internet may be closer to realization.', 'Taiwan and Audrey Tang occupy a unique spot in a world, where the ascendance of the internet and digital technology is marked by the twin dystopias of “post-truth” information chaos in the United States and China’s totalitarian, technologically mediated surveillance-and-censorship regime. With Audrey Tang as the symbolic figurehead, the island nation is making the radical argument that digital tools can be effectively used to build stronger, more open, more accountable democracies. Whether the challenge is fighting disinformation campaigns orchestrated by hostile powers or the existential threat of a virus run amok or simply figuring out how to regulate Uber, Taiwan is demonstrating the best ways technology can be used to marry the energy and talents of civil society with the administrative powers of government bureaucracy. we found that the employees who averaged the most weekly one-on-one time with their managers experienced the smallest increase in working hours  if you have a fleet of healthy services written in a single application stack, then it’s a good idea to think twice before introducing a service mesh. By simply introducing or evolving a shared RPC library, you’ll get the exact same benefits and avoid dealing with the downsides of maintaining service meshes.', 'why aren’t we — ostensibly the people writing software — doing more with AI in our day-to-day? Why are things like TabNine and Kite so often seen as curiosities instead of game-changers? If you take seriously the idea that ai will fundamentally change the nature of many occupations in the coming decade, what reason do you have to believe that you’ll be immune from that because you work in software? Looking at the code you’ve been paid to write over the past few years, how much of that can you honestly say is truly novel? The truth is, I’ve come around to thinking that programming isn’t the most important thing for programmers to pay attention to right now. represent our Domain Model declaratively, as an in-program data structure (a ‘meta-database’); derive the ‘machine’ behaviour generically from this representation. We have developed a new methodology that retains the ease-of-use, familiarity, and (some of) the free-form nature of informal methods, while benefiting from the rigor, structure, and potential for automation characteristic of formal methods. Our approach aims to foster thoughtful and timely analysis through the introduction of structure, and collaboration through access to the corporate memory of current and past analytic results. This article covers the outcomes of research performed in 2019 on how engineers at Google debug production issues, including the types of tools, high-level strategies, and low-level tasks that engineers use in varying combinations to debug effectively. It examines the research approach used to capture data, summarizing the common engineering journeys for production investigations and sharing examples of how experts debug complex distributed systems. Finally, the article extends the Google specifics of this research to provide some practical strategies that you can apply in your organization.', 'Our diff solution involves embedding each line into a low dimensional vector and (optionally “fine-tuning” or updating the embedding model at the same time), assigning it to a cluster, and identifying lines in different clusters as “different”. Locality sensitive hashing is a probabilistic algorithm that permits constant time cluster assignment and near-constant time nearest neighbors search. Good leaders can walk into a situation where people have lost track of their goals and get everyone aligned on a clear path forward. They remove unimportant details, distill complex situations to their essence, and get the right decision-maker to make a call – even if it’s not them. They’re able to not only stop bad plans before it’s too late, but get them moving again in the right direction.', 'Apple was responsible for more edits in 2019 than Mapbox accounted for in its entire corporate history. See also the 2020: Curious Cases of Corporations in OpenStreetMap talk from State of the Map. (via Simon Willison)     Azerbaijan, frustrated at a peace process that it felt delivered nothing, used its Caspian Sea oil wealth to buy arms, including a fleet of Turkish Bayraktar TB2 drones and Israeli kamikaze drones (also called loitering munitions, designed to hover in an area before diving on a target). […] Azerbaijan used surveillance drones to spot targets and sent armed drones or kamikaze drones to destroy them, analysts said. […] Their tally, which logs confirmed losses with photographs or videos, listed Armenian losses at 185 T-72 tanks; 90 armored fighting vehicles; 182 artillery pieces; 73 multiple rocket launchers; 26 surface-to-air missile systems, including a Tor system and five S-300s; 14 radars or jammers; one SU-25 war plane; four drones and 451 military vehicles. an efficient, single-machine system for performing data mining tasks on large graphs. Some graph mining applications include: Finding frequent subgraphs; Generating the motif/graphlet distribution; Finding all occurrences of a subgraph. Peregrine is highly programmable, so you can easily develop your own graph mining applications using its novel, declarative, graph-pattern-centric API. To write a Peregrine program, you describe which graph patterns you are interested in mining, and what to do with each occurrence of those patterns. You provide the what and the runtime handles the how. I found that the marginal returns of researchers are rapidly declining. There is what’s called a “standing on toes” effect: researcher productivity declines as the field grows. Because ML has recently grown very quickly, this makes better ML models much harder to find.  ', 'Terminal/CLI Epub reader Simply put, ur-technical debt arises when my ideas diverge from my code. That divergence is inevitable with an iterative process. […] “[I]f you develop a program for a long period of time by only adding features and never reorganizing it to reflect your understanding of those features, then eventually that program simply does not contain any understanding and all efforts to work on it take longer and longer.” a real-time [REST and GraphQL] API and App dashboard for managing SQL database content.', 'If software is such stuff as dreams are made on, how do we talk about nightmares? Software is not the tangible, kickable stuff our senses are tuned to, so we draw on metaphor to communicate and reason about it. The 1970s offered up spaghetti code to describe the tangle of unstructured control flow. This has inspired many software-as-pasta descriptions, from lasagne for layered architectures to ravioli for—pick a decade—objects, components, modules, services, and microservices. Beyond its disordered arrangement, however, spaghetti has little to offer us as a metaphor. It doesn’t provide us with a useful mental model for talking about code, and has far too many positive associations. If you love both ravioli and spaghetti, it’s not obvious that one of these is worse for your software architecture than the other. A metaphor is a mapping that we use to describe one thing in terms of another—sometimes because we want to show something familiar from an unfamiliar angle, as in poetry, but sometimes because we want to show something unfamiliar or abstract in a more familiar light, as in software. To be considered good, a metaphor has to offer a number of points of useful correspondence with what is being described. Pasta doesn’t quite do this. Another quality of a good metaphor is that it should not have too many obvious points of conflict. It will never map its target perfectly—a metaphor is a conceit not an identity—but a good metaphor is one whose key qualities don’t contradict the very thing we are trying to say, whose points of difference don’t distract from the mental model being shared. We sometimes talk about code decay and software rot. These terms give a sense of degradation over time. This seems accurate and relatable. They also suggest a response: cleaning (we brush our teeth to reduce the chance of tooth decay) or treatment (we treat wood to avoid it rotting). So far so good… but the problem with these metaphors is they refer to natural processes that happen independently of anything we do. If you don’t brush your teeth, you will experience decay. If you don’t touch code, it doesn’t intrinsically degrade. The third quality of a metaphor that makes it effective is familiarity to its audience. Explaining something unfamiliar in terms of something else that is also unfamiliar can be a long road to travel a short distance (or to end up where you started). If you are familiar with the concept of entropy in statistical mechanics, with the second law of thermodynamics, and with the idea that work is needed to reduce entropy and increase order in a system, then software entropy might strike you as a descriptive metaphor—and not simply because the word work transfers happily from the world of thermodynamics to the day-to-day experience of developers. If, however, these concepts are not accessible and require explanation, then, regardless of its other merits, software entropy may not be the best way to talk about accidental complexity in code. Perhaps the most popular metaphor in use is based on financial debt, originating with Ward Cunningham in 1992. As Martin Fowler described in 2003: Technical Debt is a wonderful metaphor developed by Ward Cunningham to help us think about this problem. In this metaphor, doing things the quick and dirty way sets us up with a technical debt, which is similar to a financial debt. Like a financial debt, the technical debt incurs interest payments, which come in the form of the extra effort that we have to do in future development because of the quick and dirty design choice. When we look at technical debt, we see a metaphor that checks all three boxes: it has a number of useful points of correspondence; the points of difference don’t overwhelm the core idea; it is familiar. Furthermore, it brings with it a useful working vocabulary. For example, consider what the following debt-related words suggest to you in a software context: repayment, consolidation, creditworthiness, write-off, borrowing. Although we know that by definition no metaphor is perfect, there are two common ways in which the metaphor is misapplied: assuming technical debt is necessarily something bad; equating technical debt with a financial debt value. The emphasis of the former is misaligned and the latter is a category error. If we are relying on the common experience of our audience, financial debt is almost always thought of as a burden. If we take that together with the common experience of code quality and nudge it with leading descriptions such as “quick and dirty,” it is easy to see how in everyday use technical debt has become synonymous with poor code and poor practice. We are, however, drawing too heavily on the wrong connotation. Rather than reckless debt, such as from gambling, we should be thinking more along the lines of prudent debt, such as a mortgage. A mortgage should be offered based on our credit history and our ability to pay and, in return, we are able to buy a house that might otherwise have been beyond our reach. Similarly, Ward’s original motivation was to highlight how debt in code can be used for competitive advantage: Shipping first time code is like going into debt. A little debt speeds development so long as it is paid back promptly with a rewrite. This comes with a clear caveat and implication: a debt is a loan. A debt is for repayment, not for running up: The danger occurs when the debt is not repaid. Every minute spent on not-quite-right code counts as interest on that debt. Entire engineering organizations can be brought to a stand-still under the debt load of an unconsolidated implementation. As in the real world, how we run up debt and how we manage it turn out to be more complex than the simplicity of our best intentions. There are teams that make time-saving decisions wisely, revisiting and addressing them later in a timely manner. But in most cases where debt is incurred, discussed, and lamented, codebases reflect the firefight of different priorities, skills, and people. It’s still technical debt, but it lacks the prudence and intention of Ward’s original purpose. There are also teams and tools that embrace the debt metaphor so tightly that they forget it’s a metaphor. They treat it literally and numerically, converting code quality into a currency value on a spreadsheet or dashboard. The consequences of this thinko range from being a harmless fiction largely ignored by developers and managers to a more damaging numerology that, even though it’s well intentioned, can mislead development effort. If we’re going to quantify it, what is it we’re quantifying? Do we list off code smells? What is the debt value of a code smell? Is it constant per kind of code smell? For example, is duplicate code characterised by a single cost? And are code smells independent of one another? Consider that, for example, duplication is sometimes used to reduce coupling, so the debit becomes a credit in that context. We can conclude that a code smell is not an isolated thing with a single look-up debt value, so this is clearly a more complex problem dependent on many factors. As a multivariable problem, what does it depend on? And how? And how do we know? And what would the value or—more likely—value distribution reflect? The cost of fixing? Or, more honestly, an estimate of the cost of fixing? But even if we are somehow able to conjure a number out of this ever-growing list of considerations—and even if that number has some relation to observed reality—we have put a number to the wrong quantity. We have, in fact, missed the whole point of the metaphor. Technical debt is not the cost of repaying the debt: it is the cost of owning the debt. These are not the same. That is the message of the technical debt metaphor: it is not simply a measure of the specific work needed to repay the debt; it is the additional time and effort added to all past, present, and future work that comes from having the debt in the first place. By taking the metaphor literally, we have robbed it of its value. Its value is to offer us a figure of speech not of currency, a mental model for talking and reasoning about qualities of our code that are not simply stated in code. No matter how well meant, pushing any metaphor beyond its applicability leads to metaphor shear. It is, after all, metaphor and not identity.', 'Network and Distributed System Security Symposium We present Lifty, a domain-specific language for data-centric applications that manipulate sensitive data. A Lifty programmer annotates the sources of sensitive data with declarative security policies, and the language statically and automatically verifies that the application handles the data according to the policies. Moreover, if verification fails, Lifty suggests a provably correct repair, thereby easing the programmer burden of implementing policy enforcing code throughout the application. Create a schema by using visual blocks system. GraphQL Editor will transform them into code.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7JuwNTW4JIn"
      },
      "source": [
        "### Sentence tokenize each document in the list of documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDAJFU15glfC",
        "outputId": "e550b8c0-474d-4bcc-8ddf-6e639cf602f8"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zbqurmN4JIp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3056b93-1198-4d71-85c0-6e695c28a5ff"
      },
      "source": [
        "#sentence tokenize for all docs\n",
        "sents_tk = [sent_tokenize(doc) for doc in docs]\n",
        "print(sents_tk)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['2020 has been a year of great challenges for so many, but it’s not all negative.', 'Around the world, organizations and their workforces have risen to the occasion, recognizing the importance of expanding their knowledge, taking on new tasks, and bettering themselves both personally and professionally.', 'With the uptick in virtual conferencing, remote work, and, for some, reentering the job market, new technology adoption was accelerated, driving the workforce to build new skills.', 'While 2020 was the year of the global COVID-19 pandemic, it will also be commemorated as the year online learning prevailed.', 'As vaccine development persists and life gets back to normal, with it will come a more future-proof workforce ready to share their new knowledge with the world.', 'Since the onset of the pandemic, online courses and programs have seen dramatic spikes in consumption and enrollment, and O’Reilly has been no different.', 'A big contributor to O’Reilly’s continued success during these unprecedented times has been its live virtual training courses.', 'This year, more than 900,000 users have registered for live events through O’Reilly online learning—a 96% increase from last year.', 'This functionality also allowed O’Reilly to introduce its Superstream Series, a new lineup of virtual conferences featuring expert speakers delivering talks and training sessions on the most important topics and emerging trends in technology.', 'So what are the trends driving this uptick in learning?', 'Companies are increasingly interested in understanding how to successfully adjust to remote work and effectively manage time.', 'And individual O’Reilly members are looking to build and expand on their technical skills in everything from software architecture and microservices to AI and programming languages.', 'But which topics are the brightest minds in technology most focused on?', 'We’ve compiled the top 20 live online training courses of 2020 to shed some light on what those in the know want to know.', 'Top 20 live online training courses of 2020 For a more in-depth analysis of the hot technology topics of 2020, based on data from O’Reilly online learning, stay tuned for our upcoming report, Wrapping Up 2020 (and What to Expect for 2021): Trends on O’Reilly online learning.'], ['It has long seemed to me that functional programming is, essentially, programming viewed as mathematics.', 'Many ideas in functional programming came from Alonzo Church’s Lambda Calculus, which significantly predates anything that looks remotely like a modern computer.', 'Though the actual history of computing runs differently: in the early days of computing, Von Neumann’s ideas were more important than Church’s, and had a tremendous influence on the design of early computers—an influence that continues to the present.', 'Von Neumann’s thinking was essentially imperative: a program is a list of commands that run on a machine designed to execute those commands.', 'So, what does it mean to say that functional programming is programming “viewed as mathematics”?', 'Von Neumann was a “mathematician,” and programming of all kinds found its first home in Mathematics departments.', 'So, if functional programming is mathematical, what does that mean?', 'What kind of math?', 'I’m not thinking of any specific branch of mathematics.', 'Yes, the Lambda Calculus has significant ties to set theory, logic, category theory, and many other branches of mathematics.', 'But let’s start with grade school mathematics and assignment statements; they’re basic to any programming language.', 'We’re all familiar with code like this: Mathematically, this is nonsense.', 'An equation is a statement about a relationship that holds true.', 'i can equal i; it can’t equal i+1.', 'And while i++ and i+=1 no longer look like equations, they are equally nonsensical; once you’ve said that i equals something, you can’t say it equals something else.', '“Variables” don’t change values; they’re immutable.', 'Immutability is one of the most important principles of functional programming.', 'Once you’ve defined a variable, you can’t change it.', '(You can create a new one in a different function scope, but that’s a different matter.)', 'Variables, in functional programming, are invariant; and that’s important.', 'You may be wondering “what about loops?', 'How can I write a for loop?” Not only do you have to do without index variables, you can’t modify any of the variables in the loop body.', 'Setting aside the (solvable) problem of iteration, there’s no reason you can’t write code in (almost) any non-functional language that has this same effect.', 'Just declare all your variables final or const.', 'In the long run, functional programming is more about a specific kind of discipline than about language features.', 'Programming languages can enforce certain rules, but in just about any modern language it’s possible to follow those rules without language support.', 'Another important principle of functional programming is that functions are “first class entities.” That is, there are minimal restrictions about where you can use a function.', 'You can also have functions without names, often called “lambdas” (which refers directly to the Lambda Calculus, in which functions were unnamed).', 'In Python, you can write code like this: The “key” is an anonymous function that returns a specific column of an array; that function is then used for sorting.', 'Personally, I’m not overly fond of “anonymous functions”; it’s often clearer to write the anonymous function as a regular, named function.', 'So I might write this: The ability to use functions as arguments to functions gives you a very nice way to implement the “strategy pattern”: I often get the sense that all programmers really want from functional programming is first-class functions and lambdas.', 'Lambdas were added to Python very early on (1.0) but didn’t reach Java until Java 8.', 'Another consequence of thinking mathematically (and possibly a more important one) is that functions can’t have side-effects and, given the same arguments, will always return the same value.', 'If a mathematician (or a high school trig student) writes they don’t have to deal with the possibility that sin(x) sets some global variable to 42, or will return a different value every time it’s called.', 'That just can’t happen; in math, the idea of a “side-effect” is meaningless.', 'All the information that sin(x) provides is encapsulated in the return value.', 'In most programming languages, side-effects happen all too easily, and in some, they’re almost an obsession.', 'Again, creating functions that have no side-effects is a matter of exercising discipline.', 'A programming language can enforce this rule, but you can follow it whether or not your language makes you do it.', 'We don’t have cartoon devils looking over our shoulders saying “Go ahead; make a side effect.', 'No one will notice.” Functional languages vary the degree to which they enforce the lack of side-effects.', 'If you’re a purist, anything that interacts with the real world is a side-effect.', 'Printing a document?', 'Changing a row in a database?', 'Displaying a value on the user’s screen?', 'Those are all side-effects (they aren’t completely encapsulated in the value returned by the function), and they have to be “hidden” using a mechanism like monads in Haskell.', 'And that’s the point at which many programmers get confused and throw up their hands in despair.', '(I’ll only point you to Real World Haskell.)', 'In both Java and Python, lambda functions can have side-effects, which means that, strictly speaking, they aren’t really “functional.” Guido van Rossum’s discussion of the addition of Lambdas to Python is worth reading; among other things, he\\xa0says\\xa0“I have never considered Python to be heavily influenced by functional languages, no matter what people say or think.” Streams are often associated with functional languages; they’re essentially long (perhaps infinite) lists that are evaluated lazily—meaning that elements of the string are only evaluated as they’re needed.', 'Maps apply a function to every element of a list, returning a new list—and that includes streams, which (for these purposes) are specialized lists.', 'That’s an incredibly useful feature; it’s a great way to write a loop without having to write a loop—and without even knowing how much data you have.', 'You can also create “filters” that choose whether to pass any element of the stream to the output, and you can chain maps and filters together.', 'If you think this sounds like a Unix pipeline, you’re right.', 'Streams, maps, filters, and the act of chaining them together really have as much to do with the Unix shell as they do with functional languages.', 'Another way to avoid writing loops is to use “comprehensions,” a feature of Python.', 'It’s easy to get very fond of list comprehensions; they’re compact, they eliminate off-by-one errors, and they’re very flexible.', 'Although comprehensions look like a compact notation for a traditional loop, they really come from set theory—and their closest computational “relatives” are to be found in relational databases, rather than functional programming.', 'Here’s a comprehension that applies a function to every element of a list: The most general way to avoid traditional loops is to use recursion: a function that calls itself.', 'Here’s the recursive equivalent to the previous comprehension: Recursion is a mainstay of functional languages: you don’t have indices being modified, and you’re not even modifying the resulting list (assuming that append doesn’t count as modification).', 'However, recursion has its own problems.', 'It’s hard to wrap your mind around recursion; you still need to do a lot of your own bookkeeping (in this case, passing in a vector so a result can be returned); and except in one (common) special case, called “tail recursion,” it can be a performance nightmare.', 'I started by saying that functional programming was programming considered as “math,” and that’s at least partially correct.', 'But is that claim useful?', 'There are many branches of mathematics that map onto programming concepts in different ways.', 'Functional programming only represents one of them.', 'If you’re a topologist, you may well like graph databases.', 'But discussing which branch of mathematics corresponds to which programming practices isn’t really helpful.', 'Remembering high school algebra may help when thinking about immutability, statelessness, and the absence of side-effects; but most programmers will never study the real mathematical origins of functional programing.', 'Lambdas are great; functions as arguments in method calls is great; even recursion is (sometimes) great; but we’re fooling ourselves if we think programmers are going to start using Java as if it were Haskell.', 'But that’s OK; for Java programmers, the value of Lambdas isn’t some mathematical notion of “functional,” but in providing a huge improvement over anonymous inner classes.', 'The tools to be functional are there, should you choose to use them.', 'In college, I learned that engineering was about making tradeoffs.', 'Since then, I’ve heard very few programmers talk about tradeoffs—but those tradeoffs are still central to good engineering.', 'And while engineering uses a lot of mathematics, engineering isn’t mathematics, in part because mathematics doesn’t deal in tradeoffs.', 'Using “mathematics” as a way to think about a particular style of disciplined coding maybe be useful, particularly if that discipline leads to fewer bugs.', 'It’s also useful to use the tools of mathematics to make good tradeoffs between rigor, performance, and practicality—which may lead you in an entirely different direction.', 'Be as functional as you need to (but no more).'], ['1.', 'Basic Processor & Memory hierarchy; 2.', 'Advanced Out-of-Order Processor; 3.', 'Data-parallel processors; 4.', 'Micro-controller introduction; 5.', 'Multicore;  6.', 'RISC-V core; 7.', 'Advanced Multicore; 8.', 'Multicore programming; 9.', 'Graphics Processing Unit (GPU); 10.', 'Heterogeneous SoC; 11.', 'GPU Programming; 12.', 'Application-Specific Instruction-Set Processor (ASIP); 13 PULP: Parallel Ultra-Low-Power Computing; 14.', 'Architecture in the Future – Wrap-up Next-generation reliable, safe, concise, and functional-first programming language.', 'Flix is a principled and flexible functional-, logic-, and imperative- programming language that takes inspiration from F#, Go, OCaml, Haskell, Rust, and Scala.', 'Flix looks like Scala, but its type system is closer to that of OCaml and Haskell.', 'Its concurrency model is inspired by Go-style processes and channels.', 'Flix compiles to JVM bytecode, runs on the Java Virtual Machine, and supports full tail call elimination.', 'supports first-class Datalog constraints enriched with lattice semantics.', 'Abundance, connectivity, healthspan, capital, AR and Spatial Web, smart devices, human-level AI, AI-Human collaboration, software shells, renewable energy, insurance industry switches to prevention, autonomous vehicles and flying cars, on-demand production and delivery, knowledge, advertising, cellular agriculture, brain-computer interfaces, VR, sustainability/environment, and CRISPR.', 'Level -2: No Authentication; Level -1: All Passwords = “password”; Level 0: Hardcode Everywhere; Level +1: Move Secrets into a Config File; Level +2: Encrypt the Config File; Level +3: Use a Secret Manager; Level +4: Dynamic Ephemeral Credentials'], ['The programming world used to be split into functional languages, object-oriented languages, and everything else (mostly procedural languages).', 'One “was” a functional programmer (at least as a hobby) writing Lisp, Haskell, or Erlang; or one “was” an OO programmer (at least professionally), writing code in Java or C++.', '(One never called oneself a “procedural programmer”; when these names escaped from academia in the 1990s, calling yourself a “procedural programmer” would be akin to wearing wide ties and bell-bottom jeans.)', 'But this world has been changing.', 'Over the past two decades, we’ve seen the rise of hybrid programming languages that combine both functional and object-oriented features.', 'Some of these languages (like Scala) were multi-paradigm from the beginning.', 'Others, like Python (in the transition from Python 2 to 3) or Java (with the introduction of Lambdas in Java 8) are object-oriented or procedural languages to which functional features were added.', 'Although we think of C++ as an object-oriented language, it has also been multi-paradigm from the beginning.', 'It started with C, a procedural language, and added object-oriented features.', 'Later, beginning with the Standard Template Library, C++ was influenced by many ideas from Scheme, a descendant of LISP.', 'JavaScript was also heavily influenced by Scheme, and popularized the idea of anonymous functions and functions as first class objects.', 'And JavaScript was object-oriented from the start, with a prototype-based object model and syntax (though not semantics) that gradually evolved to become similar to Java’s.', 'We’ve also seen the rise of languages combining static and dynamic typing (TypeScript in the JavaScript world; the addition of optional type hinting in Python 3.5; Rust has some limited dynamic typing features).', 'Typing is another dimension in paradigm space.', 'Dynamic typing leads to languages that make programming fun and where it’s easy to be productive, while strict typing makes it significantly easier to build, understand, and debug large systems.', 'It’s always been easy to find people praising dynamic languages, but, except for a few years in the late 00s, the dynamic-static paradigmatic hasn’t attracted as much attention.', 'Why do we still see holy wars between advocates of functional and object-oriented programming?', 'That strikes me as a huge missed opportunity.', 'What might “multi-paradigm programming” mean?', 'What would it mean to reject purity and use whatever set of features provide the best solution in any given context?', 'Most significant software is substantial enough that it certainly has components where an object-oriented paradigm makes more sense, and components where a functional paradigm is superior.', 'For example, look at a “functional” feature like recursion.', 'There are certainly algorithms that make much more sense recursively (Towers of Hanoi, or printing a sorted binary tree in order); there are algorithms where it doesn’t make much of a difference whether you use loops or recursion (whenever tail recursion optimizations will work); and there are certainly cases where recursion will be slow and memory-hungry.', 'How many programmers know which solution is best in any situation?', 'These are the sort of questions we need to start asking.', 'Design patterns have been associated with object-oriented programming from the beginning.', 'What kinds of design patterns make sense in a multi-paradigm world?', 'Remember that design patterns aren’t “invented”; they’re observed, they’re solutions to problems that show up again and again, and that should become part of your repertoire.', 'It’s unfortunate that functional programmers tend not to talk about design patterns; when you realize that patterns are observed solutions, statements like “patterns aren’t needed in functional languages” cease to make sense.', 'Functional programmers certainly solve problems, and certainly see the same solutions show up repeatedly.', 'We shouldn’t expect those problems and solutions to be the same problems and solutions that OO programmers observe.', 'What patterns yield the best of both paradigms?', 'What patterns might help to determine which approach is most appropriate in a given situation?', 'Programming languages represent ways of thinking about problems.', 'Over the years, the paradigms have multiplied, along with the problems we’re interested in solving.', 'We now talk about event-driven programming, and many software systems are event-driven, at least on the front end.', 'Metaprogramming was popularized by JUnit, the first widely used tool to rely on this feature that’s more often associated with functional languages; since then, several drastically different versions of metaprogramming have made new things possible in Java, Ruby, and other languages.', 'We’ve never really addressed the problem of how to make these paradigms play well together; so far, languages that support multiple paradigms have left it to the programmers to figure out how to use them.', 'But simply mixing paradigms ad hoc probably isn’t the ideal way to build large systems–and we’re now building software at scales and speeds that were hard to imagine only a few years ago.', 'Our tools have improved; now we need to learn how to use them well.', 'And that will inevitably involve blending paradigms that we’ve long viewed as distinct, or even in conflict.', 'Thanks to Kevlin Henney for ideas and suggestions!'], ['Would the mental focus on a specific hypothesis prevent us from making a discovery?', 'To test this, we made up a dataset and asked students to analyze it.', '[…] The most notable “discovery” in the dataset was that if you simply plotted the number of steps versus the BMI, you would see an image of a gorilla waving at you (Fig.', '1b).', 'my issue was the fact that the systems doing the flashing were running the yocto images and perl and the guy writing the perl was also responsible for writing the thing that actually updates the car.', 'that thing (the car-side updater) is about ~100k lines of C in a single file.', 'code reviews were always a laugh riot.'], ['system security starts at the hardware layer free home server for your comics and ebooks library a relational database management system developed by the Carnegie Mellon Database Group.', 'The research goal of the NoisePage project is to develop high-performance system components that support autonomous operation and optimization as a first-class design principle.'], ['Second-tier Scottish football club Inverness Caledonian Thistle doesn’t have a camera operator for matches at their stadium so the club uses an AI-controlled camera that’s programmed to follow the ball for their broadcasts.', 'But in a recent match against Ayr United, the AI controller kept moving the camera off the ball to focus on the bald head of the linesman, making the match all but unwatchable.', 'No fans allowed in the stadium either, so the broadcast was the only way to watch.', 'Basically at midnight at the end of 1927, the clocks went back 5 minutes and 52 seconds.', 'On average, UX improvements have substantially decreased since 2006–2008: from 247% to 75% (a 69% decrease).', 'This difference is statistically significant (p = 0.01) — we can be quite confident that average improvement scores are lower now than they were 12–14 years ago.', 'This hands-on course explores a selection of techniques from Programming Languages and Human-Computer Interaction that can help us create useful, usable programming languages and programming tools.', 'We will cover strategies for designing programming systems—e.g., need finding, formative studies, user-centered design broadly.', 'We will also cover tools and techniques that help us build user-friendly programming systems—e.g., program synthesis, structure editors, abstraction design, program slicing.', 'For the final project, individuals or teams will develop a usable abstraction, language, or programming tool of their own design.'], ['Perhaps the most important event this month isn’t technical, but the start of the US Justice Dept.’s lawsuit against Google.', 'That will certainly play out over years rather than months, but it’s significance is less about this particular case than the idea that legal and regulatory systems will play a large role in the evolution of technology in the US.', 'In the short term, it’s worth watching the CPPA, GDPR, California’s Props 22 and 24, and FCC interference with social media’s enforcement of rules around community behavior.', 'Long term, this is only the beginning.'], ['in this paper, we semi-automatically learn error-inducing patterns from a corpus of common Java coding errors and from changes that caused operational anomalies at Facebook specifically.', 'We combine the mutations with instrumentation that measures which tests exactly visited the mutated piece of code.', 'Results on more than 15,000 generated mutants show that more than half of the generated mutants survive Facebook’s rigorous test suite of unit, integration, and system tests.', 'is the companion tutorial for the paper “Algorithms for Causal Reasoning in Probability trees” by Genewein T. et al.', '(2020).', 'Probability trees are one of the simplest models of causal generative processes.They possess clean semantics and are strictly more general than causal Bayesian networks, being able to e.g.', 'represent causal relations that causal Bayesian networks can’t.', 'Even so, they have received little attention from the AI and ML community.', 'In this tutorial we present new algorithms for causal reasoning in discrete probability trees that cover the entire causal hierarchy (association, intervention, and counterfactuals), operating on arbitrary logical and causal events.', 'a method in occupational safety for avoiding mistakes by pointing at important indicators and calling out the status.', 'reduce the imperative shell and move code into the functional core'], ['In this paper, we investigate “split-second phantom attacks,” a scientific gap that causes two commercial advanced driver-assistance systems (ADASs), Telsa Model X (HW 2.5 and HW 3) and Mobileye 630, to treat a depthless object that appears for a few milliseconds as a real obstacle/object.', '1.', 'Prefer running your system in the cloud over local emulation.', '2.', 'CI/CD Pipelines are not enough, local deployment automation is crucial.', '3.', 'For AWS Serverless & “Function-as-a-Service” – monolithic functions are OK. 4.', 'Consider adopting a monorepo, with tooling appropriate for monorepos.', '5.', 'Implement all three pillars of observability.', 'I reverse engineered mcdonald’s internal api and I’m currently placing an order worth $18,752 every minute at every mcdonald’s in the US to figure out which locations have a broken ice cream machine.'], ['“On peut interroger n’importe qui, dans n’importe quel état; ce sont rarement les réponses qui apportent la vérité, mais l’enchaînement des questions.““You can interrogate anyone, no matter what their state of being.', 'It’s rarely their answers that unveil the truth, but the sequence of questions that you have to ask.“–\\xa0 Inspector Pastor in La Fée Carabine, by Daniel Pennac The authors’ jobs all involve asking questions.', 'A lot of questions.', 'We do so out of genuine curiosity as well as professional necessity: Q is an ML/AI consultant, Chris is a product manager in the AI space, and Shane is an attorney.', 'While we approach our questions from different angles because of our different roles,\\xa0 we all have the same goal in mind: we want to elicit truth and get people working with us to dig deeper into an issue.', 'Preferably before things get out of hand, but sometimes precisely because they have.', 'A recent discussion led us down the path of our favorite questions: what they are, why they’re useful, and when they don’t work so well.', 'We then each chose our top three questions, which we’ve detailed in this article.', 'We hope you’re able to borrow questions you haven’t used before, and even cook up new questions that are more closely related to your personal and professional interests.', 'Before we get too far, let’s explore what we mean by a “good question.” For one, it’s broad and open-ended.', 'It’s a lot less “did this happen?” and more “what happened?”\\xa0 It encourages people to share their thoughts and go deep.', 'There’s an implied “tell me more” in an open-ended question.', 'Follow it with silence, and (as any professional interrogator will tell you) people will fill in extra details.', 'They will get to what happened, along with when and how and why.', 'They will tell a full story, which may then lead to more questions, which branch into other stories.', 'All of this fills in more pieces to the puzzle.', 'Sometimes, it sheds light on parts of the puzzle you didn’t know existed.', 'By comparison, yes/no questions implicitly demand nothing more than what was expressly asked.', 'That makes them too easy to dodge.', 'Two, a good question challenges the person asking it as much as (if not more than) the person who is expected to answer.', 'Anyone can toss out questions at random, in an attempt to fill the silence.', 'To pose useful questions requires that you first understand the present situation, know where you want to wind up, and map out stepping-stones between the two.', 'Case in point: the Daniel Pennac line that opened this piece was uttered by a detective who was “interviewing” a person in a coma.', 'As he inspected their wounds, he asked more questions to\\xa0 explore their backstory, and that helped him to piece together his next steps of the investigation.', 'Perhaps Inspector Pennac was inspired by Georg Cantor, who once said: “To ask the right question is harder than to answer it.” Three, a good question doesn’t always have a right answer.', 'Some of them don’t have any answer at all.', 'And that’s fine.', 'Sometimes the goal of asking a question is to break the ice on a topic, opening a discussion that paints a larger picture.', 'Four, sometimes a question is effective precisely because it comes from an unexpected place or person.', 'While writing this piece, one author pointed out (spoiler alert) that the attorney asked all of the technical questions, which seems odd, until you realize that he’s had to ask those because other people did not.', 'When questions seem to come out of nowhere—but they are really born of experience—they can shake people out of the fog of status quo and open their eyes to new thoughts.', 'The opinions presented here are personal, do not reflect the view of our employers, and are not professional product, consulting, or legal advice.', 'Source: Q The backstory: This is the kind of question you sometimes have to ask three times.', 'The first time, someone will try to hand you the company’s mission statement or slogan.', 'The second time, they’ll provide a description of the company: industry vertical, size, and revenue.', 'So you ask again, this time with an emphasis on the really.', 'And then you wait for the question to sink in, and for the person to work backwards from all of the company’s disparate activities to see what it’s all truly for.', 'Which will be somewhere between the raison d’etre and the sine qua non.', 'Taking the time to work this out is like building a mathematical model: if you understand what a company truly does, you don’t just get a better understanding of the present, but you can also predict the future.', 'It guides decisions such as what projects to implement, what competitors to buy, and whom to hire into certain roles.', 'As a concrete example, take Amazon.', 'Everyone thinks it’s a store.', 'It has a store, but at its core, Amazon is a delivery/logistics powerhouse.', 'Everything they do has to end with your purchases winding up in your hot little hands.', 'Nothing else they do matters—not the slick website, not the voice-activated ordering, not the recommendation engine—unless they get delivery and logistics down.', 'How I use it: I explore this early in a consulting relationship.', 'Sometimes even early in the sales cycle.', 'And I don’t try to hide it; I’ll ask it, flat-out, and wait for people to fill the silence.', 'Why it’s useful: My work focuses on helping companies to start, restart, and assess their ML/AI efforts.', 'Understanding the company’s true purpose unlocks the business model and sheds light on what is useful to do with the data.', 'As a bonus, it can also highlight cases of conflict.', 'Because sometimes key figures have very different ideas of what the company is and what it should do next.', 'When it doesn’t work so well: This question can catch people off-guard.', 'Since I work in the AI space, people sometimes have a preconceived notion that I’ll only talk about data and models.', 'Hearing this question from an ostensibly technical person can be jarring… though, sometimes, that can actually help the conversation along.', 'So it’s definitely a double-edged sword.', 'Source: Chris The backstory: Ideation is about coming up with the “best” ideas.', 'What is the best way to solve this problem?', 'What is the most important?', 'What is best for the business?', 'The problem with “best” is that it is tied up with all of the biases and assumptions someone already has.', 'To get to what really matters we have to understand the edge of what is good or bad.', 'The gray area between those tells you the shape of the problem.', 'Half the time this question will give you real, bad ideas.', 'What has been surprising to me is that the other half of the time, the so-called “bad” idea is really a “good” idea in disguise.', 'You just have to relax certain assumptions.', 'Often these assumptions were just set at some point without a reason or much to back it up.', 'How I use it: I like to ask this after going through a lot of the “best” questions in an ideation session.', 'It can be adapted to focus on different types of “bad,” like “stupid,” “wasteful,” and “unethical.”\\xa0 Ask follow up questions about why they believe the idea is “bad” and why it might actually be “good.” Why it’s useful: How can you truly know what is good without also knowing what is bad?', 'When it doesn’t work so well: When I was a design consultant working for clients in highly regulated industries (.e.g banking, insurance, etc.', '), I found this can be a difficult question to ask.', 'In those cases you will need to get your legal team to either grant the attorney/client privilege to ask the questions, or ask the prompt/response in such a way that it protects people in the conversation.', 'Source: Shane The backstory: In the early days of ML training data, companies and research teams frequently used “some stuff we found on the Internet” as a source for training data.', 'This approach has two problems: (1) there may not be an appropriate license attached to the data, and (2) the data may not be a good representative sample for the intended use.', 'It’s worth noting that the first issue is not just limited to images collected from the Internet.', 'In recent years a number of research datasets (including Stanford’s Brainwash, Microsoft’s MS Celeb, and Duke’s MTMC) were withdrawn for reasons including a lack of clarity around the permission and rights granted by people appearing in the datasets.', 'More recently, at least one company has earned itself significant PR and legal controversy for collecting training data sources from social media platforms under circumstances that were at least arguably a violation of both the platform’s terms of service and platform users’ legal rights.', 'The safest course of action is also the slowest and most expensive: obtain your training data as part of a collection strategy that includes efforts to obtain the correct representative sample under an explicit license for use as training data.', 'The next best approach is to use existing data collected under broad licensing rights that include use as training data even if that use was not the explicit purpose of the collection.', 'How I use it: I like to ask this as early as possible.', 'You don’t want to invest your time, effort, and money building models only to later realize that you can’t use them, or that using them will be much more expensive than anticipated because of unexpected licenses or royalty payments.', 'It’s also a good indirect measure of training data quality: a team that does not know where their data originated is likely to not know other important details about the data as well.', 'Why it’s useful: No matter how the data is collected, a review by legal counsel before starting a project—and allow me to emphasise the word before—can prevent significant downstream headaches.', 'When it doesn’t work so well:\\xa0 This question is most useful when asked before the model goes into production.', 'It loses value once the model is on sale or in service, particularly if it is embedded in a hardware device that can’t be easily updated.', 'Source: Shane The backstory: One of the most interesting aspects of machine learning (ML) is its very broad applicability across a variety of industries and use cases.', 'ML can be used to identify cats in photos as well as to guide autonomous vehicles.', 'Understandably, the potential harm caused by showing a customer a dog when they expected to see a cat is significantly different from the potential harm caused by an autonomous driving model failing to properly recognize a stop sign.', 'Determining the risk profile of a given model requires a case-by-case evaluation but it can be useful to think of the failure risk in three broad categories: How I use it: I use this question to determine both the potential risk from an individual failure and the potential aggregate risk from a systemic failure.', 'It also feeds back into my question about training data: some relatively minor potential harms are worth additional investment in training data and testing if they could inconvenience millions, or billions, of users or create a significant negative PR cycle for a company.', 'Why it’s useful: This is the sort of question that gets people thinking about the importance of their model in the overall business.', 'It can also be a helpful guide that companies invest in such a model, and the kinds of business processes that are amenable to models.', 'Remember that models that work nearly perfectly can still fail spectacularly in unusual situations.', 'When it doesn’t work so well: We don’t always have the luxury of time or accurate foresight.', 'Sometimes a business does not know how a model will be used: a model is developed for Product X and repurposed for Product Y, a minor beta feature suddenly becomes an overnight success, or a business necessity unexpectedly forces a model into widespread production.', 'Source: Q The backstory: A consultant is an agent of change.', 'When a prospect contacts me to discuss a project, I find it helpful to compare the cost of the desired change to the cost of another-change or even to the cost of the not-change.', '“What happens if you don’t do this?', 'What costs do you incur, what exposures do take on now?', 'And six months from now?”\\xa0A high cost of doing nothing means that this is an urgent matter.', 'Some consultants will tell you that a high cost of doing nothing is universally great (it means the prospect is ready to move) and a low cost is universally bad (the prospect isn’t really interested).', 'I see it differently: we can use that cost of doing nothing as a guide to how we define the project’s timeline, fee structure, and approach.', 'If the change is extremely urgent—a very high cost of doing nothing—it may warrant a quick fix now, soon followed by a more formal approach once the system is stable.', 'A low cost of doing nothing, by comparison, means that we can define the project as “research” or “an experiment,” and move at a slower pace.', 'How I use it: I will ask this one, flat-out, once a consulting prospect has outlined what they want to do.', 'Why it’s useful: Besides helping to shape the structure of the project, understanding the cost of doing nothing can also shed light on the prospect’s motivations.', 'That, in turn, can unlock additional information that can be relevant to the project.', '(For example, maybe the services I provide will help them reach the desired change, but that change won’t really help the company.', 'Perhaps I can refer them to someone else in that case.)', 'When it doesn’t work so well: Sometimes people don’t have a good handle on the risks and challenges they (don’t) face.', 'They may hastily answer that this is an urgent matter when it’s not; or they may try to convince you that everything is fine when you can clearly see that the proverbial house is on fire.', 'When you detect that their words and the situation don’t align, you can ask them to shed light on their longer-term plans.', 'That may help them to see the situation more clearly.', 'Source: Chris The backstory: This is something that was inspired from the intersection of an incredibly boring decision-science book and roadmap planning.', 'Decision trees and roadmaps are very useful when building out the possible spaces of the future.', 'However, for both decision trees and roadmaps we are usually overly optimistic in how we will proceed.', 'We fail at properly considering failure.', 'To appropriately plan for the future we must consider the different ways we can be wrong.', 'Sometimes it will be at a certain decision point (“we didn’t get enough signups to move forward”) or an event trigger (“we see too many complaints”).', 'If we consider this wrong-ness and the possible next step, we can start to normalize this failure and make better decisions.', 'How I use it:\\xa0 It’s best to ask this when you find that certainty is at a high point for the project.', 'More often than not, people don’t consider ways to detect that they need to change course.', 'Why it’s useful: You build a map into the future based on what you can detect.', 'This helps make hard decisions easier because you are effectively practicing the decision process before you are in the heat of the moment.', 'When it doesn’t work so well: When things are currently going “wrong” it can be a sensitive subject for people.', 'I’ve found it is easier to talk about how to get out of a current wrong situation than considering additional future situations.', 'Source: Shane The backstory: Imagine you employ a vendor to provide or enrich your training data, or you pay for consulting services related to ML.', 'What happens to the information used by the vendors to build your product?', 'Their downstream rights there run the gamut from “absolutely nothing” to “retaining a full copy of the training data, labels, trained models, and test results.” The median position, in my observation, tends to be that the vendor retains control of any new techniques and information derived from the work that would be useful in general, such as new methods of programmatically applying error correction to a trained model, but not the specific data used to train the model or the resulting trained model.', 'From the customer perspective, downstream rights are tied to competition/cost tradeoffs and the rights associated with training data.', 'A company that considers ML a competitive advantage likely will not want their models or derivative data available to competitors, and they must balance this against the business consideration that vendors which retain downstream rights typically charge lower fees (because reselling that data or models can be a source of revenue).', 'In addition, training data usually comes with contractual limitations and customers of ML services need to ensure they are not granting downstream rights that they don’t have in their upstream agreements.', 'Finally, some kinds of training data, such as medical records or classified government data, may forbid unauthorized access or use in systems that lack adequate safeguards and audit logs.', 'How I use it: This question is less relevant to companies that have an entirely in-house workflow (they generate their own training data, train their own models, and use models with their own employees and tools).', 'It is highly relevant to companies that buy or sell ML services, use external vendors for part of their workflow, or handle sensitive data.', 'Why it’s useful:\\xa0 The notion of downstream rights is not a new question, nor is it specific to the ML world.', 'Almost all vendor relationships involve delineating the intellectual property (IP) and tools that each party brings to the project, as well as the ownership of new IP developed during the project.', 'Helping founders to recognize and establish those boundaries early on can save them a lot of trouble later.', 'When it doesn’t work so well: This is a question a company definitely wants to answer before they’ve provided data or services to a counterparty.', 'These issues can be very difficult to resolve once data has been shared or work has begun.', 'Source: Q The backstory: A risk is a potential change that comes with consequences.', 'To properly manage risk—to avoid those consequences—you need to identify those changes in advance (perform a risk assessment) and sort out what to do about them (devise your risk mitigation plans).', 'That’s where this trio of questions comes in: “What if?” is the key to a risk assessment, as it opens the discussion on ways a project may deviate from its intended path.', '“Then?” explores the consequences of that deviation.', 'The “What next?” starts the discussion on how to handle them.', '“What if … our data vendor goes out of business?', 'Then?', 'Our business is hamstrung.', 'What next?', 'We’d better have a backup data vendor in the wings.', 'Or better yet, keep two vendors running concurrently so that we can switch over with minimal downtime.” “What if … something changes, and the model’s predictions are wrong most of the time?', 'Then?', 'We’re in serious trouble, because that model is used to automate purchases.', 'What next?', 'We should implement monitors around the model, so that we can note when it’s acting out of turn.', 'We should also add a ‘big red button’ so that a person can quickly, easily, and completely shut it down if it starts to go haywire.” How I use it: \\xa0Once we’ve sorted out what the client wants to achieve, I’ll round out the picture by walking them through some “What if?', 'Then?', 'What next?” scenarios where things don’t work out.', 'Why it’s useful: It’s too easy to pretend the not-intended outcomes don’t exist if you don’t bring them up.', 'I want my clients to understand what they’re getting into, so they can make informed decisions on whether and how to proceed.', 'Going through even a small-scale risk assessment like this can shed light on the possible downside loss that’s lurking alongside their desired path.', 'All of that risk can weigh heavily on their investment, and possibly even wipe out any intended benefit.', 'When it doesn’t work so well: The business world, especially Western business culture, has a strange relationship with positive attitudes.', 'This energy can be infectious and it can help to motivate a team across the finish line.', 'It can also convince people to pretend that the non-intended outcomes are too remote or otherwise not worth consideration.', 'That’s usually when they find out, the hard way, what can really go wrong.', 'How to handle this varies based on your role in the company, internal company politics, your ability to bring about change, and your ability to weather a storm.', 'Source: Chris The backstory: The most important question is one that isn’t expected.', 'It is one that leads to unexpected answers.', 'We don’t have dialog for dialog sake; we do it to learn something new.', 'Sometimes the thing we learn is that we aren’t aligned.', 'I’ve found that the most unexpected thing is something that we wouldn’t choose based on our current thought process.', 'Randomly choosing a question from a collection appropriate for your domain is really valuable.', 'If you are building something for the web, what kinds of questions could you ask about a web project?', 'This is helpful when the checklists of things to do get too large to try all of them.', 'Pick a few at random.', 'You can take it a step further and pick questions from outside of your domain.', 'This can simply be a list of provocations that require a high amount of interpretation by you to make sense.', 'This is because randomness doesn’t work without the lens of human intuition.', 'Randomness without this intuition is just garbage.', 'We do the work to bridge from random questions to some new idea related to our problem.', 'We build the analogies in our mind even when something is seemingly not connected at first.', 'How I use it: When you find that you keep asking the same questions.', 'I have decks of cards like Oblique Strategies for provocations, Triggers for domain-specific questions, and others that can provide randomness.', 'Domain-specific random questions can also be very impactful.', 'Eventually, I expect models like GPT-n to provide appropriate random questions to prompts.', 'Why it’s useful: Even with all of the questions we ask to get out of bias, we are still biased.', 'We still have assumptions we don’t realize.', 'Randomness doesn’t care about your biases and assumptions.', 'It will ask a question that you think on the surface is stupid, but when you think about it is important.', 'When it doesn’t work so well: With teams that are high on certainty they may think of the random question as a toy or distraction.', 'The people I’ve found to be incredibly confident in their world trivialize the need to question bias.', 'They will even try to actively subvert the process sometimes.', 'If you hide the fact that a question was randomly chosen, it can go over better.', 'If you’re collecting facts—names, numbers, times—then narrow questions will suffice.', 'But if you’re looking to understand the bigger picture, if you want to get a meeting out of a rut, if you want people to reflect before they speak, then open-ended questions will serve you well.', 'Doubly so when they come from an unexpected source and at an unexpected time.', 'The questions we’ve documented here have helped us in our roles as an AI consultant, a product manager, and an attorney.', '(We also found it interesting that we use a lot of the same questions, which tells us how widely applicable they are.)', 'We hope you’re able to put our favorite questions to use in your work.', 'Perhaps they will even inspire you to devise and test a few of your own.', 'One point we hope we’ve driven home is that your goal in asking good questions isn’t to make yourself look smarter.', 'Nor is it to get the answers you want to hear.', 'Instead, your goal is to explore a problem space, shed light on new options, and mitigate risk.', 'With that new, deeper understanding, you’re more prepared to work on the wicked problems that face us in the workplace and in the world at large.'], ['Differential dataflow programs look like many standard “big data” computations, borrowing idioms from frameworks like MapReduce and SQL.', 'However, once you write and run your program, you can change the data inputs to the computation, and differential dataflow will promptly show you the corresponding changes in its output.', 'Promptly meaning in as little as milliseconds.', 'In this work, we create a true Many-to-Many multilingual translation model that can translate directly between any pair of 100 languages.', 'Our focus on non-English-Centric models brings gains of more than 10 BLEU when directly translating between non-English directions while performing competitively with the best single systems of WMT.'], ['You start with a single component, the nand gate.', 'Using this as the fundamental building block, you will build all other components necessary.', 'today we are unveiling Recursive Belief-based Learning (ReBeL), a general RL+Search algorithm that can work in all two-player zero-sum games, including imperfect-information games.', 'ReBeL builds on the RL+Search algorithms like AlphaZero that have proved successful in perfect-information games.', 'Unlike those previous AIs, however, ReBeL makes decisions by factoring in the probability distribution of different beliefs each player might have about the current state of the game, which we call a public belief state (PBS).', 'In other words, ReBeL can assess the chances that its poker opponent thinks it has, for example, a pair of aces.', 'We demonstrate our claim by implementing tensor algebra and stochastic gradient descent using lambda expressions for loss functions as a pipelined operator in a main memory database system.', 'Our approach enables common machine learning tasks to be performed faster than by extended disk-based database systems or as well as dedicated tools by eliminating the time needed for data extraction.', 'This work aims to incorporate gradient descent and tensor data types into database systems, allowing them to handle a wider range of computational tasks.'], ['Automerge is designed for creating local-first software, i.e.', 'software that treats a user’s local copy of their data (on their own device) as primary, rather than centralising data in a cloud service.', 'In this paper, we present HangFix, a software hang bug fixing framework which can automatically fix a hang bug that is triggered and detected in production cloud environments.', 'HangFix first leverages stack trace analysis to localize the hang function and then performs root cause pattern matching to classify hang bugs into different types based on likely root causes.', 'Next, HangFix generates effective code patches based on the identified root cause patterns.', 'We have implemented a prototype of HangFix and evaluated the system on 42 real-world software hang bugs in 10 commonly used cloud server applications.', 'Our results show that HangFix can successfully fix 40 out of 42 hang bugs in seconds.'], ['Focusing on the data entry and storage aspects, this article offers practical recommendations for organizing spreadsheet data to reduce errors and ease later analyses.', 'The basic principles are: be consistent, write dates like YYYY-MM-DD, do not leave any cells empty, put just one thing in a cell, organize the data as a single rectangle (with subjects as rows and variables as columns, and with a single header row), create a data dictionary, do not include calculations in the raw data files, do not use font color or highlighting as data, choose good names for things, make backups, use data validation to avoid data entry errors, and save the data in plain text files.', 'To our knowledge, this is the first exploration of a practical general purpose real number type that both reflects the mathematical laws of the real numbers, and also supports exact comparisons in situations in which that’s normally expected.', 'Here, we report a universal fabrication scheme to enable printing and room-temperature sintering of the metal nanoparticle on paper/fabric for FPCBs and directly on the human skin for on-body sensors with a novel sintering aid layer.', 'Consisting of polyvinyl alcohol (PVA) paste and nanoadditives in the water, the sintering aid layer reduces the sintering temperature.', 'Together with the significantly decreased surface roughness, it allows for the integration of a submicron-thick conductive pattern with enhanced electromechanical performance.', 'Various on-body sensors integrated with an FPCB to detect health conditions illustrate a system-level example.', 'A presentation of several novel ways to visualize 25 years of the Gartner Hype Cycle.'], ['The field of AI product management continues to gain momentum.', 'As the AI product management role advances in maturity, more and more information and advice has become available.', 'Our previous articles in this series introduce our own take on AI product management, discuss the skills that AI product managers need, and detail how to bring an AI product to market.', 'One area that has received less attention is the role of an AI product manager after the product is deployed.', 'In traditional software engineering, precedent has been established for the transition of responsibility from development teams to maintenance, user operations, and site reliability teams.', 'New features in an existing product often follow a similar progression.', 'For traditional software, the domain knowledge and skills required to develop new features differ from those necessary to ensure that the product works as intended.', 'Because product development and product operations are distinct, it’s logical for different teams and processes to be responsible for them.', 'In contrast, many production AI systems rely on feedback loops that require the same technical skills used during initial development.', 'Similarly, in “Building Machine Learning Powered Applications: Going from Idea to Product,” Emmanuel Ameisen states: “Indeed, exposing a model to users in production comes with a set of challenges that mirrors the ones that come with debugging a model.” As a result, at the stage when product managers for other types of products might shift to developing new features (or to other projects altogether), an AI product manager and the rest of the original development team should remain heavily involved.', 'One reason for this is to tackle the (likely) lengthy backlog of ML/AI model improvements that will be discovered after the product engages with the real world.', 'Another, of course, is to ensure that the product functions as expected and desired over time.', 'We describe the final responsibility of the AI PM as coordinating with the engineering, infrastructure, and site reliability teams to ensure all shipped features can be supported at scale.', 'This article offers our perspective into the practical details of the AI PM’s responsibilities in the latter parts of the AI product cycle, as well as some insight into best practices in execution of those responsibilities.', 'In Bringing an AI Product to Market, we distinguished the debugging phase of product development from pre-deployment evaluation and testing.', 'This distinction assumes a slightly different definition of debugging than is often used in software development.', 'We define debugging as the process of using logging and monitoring tools to detect and resolve the inevitable problems that show up in a production environment.', 'Emmanuel Ameisen again offers a useful framework for defining errors in AI/ML applications: “…three areas in particular are most important to verify: inputs to a pipeline, the confidence of a model and the outputs it produces.” To support verification in these areas, a product manager must first ensure that the AI system is capable of reporting back to the product team about its performance and usefulness over time.', 'This may manifest in several ways, including the collection of explicit user feedback or comments via channels outside of the product team, and the provision of mechanisms to dispute the output of the AI system where applicable.', 'Proper AI product monitoring is essential to this outcome.', 'From a technical perspective, it is entirely possible for ML systems to function on wildly different data.', 'For example, you can ask an ML model to make an inference on data taken from a distribution very different from what it was trained on—but that, of course, results in unpredictable and often undesired performance.', 'Therefore, deployed AI products should include validation steps to ensure that model inputs and outputs are within generally expected limits, before a model training or inference task is accepted as successful.', 'Ideally, AI PMs would steer development teams to incorporate I/O validation into the initial build of the production system, along with the instrumentation needed to monitor model accuracy and other technical performance metrics.', 'But in practice, it is common for model I/O validation steps to be added later, when scaling an AI product.', 'Therefore, the PM should consider the team that will reconvene whenever it is necessary to build out or modify product features that: The composition of these teams will vary between companies and products, but a typical cross-functional team would likely include representatives from Data Science (for product-level experimentation and inference task validation), Applied Science (for model performance and evaluation), ML Engineering (for data and feature engineering, as well as model pipeline support) and Software/Feature Engineering (for integration with the full stack of the AI product—such as UI/UX, cloud services, and dev ops tools).', 'Working together, this post-production development team should embrace continuous delivery principles, and prioritize the integration of any additional necessary instrumentation that was not already implemented during the model development process.', 'Finally, the AI PM must work with production engineering teams to design and implement the alerting and remediation framework.', 'Considerations include where to set thresholds for each persona, alert frequency, and the degree of remediation automation (both what’s possible and desired).', 'During testing and evaluation, application performance is important, but not critical to success.', 'In the production environment, when the outputs of an ML model are often a central (yet hidden) component of a greater application, speed and reliability are critically important.', 'It is entirely possible for an AI product’s output to be absolutely correct from the perspective of accuracy and data quality, but too slow to be even remotely useful.', 'Consider the case of autonomous vehicles: if the outputs from even one of the many critical ML models that comprise the vehicle’s AI-powered “vision” are delivered after a crash, who cares if they were correct?', 'In engineering for production, AI PMs must take into account the speed at which information from ML/AI models must be delivered (to validation tasks, to other systems in the product, and to users).', 'Technologies and techniques—such as engineering specifically for GPU/TPU performance and caching—are important tools in the deployment process, but they are also additional components that can fail, and thus be responsible for the failure of an AI product’s core functionality.', 'An AI PM’s responsibility is to ensure that the development team implements proper checks prior to release, and—in the case of failure—to support the incident response teams, until they are proficient in resolving issues independently.', 'AI product managers must also consider availability: the degree to which the service that an AI product provides is available to other systems and users.', 'Service Level Objectives (SLOs) provide a useful framework for encapsulating this kind of decision.', 'In an incident management blog post, Atlassian defines SLOs as: “the individual promises you’re making to that customer… SLOs are what set customer expectations and tell IT and DevOps teams what goals they need to hit and measure themselves against.', 'SLOs can be useful for both paid and unpaid accounts, as well as internal and external customers.” Service Level Indicators, Objectives, and Agreements (SLIs, SLOs, and SLAs) are well-known, frequently used, and well-documented tools for defining the availability of digital services.', 'For cloud infrastructure some of the most common SLO types are concerned with availability, reliability and scalability.', 'For AI products, these same concepts must be expanded to cover not just infrastructure, but also data and the system’s overall performance at a given task.', 'While useful, these constructs are not beyond criticism.', 'Chief among the challenges are: choosing the correct metrics to begin with, measuring and reporting once metrics are selected, and the lack of incentive for a service provider to update the service’s capabilities (which leads to outdated expectations).', 'Despite these concerns, service level frameworks can be quite useful, and should be in the AI PM’s toolkit when designing the kind of experience that an AI product should provide.', 'You must also take durability into account when building a post-production product plan.', 'Even if well-designed, multi-layer fault detection and model retraining systems are carefully planned and implemented, every AI-powered system must be robust to the ever-changing and naturally stochastic environment that we (humans) all live in.', 'Product managers should assume that any probabilistic component of an AI product will break at some point.', 'A good AI product will be able to self-detect and alert experts upon such a failure; a great AI product will be able to detect the most common problems and adjust itself automatically—without significant interruption of services for users, or high-touch intervention by human experts.', 'There are many ways to improve AI product durability, including: It’s worth noting that model durability and retraining can raise legal and policy issues.', 'For example, in many regulated industries, changing any core functionality of an AI system’s decision-making capability (i.e., objective functions, major changes to hyperparameters, etc.)', 'require not only disclosure, but also monitored testing.', 'As such, an AI Product Manager’s responsibility here extends to releasing not only a usable product, but one that can be ethically and legally consumed.', 'It’s also important to remember that no matter what the approach to developing and maintaining a highly durable AI system, the product team must have access to high quality, relevant metrics on both model performance and functionality.', 'Proper monitoring (and the software instrumentation necessary to perform it) is essential to the success of an AI product.', 'However, monitoring is a loaded term.', 'The reasons for monitoring AI systems are often conflated, as are the different types of monitoring and alerting provided by off-the-shelf tools.', 'Emmanuel Ameisen once again provides a useful and concise definition of model monitoring as a way to “track the health of a system.', 'For models, this means monitoring their performance and the equity of their predictions.” The simplest case of model monitoring is to compute key performance metrics (related to both model fit and inference accuracy) regularly.', 'These metrics can be combined with human-determined thresholds and automated alerting systems to inform when a model has “drifted” beyond normal operating parameters.', 'While ML monitoring is a relatively new product area, standalone commercial products (including Fiddler and superwise.ai) are available, and monitoring tools are incorporated into all the major machine learning platforms.', 'Separate from monitoring for model freshness, Ameisen also mentions the need to apply technical domain experience in designing monitoring systems that detect fraud, abuse, and attack from external actors.', 'AI PMs should consult with Trust & Safety and Security teams to combine the best principles and technical solutions with existing AI product functionality.', 'In some specific domains—such as financial services or medicine—no easy technical solutions exist.', 'In this case, it is the responsibility of the AI product team to build tools to detect and mitigate fraud and abuse in the system.', 'As we’ve mentioned previously, it’s not enough to simply monitor an AI system’s performance characteristics.', 'It is even more important to consistently ensure that the AI product’s user-facing and business purposes are being fulfilled.', 'This responsibility is shared by the development team with Design, UX Research, SRE, Legal, PR, and Customer Support teams.', 'The AI PM’s responsibility is again to orchestrate reasonable and easily repeatable mitigations to any problems.', 'It is crucial to design and implement specific alerting capabilities for these functions and teams.', 'If you simply wait for complaints, they will arise far too late in the cycle for your team to react properly.', 'No matter how well you research, design, and test an AI system, once it is released, people are going to complain about it.', 'Some of those complaints will likely have merit, and responsible stewardship of AI products requires that users are given the ability to disagree with the system’s outputs and escalate issues to the product team.', 'It is also entirely possible for this feedback to show you that the system is underserving a particular segment of the population, and that you may need a portfolio of models to serve more of the user base.', 'As an AI PM, you have the responsibility to build a safe product for everyone in the population who might use it.', 'This includes consideration of the complexities that come into play with intersectionality.', 'For example, an AI product might produce great outcomes for wealthy, American, cisgender, heterosexual, White women—and although it might be tempting to assume those outcomes would apply to all women, such an assumption would be incorrect.', 'Returning to previous anti-bias and AI transparency tools such as Model Cards for Model Reporting (Timnit Gebru, et al.)', 'is a great option at this point.', 'It is important not to pass this development task off to researchers or engineers alone; it is an integral part of the AI product cycle.', 'If done right, users will never be aware of all the product monitoring and alerting that is in place, but don’t let that trick you.', 'It’s essential to success.', 'One question that an AI PM might ask when pondering these post-production requirements is: “This seems hard; can’t I just buy these capabilities from someone else?” This is a fair question, but—as with all things related to machine learning and artificial intelligence—the answer is far from a binary yes or no.', 'There are many tools available to help with this process, from traditional vendors and bleeding edge startups alike.', 'Deciding what investment to make in MLOps tooling is an inherently complex task.', 'However, careful consideration and proactive actions often lead to defendable competitive advantages over time.', 'Uber (the developer of Michelangelo), Airbnb (developer of zipline), and Google have all taken advantage of superior tooling and operations skills to build market-leading AI products.', 'Nearly every ML/AI library touts full end-to-end capabilities, from enterprise-ready stacks (such as H20.ai, MLFlow, and Kubeflow) to the highly specialized and engineer-friendly (such as Seldon.io) and everything in-between (like Dask).', 'Enterprise level-frameworks often provide deep and well-supported integration with many common production systems; smaller companies might find this integration unnecessary or overly cumbersome.', 'Regardless, it’s a safe bet that getting these off-the-shelf tools to work with your AI product in the exact ways you need them to will be costly (if not financially, then at least in time and human labor).', 'That said—from a scale, security and features perspective—such capabilities may be required in many mature AI product environments.', 'On the other hand, building and scaling a software tool stack from scratch requires a significant sustained investment in both developer time and technology.', 'Facebook, Uber, AirBnB, Google, Netflix, and other behemoths have all spent millions of dollars to build their ML development platforms; they also employ dozens to hundreds of employees, each tasked with building and scaling their internal capabilities.', 'The upside here is that such end-to-end development to deployment frameworks and tools eventually become a competitive advantage, in and of themselves.', 'However, it’s worth noting that in such environments, employing a single AI PM is not feasible.', 'Instead, a cadre of PMs focused on different components of the AI product value chain are needed.', 'Building great AI products is a significant, cross-disciplinary, and time-consuming undertaking, even for the most mature and well-resourced companies.', 'However, what ML and AI can accomplish at scale can be well worth the investment.', 'Although a return on investment is never guaranteed, our goal is to provide AI PMs with the tools and techniques needed to build highly engaging and impactful AI products in a wide variety of contexts.', 'In this article, we focused on the importance of collaboration between product and engineering teams, to ensure that your product not only functions as intended, but is also robust to both the degradation of its effectiveness and the uncertainties of its operating environment.', 'In the world of machine learning and artificial intelligence, a product release is just the beginning.', 'Product managers have a unique place in the development ecosystem of ML/AI products, because they cannot simply guide the product to release and then turn it over to IT, SRE, or other post-production teams.', 'AI product managers have a responsibility to oversee not only the design and build of the system’s capabilities, but also to coordinate the team during incidents, until the development team has completed enough knowledge transfer for independent post-production operation.', 'The evolution of AI-enabled product experiences is accelerating at breakneck speed.', 'In parallel, the emerging role of AI product management continues to evolve at a similar pace, to ensure that the tools and products delivered to the market provide true utility and value to both customers and businesses.', 'Our goal in this four-part series on AI product management is to increase community awareness and empower individuals and teams to improve their skill sets in order to effectively steer AI product development toward successful outcomes.', 'The best ML/AI products that exist today were brought to market by teams of PhD ML/AI scientists and developers who worked in tandem with resourceful and skilled product teams.', 'All were essential to their success.', 'As the field of AI continues to mature, so will the exciting field of AI product management.', 'We can’t wait to see what you build!', 'We would like to thank the many people who have\\xa0 contributed their expertise to the early drafts of the articles in this series, including: Emmanuel Ameisen, Chris Albon, Chris Butler, Ashton Chevalier, Hilary Mason, Monica Rogati, Danielle Thorp, and Matthew Wise.'], ['CG/SQL is a code generation system for the popular SQLite library that allows developers to write stored procedures in a variant of Transact-SQL (T-SQL) and compile them into C code that uses SQLite’s C API to do the coded operations.', 'CG/SQL enables engineers to create highly complex stored procedures with very large queries, without the manual code checking that existing methods require.', '(1) Social media is addictive, and we are powerless to resist it.', 'The concept of addiction does not encompass the full range of pleasures, risks, and uses that people create with technology.', '(2) Technology companies can fix the problems they create with better technology.', 'Some technology cannot be fixed by more design, and some technology should not be built at all.', '(3) Growth and engagement metrics are the best drivers of decision making at tech companies.', 'Many of the most important parts of digital well-being cannot be captured by quantitative metrics.', '(4) Our health and well-being depend on spending less time with screens and social media platforms.', 'Health and well-being cannot be reduced to the single variable of screen time.', 'a web-based multi-emulator (RetroArch/libretro) designed to recreate the experience of playing console games with a single controller in a room full of friends.'], ['The release of GPT-3 has reinvigorated a discussion of creativity and artificial intelligence.', 'That’s a good discussion to have, primarily because it forces us to think carefully about what we mean when we use words like “creativity” and “art.” As I’ve argued in the past, each time we have this discussion, we end up raising the bar.', 'Each time an AI system does something that looks “intelligent” or creative, we end up deciding that’s not what intelligence really is.', 'And that’s a good thing.', 'AI is likely to teach us more about what intelligence and creativity are not than about what they are.', 'I’m not terribly interested in whether AI can imitate human creativity.', '“Can an AI create a ‘new’ poem that reads as if it were written by Keats, or a new piano sonata that sounds like Beethoven” isn’t a question that’s worth asking.', 'Of course it can—if not now, it will be able to in the near future.', 'We really don’t need a new Beethoven sonata; the 32 he wrote are enough.', 'Nor do we need a new Keats ode, limited though his output was.', 'Or a new Rembrandt.', 'Imitation is ultimately a party trick: clever and amusing, but not really important.', 'Sure, if you want texts for greeting cards or elevator music (or maybe even commercial pop), algorithms may do the trick.', 'What’s really important is the transition between different forms of creativity.', 'How do you get something that’s qualitatively new, and not just imitation?', 'Creativity isn’t about the artifacts as much as it’s about the transitions.', 'How do you get from Bach to Haydn?', 'How do you get from Haydn to Beethoven?', 'And, even within the career of a single artist: how do you get from the beginning to the end?', 'How do you get from Beethoven’s first piano sonata, which sounds like Haydn, to the last, which at some points anticipates jazz?', 'Artists aren’t stagnant.', 'But I have no idea how even to ask whether an AI system can “mature” or “grow” in its output.', 'Great artists (and yes, I’m presuming a lot with the word “great”) frequently work by defining themselves against what came before.', 'This is particularly clear with artists of the Romantic period in Germany, England, and France.', 'The term Romanticism didn’t come around until some years later, but they left behind several manifestos describing what they were trying to do, and how it was different from what came before.', 'That is still how artists work: Nnedi Okorafor’s Africanfuturism is important as a way of defining and directing her own work.', 'The importance of these defining statements isn’t so much about “rightness” (in the sense of “this is what art is or should be”) but in setting a direction for their project.', 'Can a machine do that?', 'Can it decide how its work will be different from what came before?', 'It’s not clear to me that it can’t, but that’s a significant step beyond any machine learning projects we currently have.', 'Although artists work by projecting their work into the future, by defining something “new,” their work is also derived from what came before—possibly as misinterpretation, but almost always as revision.', 'Listen to the Beatles, and you hear something that was really built on the backbone of blues, filtered through some British pop-cultural trends.', 'At the end of the “His Dark Materials” trilogy, Phillip Pullman thanks all the authors he stole from.', 'Or, as T. S. Eliot said, “Immature poets imitate; mature poets steal; bad poets deface what they take, and good poets make it into something better, or at least something different.” Artists break from the past by reinterpreting that past.', 'For AI-generated art–where does that sense of “different” come from?', 'Can artificial intelligence learn to steal and reinterpret?', 'Where does that engagement with history, current events and even selfhood come from?', 'Without that, there’s no basis for reinterpretation aside from random perturbation.', 'It’s not clear that a sense of history couldn’t come from a big model trained on a gigantic corpus (although the best we can do now is build models that have no idea what they are saying).', 'What kind of model would take that corpus and make something that was different, something that hadn’t been seen before?', 'Would we care if Hamlet wasn’t written by Shakespeare in a specific historical context, but in 2025 by a computer trained on an archive of Elizabethan politics, drama, and history?', '(Never mind that an archive of Elizabethan drama would be very skimpy; most plays from that period were never published.)', 'Would anyone care about the opera Nixon in China if it didn’t reflect a composer’s, and a librettist’s, thinking about historical events?', 'There’s another way in which I find computed-generated artworks unsatisfactory, particularly in music.', 'AI-generated music is often interesting over the short term, but fails at larger-scale structure.', 'Much of music history, from four-bar blues to Beethoven’s massive experiments in sonata form, is about building structures that can be interesting over the long term, whether “long term” means a few minutes of a blues song to several hours of opera.', 'That was Beethoven’s project; more recently, it was the project of rock groups like Pink Floyd.', 'It seems conceivable that a model could learn to generate such longer-form structures, but I’ve yet to see one that does it satisfactorily.', 'Is this where collaboration between humans and machines comes in, and if so, what does that say about creativity?', 'A machine could conceivably do the pattern-matching and combinatorics, assembling a creative work out of news clippings and stylistic mimicry.', 'But a human still needs to supply the sense of history that makes the work something we care about.', 'A human still needs to provide the structure that makes art works more than brief curiosities.', 'Is it possible for a human could tweak a model like GPT-3 to give it that sense of direction and context?', 'What kinds of user interfaces would facilitate this kind of interaction?', 'I can’t answer those questions, but that sounds like a much more interesting form of digital collaboration than having an algorithm write hundreds of dull poems or songs and “curating” a few that aren’t boring.', 'That’s just a recipe for greeting-card sentiment and elevator music.', 'I mean no disrespect to Hallmark—mass-produced poetry for greeting cards serves a purpose—but when we think about what kinds of creativity we want, and how that creativity will be mediated by AI, we should demand more.'], [' To analyze the possible consequences, we study experimentally the behavior of algorithms powered by Artificial Intelligence (Q-learning) in a workhorse oligopoly model of repeated price competition.', 'We find that the algorithms consistently learn to charge supracompetitive prices, without communicating with one another.', 'The high prices are sustained by collusive strategies with a finite phase of punishment followed by a gradual return to cooperation.', 'This finding is robust to asymmetries in cost or demand, changes in the number of players, and various forms of uncertainty.', 'RAND researchers developed Hedgemony, a wargame designed to teach U.S. defense professionals how different strategies could affect key planning factors in the trade space at the intersection of force development, force management, force posture, and force employment.', 'The game presents players, representing the United States and its key strategic partners and competitors, with a global situation, competing national incentives, constraints, and objectives; a set of military forces with defined capacities and capabilities; and a pool of periodically renewable resources.', 'The players are asked to outline their strategies and are then challenged to make difficult choices by managing the allocation of resources and forces in alignment with their strategies to accomplish their objectives within resource and time constraints.', 'Run Android applications on any GNU/Linux operating system.'], ['On its own, using a simple DC voltage as the input, the device outputs not just simple spikes, as some other devices can manage, but the whole array of neural activity—bursts of spikes, self-sustained oscillations, and other stuff that goes on in your brain connected communities; the Truth Sandwich; pre-bunking; distributed debunking; localize the context; humor over rumor.'], ['This month, the big surprise is that there’s no significant technology news about COVID.', 'And there is more news than ever about legislation and regulation.', 'I suspect that the legal system will be a big driver for technology over the next year.', 'Another trend that doesn’t quite count as technology news but that definitely bears watching is that college enrollment in the US is down.', 'Grad schools are up, 4 year colleges are down slightly; the big hit is in 2 year colleges.', 'COVID is probably the biggest contributing factor, but regardless of the cause, this is an inauspicious trend.'], [' So he then examined the mechanism the coffee maker used to receive firmware updates.', 'It turned out they were received from the phone with—you guessed it—no encryption, no authentication, and no code signing.', 'Big machines are sometimes more efficient.', 'But they cost more, so fewer can be produced with a finite budget.', 'Small machines are cheaper and may benefit from improvement over time, driven by experience in building more units.', 'When does this experience lead to greater overall efficiency?', 'We derive an approximation which, given a learning rate, tells how much smaller a machine must be to overcome an initial efficiency disadvantage.', 'The first work that specifically addressed the detection of automated accounts in online social networks dates back to January 2010.'], ['A program P is incremental if repeating P with a changed input is faster than from-scratch computation.', 'Adapton offers programming language abstractions for incremental computation.', 'Keep your migration scripts away from your production code; Keep it low-tech, don’t deserialize; Write tests to exercise each migration script individually; Consider running long migrations online; Consider versioning your documents.', 'However, we do not look at image formats from a general point of view, but rather think of ways to glitch them.', 'When we look at PNG from the point of view of glitch, what kind of peculiarity does it have?'], ['Figuring out what shapes proteins fold into is known as the “protein folding problem”, and has stood as a grand challenge in biology for the past 50 years.', 'In a major scientific advance, the latest version of our AI system AlphaFold has been recognised as a solution to this grand challenge by the organisers of the biennial Critical Assessment of protein Structure Prediction (CASP).', 'The organizers even worried DeepMind may have been cheating somehow.', 'So Lupas set a special challenge: a membrane protein from a species of archaea, an ancient group of microbes.', 'For 10 years, his research team tried every trick in the book to get an x-ray crystal structure of the protein.', '“We couldn’t solve it.” But AlphaFold had no trouble.', 'It returned a detailed image of a three-part protein with two long helical arms in the middle.', 'The model enabled Lupas and his colleagues to make sense of their x-ray data; within half an hour, they had fit their experimental results to AlphaFold’s predicted structure.', '“It’s almost perfect,” Lupas says.', '“They could not possibly have cheated on this.', 'I don’t know how they do it.” Far more useful (and to me, more impressive) than AlphaGo.', 'Purpose-First Programming — Some students resist the cognitively-heavy tasks of simulating program execution.', 'The secret to teaching those folks to program may be “purpose-first programming”: She used Github repositories and expert interviews to identify a few programming plans (just like Elliot Soloway and Jim Spohrer studied years ago) that were in common use in a domain that her participants cared about.', 'She then taught those plans.', 'Students modified and combined the plans to create programs that the students found useful.', 'Rather than start with syntax or semantics, she started with the program’s purpose.', 'Very reminiscent of the late 90s Perl and PHP copy-and-change coding boom that got orders of magnitude more people programming than were coming through CS courses at the time.', 'She used Github repositories and expert interviews to identify a few programming plans (just like Elliot Soloway and Jim Spohrer studied years ago) that were in common use in a domain that her participants cared about.', 'She then taught those plans.', 'Students modified and combined the plans to create programs that the students found useful.', 'Rather than start with syntax or semantics, she started with the program’s purpose.'], ['This class examines ethical frameworks, modern ethical concerns related to computer science and technology, and clear oral and written communication.', 'Topics we will explore include policy vacuums created by new technology, copyright and patent, software bugs and liability, freedom of speech, privacy, security, employment and job markets, warfare and state-building, wealth discrepancy and consumerism, environmental impact, and changing cultural norms and social contracts.', 'What follows is part position paper and part ahistorical review.', 'This essay introduces the term hardware lottery to describe when a research idea wins because it is compatible with available software and hardware and not because the idea is superior to alternative research directions.', 'We argue that choices about software and hardware have often played a decisive role in deciding the winners and losers in early computer science history.', 'a Bitsy game about learning to rely on others and fighting against hopelessness, together.', 'If you are writing down “rules” and insisting that developers abide by them, it’s probably because your developers are continuously doing things you wish they wouldn’t.', 'Usually, this isn’t because your developers don’t understand “the rules” and/or don’t like you—it’s because they know what the organization values, and those values are in conflict with your “rules,” and they’re trying to deliver that value.'], ['a higher level taxonomy that I use to think about concurrent performance.', 'We’ll group the performance of concurrent operations into six broad levels running from fast to slow, with each level differing from its neighbors by roughly an order of magnitude in performance.', 'without more geographic representation, they’ll produce a global vision for AI ethics that reflects the perspectives of people in only a few regions of the world, particularly North America and northwestern Europe.', '[…] This lack of regional diversity reflects the current concentration of AI research (pdf): 86% of papers published at AI conferences in 2018 were attributed to authors in East Asia, North America, or Europe.', 'And fewer than 10% of references listed in AI papers published in these regions are to papers from another region.', 'Patents are also highly concentrated: 51% of AI patents published in 2018 were attributed to North America.', 'As a result, the local model is only useful for queries with a very strong “signal.” Apple’s system, for example, uses the local model to estimate the popularity of emojis, but the results are only useful for the most popular emojis (i.e.', 'where the “signal” is strongest).', 'The local model is typically not used for more complex queries, like those used in the U.S. Census [3] or applications like machine learning.'], ['AI is a field where value, in the form of outcomes and their resulting benefits, is created by machines exhibiting the ability to learn and “understand,” and to use the knowledge learned to carry out tasks or achieve goals.', 'AI-generated benefits can be realized by defining and achieving appropriate goals.', 'These goals depend on who the stakeholder is; in other words, the person or company receiving the benefits.', 'There are three potential stakeholders for AI applications, with a single application often involving all three.', 'They are business stakeholders, customers, and users.', 'Each type of stakeholder has different and unique goals; each group is most interested in having their specific objectives met, or problems solved.', 'My book, AI for People and Business, introduces a framework that highlights the fact that both people and businesses can benefit from AI in unique and different ways.', 'A typical social media platform needs to satisfy all three stakeholders.', 'In the case of Twitter, the business stakeholder’s top goals are likely centered around profits and revenue growth.', 'Customer stakeholders are the people and companies that advertise on the platform, and are most concerned with ROI on their ad spend.', 'User stakeholders are interested in benefiting from the platform’s functionality: staying up-to-date, quickly finding new people and topics to follow, and engaging with family and friends.', 'Goals should be defined specifically and at a granular level for each stakeholder and relevant use case.', 'Twitter has no doubt went through this exercise long ago; but if we imagine Twitter taking its first steps towards AI, some specific and granular goals could be to build a recommendation engine that helps users find the most relevant people to follow (a goal for users), while also building an AI-powered advertising targeting engine that best matches ads with those most likely to be interested in the product or service being advertised (for customers).', 'This in turn would increase the platform’s value for users and thus increase engagement, which would result in more eyes to see and interact with ads, which would mean better ROI on ad spend for customers, which would then achieve the goal of increased revenue and customer retention (for business stakeholders).', 'The key is to start with small and easily identifiable AI projects that will trickle value upwards towards a company’s highest priority goals.', 'For companies early in their AI journey, setting appropriate goals helps create a foundation from which to build AI maturity.', 'It also helps companies learn how to translate existing AI capabilities into solving specific real-world problems and use cases.', 'In my book, I introduce the Technical Maturity Model: I define technical maturity as a combination of three factors at a given point of time.', 'These factors are: There’s a lot of overlap between these factors.', 'Defining them precisely isn’t as important as the fact that you need all three.', 'Higher levels of experience, technical sophistication, and technical competence increase technical maturity.', 'Increased AI technical maturity boosts certainty and confidence, which in turn, results in better and more efficient AI-powered outcomes and success.', 'Technical maturity is a major factor behind why some companies are very successful with AI, while other companies struggle to get started and/or achieve success.', 'Turning an AI idea into actual benefits is difficult and requires the “right” goals, leadership, expertise, and approach.', 'It also requires buy-in and alignment at the C-level.', 'Identifying, prioritizing, and goal-setting for AI opportunities is a multi-functional team effort that should include business folks, domain experts, and AI practitioners and researchers.', 'This helps ensure alignment with company goals, while also including necessary business and domain expertise.', 'AI initiatives may also require significant considerations for governance, compliance, ethics, cost, and risk.', 'Further, while the technical details of AI are complex, the outputs of AI techniques are relatively simple.', 'In most cases, AI solutions are built to map a set of inputs to one or more outputs, where the outputs fall into a small group of possibilities.', 'Outputs from trained AI models include numbers (continuous or discrete), categories or classes (e.g., spam or not-spam), probabilities, groups/segments, or a sequence (e.g., characters, words, or sentences).', 'Therefore, AI techniques don’t just solve real-world problems out of the box.', 'They don’t automatically generate revenue and growth, maximize ROI, or keep users engaged and loyal.', 'Likewise, AI doesn’t inherently optimize supply chains, detect diseases, drive cars, augment human intelligence, or tailor promotions to different market segments.', 'Setting a company-wide goal of reducing customer churn by 25% is great, but, unfortunately, is far too broad for most AI applications.', 'That’s why customer churn reduction is not a natural output of AI techniques.', 'The mismatch between goals like reducing customer churn and actual AI outputs must be properly handled and mapped.', 'AI goals should be appropriate for a given company’s technical maturity, and should be chosen to maximize the likelihood of success, prove value, and build a foundation from which to create increasingly sophisticated AI solutions that achieve higher-level business goals.', 'A crawl, walk, run approach is a good analogy for this.', 'Goals should be well-formed, meaning they are stakeholder-specific, map actual AI outputs to applications and use cases that achieve business goals, and are appropriately sized.', 'For companies early in their AI maturity, appropriately-sized goals mean that they should be small and specific enough to experiment with, and prove potential value from, relatively quickly (think lean methodologies and incremental).', 'As AI maturity increases, a non-incremental, holistic, and organization-wide AI vision and strategy should be created to achieve hierarchically-aligned AI goals of varying granularity—goals that drive all AI initiatives and development.', 'This should be accompanied by a transition from incremental thinking to big vision, “applied AI transformation” thinking.', 'Let’s consider the overall goal of reducing customer churn.', 'In an early stage of AI maturity, we can build AI solutions that reduce search friction (e.g., Netflix and Amazon recommendation engines), increase stickiness through personalized promotions and content that is more relevant and engaging, create a predictive model to identify customers most likely to churn and take appropriate preventative actions, or automate and optimize results in areas that are outside of a person’s primary area of expertise (e.g., automated retirement portfolio rebalancing and maximized ROI).', 'When transitioning to developing a bigger AI vision and strategy, we may create a prioritized product roadmap consisting of a suite of recommendation engines and an AI-based personalized loyalty program, for example.', 'At the individual goal level, and for each well-formed goal, the same multi-functional team mentioned earlier must work collaboratively to determine what AI opportunities are available, select and prioritize the ones to pursue, and determine the technical feasibility of each.', 'There are frameworks like SMART to help characterize well-formed goals, but since AI is a field that I characterize as scientific innovation (like R&D), characteristics like being achievable and time-bound may not be the best goals.', 'Results are typically achieved through a scientific process of discovery, exploration, and experimentation, and these processes are not always predictable.', 'Given the scientific nature of AI, goals are better expressed as well-posed questions and hypotheses around a specific and intended benefit or outcome for a certain stakeholder.', 'With well-formed goals, data scientists and machine learning engineers can then apply the scientific method to test different approaches in order to determine the validity of the hypothesis, and assess whether a given approach is feasible and can achieve the goal.', 'For example, by introducing the “Frequently bought together” recommendations (and other recommendations), Amazon was able to increase average customer shopping cart size and order amount (i.e., up-sell and cross-sell), which in turn increases average revenue per customer, which in turn increases Amazon’s e-commerce generated revenue per quarter.', 'McKinsey estimates that up to 35% of Amazon’s revenue and 75% of everything watched on Netflix comes from AI-powered recommendations.', 'But when defining an AI project, the goal or hypothesis in this case isn’t to increase top-line revenue for the company, but rather to posit that building an application that groups products by likelihood to be purchased together will increase average customer order size, which in turn will have an upward impact on top level goals like increasing average revenue per customer and top-line revenue.', 'Another example would be setting a goal around building a well-performing AI model that can predict demand (number of units likely to be purchased) for a specific product for a given day, time, and weather conditions.', 'If accurate, this prediction can help a retailer ensure that they do not run out of stock, which means that there is no lost revenue because a product is out of stock.', 'An added benefit is improved customer experience, which results in happier and more loyal customers who are able to buy the products they want whenever they want to buy it.', 'This same approach can be applied to virtually any other application of AI.', 'AI and machine learning technologies have come a long way in terms of capabilities and accessibility, but off-the-shelf AI solutions aren’t yet available for specific industries or business domains, companies, sets of data, applications, and use cases.', 'The key to success with AI is assembling a multi-functional team that defines appropriate goals, then letting these goals drive the AI initiatives and projects.'], ['In our paper, A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild, ACM Multimedia 2020, we aim to lip-sync unconstrained videos in the wild to any desired target speech.'], ['“There are lot of definitions of what a developer is […] It’s not just people who write code.” […] Microsoft has even given these “civilian” programmers a persona: Mort.', '[…] The fictional “Mort” is a skilled professional, anyone from a business analyst to a construction site cost estimator, who needs computers to perform specific functions without mastering the intricacies of full-blown programming.', 'We don’t need complete, perfect solutions; we need partial solutions in situations where we don’t have all the information, and we need the ability to explore those solutions with an (artificially) intelligent partner.'], ['In a conversation with Kevlin Henney, we started talking about the kinds of user interfaces that might work for AI-assisted programming.', 'This is a significant problem: neither of us were aware of any significant work on user interfaces that support collaboration.', 'However, as software developers, many of us have been practicing effective collaboration for years.', 'It’s called pair programming, and it’s not at all like the models we’ve seen for interaction between an AI system and a human.', 'Most AI systems we’ve seen envision AI as an oracle: you give it the input, it pops out the answer.', 'It’s a unidirectional flow from the source to the destination.', 'This model has many problems; for example, one reason medical doctors have been slow to accept AI may be that it’s good at giving you the obvious solution (“that rash is poison ivy”), at which point the doctor says “I knew that…” Or it gives you a different solution, to which the doctor says “That’s wrong.” Doctors worry that AI will “derail clinicians’ conversations with patients,” hinting that oracles are unwelcome in the exam room (unless they’re human).', 'Shortly after IBM’s Watson beat the world Jeopardy champions, IBM invited me to see a presentation about it.', 'For me, the most interesting part wasn’t the Jeopardy game Watson played against some IBM employees; it was when they showed the set of answers Watson considered before selecting its answer, weighted with their probabilities.', 'That level was the real gold.', 'We don’t need an AI system to tell us something obvious, or something we can Google in a matter of seconds.', 'We need AI when the obvious answer isn’t the right one, and one of the possible but rejected answers is.', 'What we really need is the ability to have a dialog with the machine.', 'We still don’t have the user interface for that.', 'We don’t need complete, perfect solutions; we need partial solutions in situations where we don’t have all the information, and we need the ability to explore those solutions with an (artificially) intelligent partner.', 'What is the logic behind the second, third, fourth, and fifth solutions?', 'If we know the most likely solution is wrong, what’s next?', 'Life is not like a game of Chess or Go—or, for that matter, Jeopardy.', 'What would this look like?', 'One of the most important contributions of Extreme Programming and other Agile approaches was that they weren’t unidirectional.', 'These methodologies stressed iteration: building something useful, demo-ing it to the customer, taking feedback, and then improving.', 'Compared to Waterfall, Agile gave up on the master plan and specification that governed the project’s shape from start to finish, in favor of many mid-course corrections.', 'That cyclic process, which is about collaboration between software developers and customers, may be exactly what we need to get beyond the “AI as Oracle” interaction.', 'We don’t need a prescriptive AI writing code; we need a round trip, in which the AI makes suggestions, the programmer refines those suggestions, and together, they work towards a solution.', 'That solution is probably embedded in an IDE.', 'Programmers might start with a rough description of what they want to do, in an imprecise, ambiguous language like English.', 'The AI could respond with a sketch of what the solution might look like, possibly in pseudo-code.', 'The programmer could then continue by filling in the actual code, possibly with extensive code completion (and yes, based on a model trained on all the code in GitHub or whatever).', 'At this point, the IDE could translate the programmer’s code back into pseudo-code, using a tool like \\xa0Pseudogen (a promising new tool, though still experimental).', 'Any writer, whether of prose or of code, knows that having someone tell you what they think you meant does wonders for revealing your own lapses in understanding.', 'MISIM is another research project that envisions a collaborative role for AI.', 'It watches the code that a developer is writing, extracting its meaning and comparing it with similar code.', 'It then makes suggestions about rewriting code that looks buggy or inefficient, based on the structure of similar programs.', 'Although its creators suggest that MISIM could eventually lead to machines that program themselves, that’s not what interests me; I’m more interested in the idea that MISIM is helping a human to write better code.', 'AI is still not very good at detecting and fixing bugs; but it is very good at asking a programmer to think carefully when it looks like something is wrong.', 'Is this pair programming with a machine?', 'Maybe.', 'It is definitely enlisting the machine as a collaborator, rather than as a surrogate.', 'The goal isn’t to replace the programmers, but to make programmers better, to work in ways that are faster and more effective.', 'Will it work?', 'I don’t know; we haven’t built anything like it yet.', 'It’s time to try.'], ['Don’t push the responsibility of maintaining invariants required by your class on to its callers.', 'a scalable open-source multi-model database natively supporting graph, document and search.', 'All supported data models & access patterns can be comnbined in queries allowing for maximal flexibility.'], ['A VS Code extension for visualizing data structures while debugging.', 'Like the VS Code’s watch view, but with rich visualizations of the watched value.', 'an integrated dataflow environment for end-users.', 'It allows users to interact with modules that implement functionality for different domains from a single user interface and combine these modules in creative ways.', 'a library that allows you to write declarative logic programs in Rust, with a Datalog-like syntax From initial testing, the generated code is very fast.', 'Variants of transitive closure for large graphs (~1000 nodes) run at comparable speed to compiled Souffle, and at a fraction of the compilation time.'], ['Compared to the last few months, there are relatively few items about COVID.', 'And almost no items about Blockchains, though the one item I’ve listed, about China’s Blockchain Services Network, may be the most important item here.', 'I’m seeing a steady stream of articles about various forms of no-code/low-code programming.', 'While many programmers scoff at the idea of programming-without-programming, spreadsheets are an early example of low-code programing.', 'Excel is hardly insignificant.', 'On the AI front, the most significant change is discussion (see the thread below) of a “Deep Learning Recession,” as companies under pressure from COVID look for results and can’t find them.'], ['app that automatically tracks how you spend time on your devices I’ve done some previous digging into natural language SQL queries — there’s a good amount of research around this.', 'But the error rate is always too high for useful production deployment and the systems I’ve looked at never handled ambiguity well.', 'The target user for this is someone who knows nothing about SQL so ambiguity is guaranteed.', 'I worked for Cleargraph, a startup which built a natural language query layer on top of RDBMSes and which we sold to Tableau.', 'We reached the same conclusions as you: that such a system must properly handle ambiguous queries, and users need to explicitly understand the results they’re viewing.', 'Futuristic Sci-Fi and Cyberpunk Graphical User Interface Framework for Web Apps.'], ['This month’s collection of interesting articles that point to important trends is dominated by AI.', 'That’s not surprising; AI has probably been the biggest single category all year.', 'But its dominance over other topics seems to be increasing.', 'That’s partly because there’s more research into why AI fails; partly because we’re beginning to see AI in embedded systems, ranging from giant gas and oil wells to the tiny devices that Pete Warden is working with.'], ['The Covid-19 pandemic has changed how people and businesses spend and operate.', 'Over the coming pages we’ll explore ways in which our current world is already very different from the one we knew just a few months ago, as well as predictions of our “new normal” once the proverbial boat stops rocking.', 'Specifically, we’ll see this through the lens of decision-making: how has Covid-19 changed the way we think?', 'And what does this mean for our purchase patterns and business models?', 'You’re used to a certain level of uncertainty in your life, sure.', 'But the pandemic has quickly turned up the uncertainty on even basic planning.', 'Your dishwasher, piano, or clothes dryer is making an odd sound.', 'Do you proactively call a repair service to check it out?', 'Your ounce of prevention will also cost you two weeks’ wondering whether the repair technician was an asymptomatic carrier.', 'If you hold off, you’re placing a bet that the appliance lasts long enough for treatment to become widely available, because you certainly don’t want it to break down just as infection rates spike.', 'Stresses on a system reveal that some of our constants were really variables in disguise.', '“I can always leave my house.”\\xa0 “I can get to the gym on Friday.”\\xa0 “If I don’t go grocery shopping tonight, I can always do it tomorrow.', 'It’s not like they’ll run out of food.”\\xa0 These weren’t exactly bold statements in January.', 'But by March, many cities’ shelter-in-place orders had turned those periods into question marks.', 'Even as cities are starting to relax those restrictions, there’s the worry that they may suddenly return as the virus continues to spread.', 'As this reality sets in, some of us are even weighing what we call “acceptance purchases”: items which show that we’re in this for the long haul.', 'Your gym isn’t closed, but it’s as good as closed since the city can quickly order it to shut down if local case counts climb again.', 'So maybe it’s time to buy that fancy exercise bike.', 'And ride-hailing services were appealing until using them increased your exposure to the virus.', 'Maybe now you’ll buy that car you sometimes think about?', 'You had considered downsizing your home, but you’ll appreciate the extra space if you’re spending more time indoors.', 'Those sorts of purchases are meant to last you for years, though, which means they’re only wise investments if the pandemic (and its impact on the local economy) continues for a long time.', 'What if we see improved prevention or widespread treatment within a few months?', 'Do we want to try to offload an exercise bike or a car that we no longer need?', 'The longer you hold off on making those decisions, the greater the chances that you’ll make those purchases too late.', 'It’s tough to make a decision when you can’t rely on your near-term world falling within some certain, predictable scope.', 'You try to keep all of your options open at all times so that you can be ready for any possibility.', 'But that’s a lot of extra strain on your brain.', 'And it’s tiring.', 'The pandemic has made a number of businesses less desirable or outright inaccessible.', 'Doing that work yourself reduces the impact around the uncertainty of when they’ll return.', 'It’s also a lot more responsibility for you.', 'Congratulations on running a restaurant, cafe, bar, cinema, gym, school, daycare, office, and storage facility.', 'You get to buy workout equipment, cooking gear, hair care tools, teaching supplies, and anything else needed to backfill services to which you used to outsource.', 'You’re responsible for the decisions on which models of equipment to buy, as well as the upkeep thereof.', 'You suddenly need to know a lot of things about a lot of things, but you don’t have the time to become an expert in any one of them.', 'Welcome to the diseconomies of non-scale: being small and self-sufficient is expensive.', 'Like a factory, hotel event space, or a fast-food kitchen, you find yourself constantly partitioning or re-tooling rooms to compensate for your limited space.', 'The gym becomes the reading room becomes the video meeting space becomes the work area.', '(You and your spouse flip a coin to see who gets the real office and who gets that basement corner.', 'Hint: if you want the nicer space, make sure you’re on more video calls.', 'And pretend that you can’t get Zoom backgrounds to work.)', 'The kitchen table flips between a dining area and a school, three times a day.', 'The bathroom becomes the hair salon every week and quickly switches back again.', 'All of these swaps take time, effort, and mental energy, what economists collectively refer to as switching costs.', 'They add up.', 'Quickly.', 'Nor is this just about the size of the home.', 'It’s a matter of how much you were using it before the pandemic started, the number of spaces present, and the ratio of people to square feet.', 'If you live in a sprawling, suburban house, but every room was already dedicated to some function or someone’s personal space, then you’re just marginally better off than the person occupying a small, urban apartment.', 'This isn’t just about “working from home,” either.', 'That usually means that you have a space set aside in your house to work and to take calls, and you have the house to yourself during working hours.', 'Experienced remote-work professionals will tell you that we’re living a very different scenario.', 'This increasing need to run a standalone, be-everything home means that we are suffering from the curse of generalism: we’re our children’s teachers, our cooks, our housekeepers, our barbers, our IT department, and our fix-it crew.', 'We’re becoming more self-sufficient, but at the expense of having less time to specialize in our main jobs.', 'All of this means that we’re spending a lot of time just getting by, and not much time advancing.', 'In most places, you don’t need to be “connected” to get by day-to-day.', 'Whom you know is less important because, with even a modest income, you can get most of what you need.', 'You mostly care about diversifying your professional network, because that helps you to find a new job, which is what provides you the money that allows you to compensate for not knowing anyone else.', 'When a pandemic triggers stresses in our supply chains, that idea breaks down.', 'Whom you\\xa0 know in your personal sphere suddenly counts a lot more.', 'Instead of money, it’s your social network that gets you through the day.', 'Do you know someone whose job gives them leading indicators on the spread of the virus?', 'Early in the pandemic, friends of medical professionals got some advance warning of what was to come.', 'They could gather information from their professional spheres to let their personal networks know that something nasty was brewing.', 'The same holds for anyone whose business sells protective gear or cleaning supplies.', 'Over casual drinks, they might mention: “It’s weird … we’re getting a lot of new orders, and not from our usual customers.', 'Something’s up.', 'You may want to buy some extras, just in case.”\\xa0 (This, by the by, shows the value of keeping an eye on your company’s data.', 'If you don’t have systems to tell you when your sales numbers are abnormal, you may miss information that you already had in-hand.', 'And in this case, it would have been time-sensitive info.)', 'Communities of shared experience are home to these socially-strong yet professionally-diverse networks.', 'Family and close friendships top the list, with religious and ethnic ties running a close second.', '(People who were part of the same wave of immigration from the same country often forge ties that are as strong as family.)', 'Neighbors and people who share a hobby are also in there, though to a lesser degree.', 'Within these groups there’s always somebody who has a quick tip, someone who “knows a guy,” someone you can pull aside for a quick “Hey can I ask you about …”\\xa0 Maybe your niece works at a big grocery chain, and she can tell you when the shipments of hand sanitizer arrive.', 'In December, this would have been a trivial mention.', 'Today, when goods are scarce, this is timely information and it can make a difference.', 'Personal networks often have the benefit of being geographically dispersed.', 'Your best friend can ship you cleaning supplies, since they are plentiful in his part of the country.', 'Your extended family, which stretches from Paris to Singapore, can tell you how their cities are handling shelter-in-place rules.', 'Chatting with those far-flung aunts and uncles gives you several weeks’ advance notice on how your city’s rules may turn out.', 'That reduces your uncertainty, which makes it easier for you to prepare, which reduces your stress and decision fatigue.', 'Your ability to forge new relationships can compensate for a smaller social network.', 'If you don’t have a relative who works at Target, you can ask someone who works there, so long as you have the skill to spot whom to ask.', 'You have to be able to read people, to see who would be receptive to that question.', 'And you also need to tell whether this would be a simple favor, or something that merits monetary compensation.', 'The value on that information just increased by a wide margin; shouldn’t the price follow?', 'Relationship-building also counts in the B2B setting.', 'Such was the case with grocery chain Trader Joe’s.', 'They’ve managed to avoid shortages during this pandemic, most notably in toilet paper.', 'When other stores seemed to run out, Trader Joe’s always magically had some in stock.', 'That’s because they were able to strike a deal with an unnamed hotel chain to buy supplies that were going unused due to dramatic cuts in travel.', 'Granted, Trader Joe’s very business model—white-labeling manufacturers’ goods—smoothed this road.', 'But their ability to forge that relationship counted just as much as their ability to execute on selling the goods.', 'We can reduce our pandemic-driven stresses by reducing the uncertainty.', 'To do that, we can trace chains of knock-on effects to determine what changes are coming, and plan accordingly.', 'For example:\\xa0 “many restaurants have closed up,” therefore, “there’s less waste from restaurants,” therefore, “there’s less food for rats,” therefore, “expect rats to get more bold.”\\xa0 So be careful when taking out your trash.', '“The pandemic has drastically cut air travel,” therefore “airlines will have less revenue,” therefore “airlines will furlough employees,” therefore “businesses those employees patronized—from in-airport restaurants to hotel shuttle services to their at-home economies—will suffer.” Though we can trace just one chain of effects at a time, multiple paths spin out of every “what next?” and spread out like a spider-crack in a window.', 'They connect down the line to weave a fabric of impacts.', 'Case in point: WSJ’s Scott McCartney points out that the sudden drop in air travel has upset airlines’ ability to set prices, since they take such a data-driven approach.', 'People who work in the ML/AI field will tell you that this is not just an airline problem: a sudden shift will upend any predictive models built on past behaviors, regardless of industry.', 'That will affect other fields’ dynamic pricing, yes, but also fraud detection (your credit card reflects a lot of outlier purchases, times, and locations since February) and demand forecasts (a knock-on effect of our collective outlier purchases).', 'That, in turn, ties to inventory management, which is tied to supply chains, which involve all of the players in the shipping industry, which is tied to fuel consumption and vehicle maintenance… As with any tightly-coupled, complex system, all of these connections work in our favor until they suddenly don’t.', 'Expect pandemic-related changes to cascade, revealing both endogenous and systemic problems that are related in unexpected ways.', 'One problem with tight coupling, Charles Perrow notes in Normal Accidents, is that materials only have one path to take through the system.', 'If a component in the middle breaks, everything backs up so the entire system is as good as broken.', 'You can repair or re-create the old paths (when possible) or create new connections between components.', 'In Covid-speak, that means our long-term solutions fall into “partially restoring and re-thinking the pre-pandemic life” and “creating new ways to handle the day-to-day when there’s a highly infectious disease running around.”\\xa0 There are business opportunities in both camps.', 'We mostly assume the phrases “contactless” and “touch-free” refer to electronic payments.', 'Those are very much in demand right now, but the touch-free space now extends to the wider notion of strangers not interacting in-person, and not handling the same objects at the same time.', 'That opens the door to online learning, telemedicine, tele-anything.', 'If you can provide your service at a distance, you have a lot of new prospects.', 'Entertainment already had a firm footing in the online world thanks to video streaming services.', 'The pandemic, and its dramatically reduced cinema attendance, has provided them even more leverage as some movies will have a shorter time on the big screen before they shift to online video.', '(As a side note, there’s another chain of knock-on effects to explore: since studios have been known to time releases to coincide with certain seasons and to have a better shot at industry awards, how will that change when films head into living rooms that much sooner?)', 'Other groups, like Chicago’s Lyric Opera and New York’s Met Opera, are hosting performances online as their subscribers can no longer attend in-person.', 'Still, it’s become more difficult for performances that rely on people being in the same space.', 'Stand-up comedians from Nimesh Patel to Dave Chappelle have recently been able to pull off outdoor gigs with live-but-socially-distanced audiences.', 'Fire-spinning and belly-dance performer Dawn Xiana Moon, of Raks Inferno and Raks Geek, combined multiple streaming services to simulate all of her performers being “on-stage” at the same time.', 'This required her to leverage her technology background, a skill set that is admittedly rare in the live-act world, and she’d still prefer a single platform that just works.', 'By comparison, TV and movie studios have yet to explain how they will manage to film while keeping cast and crew socially distant.', 'The bottom line is that companies that create tools to improve filming multiple, simultaneous, geographically-dispersed teams will have a lot of customers.', '(And for movies, will we see an increase in animation to fill the gap?)', 'People are also demanding more of their home internet infrastructure to support increased school- and work-related loads.', '(The authors know several people who have shelled out to their ISPs for greater bandwidth.)', 'That also means a greater load on mid-tier services like social media sites, videoconference services, and the aforementioned streaming video platforms.', 'If you sell networking hardware or provide network operations services to those companies, you will have no shortage of work.', 'If the pandemic continues long enough, we expect to see a deeper penetration of home broadband service, especially wireless broadband.', 'This is another touch-free offering, as it permits your provider to establish and troubleshoot internet connectivity issues without sending a technician into your home.', '(As another knock-on effect, this means providers will be able to limit field technicians’ service radius to their towers and datacenters, which should let them cover more territory on the same number of staff.)', 'Traditional, multi-year lease commercial real estate was already experiencing disruption due to coworking spaces.', 'They’ll now both suffer as companies rethink their post-pandemic office needs.', 'Doubly so since some newly-remote workers are taking the opportunity to move out of state.', '(Yet another knock-on effect: without office workers, what will happen to the lunch spots and bars that lined the dense urban-business landscape?)', 'You also have retailers abandoning spaces since there are far too few customers to browse stores.', 'Two types of consumers may pick up that inventory, though.', 'The first, in the short term, Amazon may convert some old mall spaces into distribution centers.', 'Other businesses will undoubtedly find ways to repurpose empty urban office spaces at deep-discount prices.', 'Second, and in the longer term, we’ll accept that our homes are simply not large enough to be our Everything Place.', 'People who choose to remain in dense urban environments will want their apartments to be more like standalone houses, which means having space for in-unit washer/dryer, multiple bathrooms, and multiple rooms to serve as offices.', 'Perhaps cities will divvy up old office buildings into large apartments to meet that need.', 'That’s admittedly more of a stretch, if for no other reason than the time scale involved for the construction effort and the zoning law changes.', 'For now, some percentage of urban residents will simply pack up for the suburbs, or even more rural areas out of state.', 'Let’s face it: if the restaurant scene has dwindled and public transit feels like too much of a coronavirus risk, then urban living has lost a lot of its luster.', 'Wherever we choose to live, we’ll need more support for running our homes.', 'This will include ways to make the most of our limited space, such as smaller-scale workout equipment and compact storage, and increased support for DIY repairs, like video tutorials from manufacturers on how to service their products.', 'This is another stretch, but if the pandemic lasts long enough, manufacturers will modify their products to make them easier to service.', 'That, or cheaper to just throw away and replace when they encounter a problem.', 'The difficulties of schooling don’t end with table space and bandwidth needs.', 'There are also the socioemotional concerns such as college students learning how to live away from home, and how the K-12 set learns to socialize when they don’t interact in person.', 'Not to mention, who will do the teaching?', 'In March, when stay-at-home orders started to hit US cities, many parents suddenly had to balance their full-time jobs with being full-time teachers.', '(Technology consultants Sarah Aslanifar and Bobby Norton jokingly refer to their new roles as, “working from home-school.”)\\xa0 Businesses took a double hit as they had to scramble to find a way for people to work from home, and then those same people spent the next several weeks distracted during the workday.', 'Some parents have since formed social “pods” with neighbors whom they trust to perform compatible pandemic hygiene.', 'Some of those have evolved into educational pods, wherein parents spring for someone to teach their group of kids.', 'An article in MIT Tech Review mentions a price tag of $10,000 per student, per semester.', 'This isn’t accessible to everyone; but for high-earning parents, it’s a simple economic decision: the cost to outsource schooling is smaller than the amount of money they’ll earn when they can perform their day jobs at full capacity.', 'Higher education was already experiencing some disruption—boot camps and certificate programs on one end, and students questioning their post-college job prospects on the other—and the onset of the pandemic has increased the pressure.', 'This goes beyond the last few months of sorting out whether and how to open campuses for autumn 2020.', 'Parents and students alike also question the price tag of a fancy four-year college when students will be attending classes from their kitchen table.', '(One SNL sketch framed the experience as “University of Phoenix Online, with worse tech support.”)\\xa0 For the time being, colleges can busy themselves by shoring up courseware and videoconferencing platforms in order to set autumn 2020 classes in motion.', 'They’ll quickly need to sort out other near-term concerns (shoring up lost profits from empty student housing) as well as their future prospects (demonstrating their value compared to vocational programs, especially if the job market suffers over the long term).', 'If schools can’t sort this out on their own, they’ll likely pay someone to sort it out for them.', 'There’s also a business opportunity in providing a centralized, one-stop SaaS platform such that colleges won’t have to cobble together their own with a mix of one-off tools.', 'One silver lining of working from home is that your job prospects just opened up.', 'Covid-19 has forced a lot of companies to admit that the old “this work can’t be done from home” excuse doesn’t hold up.', 'Some of them are even starting to like it: they see how much money they were burning on an office for people who already knew they’d be more effective working from home.', 'Many of them will scratch that line item from next year’s budget.', 'This means we’ll see more remote hiring in the sectors that can support it.', 'That will establish a clear boundary between the companies that see the benefits (“we’re now able to hire across the country for these hard-to-fill roles”) and those that do not (“we’re only hiring people who live in this city, for when we go back to the office”).', 'Big tech-sector names like Google and Facebook have already announced plans to extend work-from-home support, while Twitter and Atlassian have flat-out said that their crews can work from home indefinitely.', 'In some fields, failing to provide a remote-work option may limit your talent pool.', 'It will be the equivalent of running an office space in the suburbs when most companies, and their prospective employees, exist in the dense urban center and have no desire to commute.', 'Just as we’ll pay for help adapting to the current state of things, we’ll also pay for some semblance of “the old normal.” People generally like meeting up, whether one-on-one for a tea or in larger groups for a party.', 'We’re already using videoconferencing tools to hang out with friends and family, and to attend events.', 'But we’re adapting to the tools more than the other way around.', 'Right now services like Hangouts, Meet, and\\xa0 Zoom are still very much designed for, well, video versions of office conference calls: one person speaks at a time, and you get a “Brady Bunch” grid view of attendees.', 'Expect the incumbent vendors as well as new upstarts to create tools that are better suited for [specific interaction]-over-video, like conferences, classroom teaching, or music lessons.', 'We’re really feeling this in online conferences.', 'While webinar tools fulfill the mission of letting a person deliver a talk to a large number of attendees, they don’t support other aspects of an in-person event.', 'Randomly bumping into people and “hallway track” sessions have forged long-term bonds between conference attendees, far more than the talks themselves.', 'This could serve as a driver for VR, as that will take us away from “attending events from our living room” to “being in our living room, but actually attending events in a dedicated space.”\\xa0 There is a big difference.', 'Another reason people meet up is to play games.', 'Online games are nothing new, and they’ve even gained some mainstream street cred thanks to casual gaming.', 'Expect to see improved coordination, such that you can play with people of your choosing (a feature lacking in a number of iOS Game Center offerings).', 'People playing more video games may also lead to greater participation in esports leagues, and even taking business meetings over a gaming session.', 'In-person interaction is our most risky form of socializing at the moment, but it’s also the one people want the most.', 'Goods and services that help us to (safely) meet face-to-face will not just help us on an emotional level, but they could play a key role in helping the economy get back on its feet.', 'We have masks and face shields, which are good for being in public.', 'What about protective overgarments, reminiscent of 1950s interpretations of outer-space wear?', 'We could wear them to protect our entire body in public transit or airplanes, and then shed them before entering a friend’s home.', 'There’s also the down-to-earth business of designing and installing plastic shields between restaurant tables.', 'Maybe someone will create transparent, oversized cabins that allow you and a few trusted friends to be “on the beach” but still be indoors and away from others.', 'Meeting in person also counts for office space.', 'In a work-from-home world, some teams still prefer the in-person experience.', 'What can we do to make it safer to be in the office, beyond standing several feet apart at all times?', 'An effective but low-tech offering could involve installing protective shields around conference tables (not unlike what we see in some restaurants) or modifying office layouts to discourage crowding.', 'The next step up would increase touch-free actions, such as choosing your elevator floor through a smartphone app.', 'Larger and higher-tech offerings would go deep into the guts of the building to install virus purifiers in building HVAC systems and the accompanying ductwork.', 'Where do we go from here?', 'That depends how long we go without treatment or improved preventative measures.', 'One thing’s for sure: Covid-19 is a driver of change.', 'There is no more “normal” in terms of how we shop for groceries, attend events, or even lay out our homes.', 'It’s up to us to adapt to our present, even as that present continues to change, and that will influence how we decide what to buy and sell.', 'How much we change, as people, depends on how long the pandemic lasts.', 'It’s possible that it will carve deep grooves in our collective social memory, similar to the Great Depression, and its impact will influence how people behave long after the disease is a threat.', 'It also depends on how much we are willing to adapt.', 'That is a function of how soon we’re willing to let go of “normal,” which is really a euphemism for “the past.”\\xa0 Especially since the past is heavily mythologized.'], ['It is primarily written for the survey respondents, and anyone dealing with burn-out and resilience issues either in themselves, family members and employees.', 'If you’re only interested in how to address burn-out skip to section seven.', 'Ethics committees have at least three roles to play.', 'The first is education.', '[…] The second role of ethics committees is policy formation and review.', '[…] The third role of ethics committees is to provide ethical consultation.', '[T]echnological decisions are not only about facts (for example, about what is more efficient), but also about the kind of life we want and the kind of society we strive to build.', 'Simple, yet powerful ORM for modeling and querying data.', '(a) Schema As Code – model any database schema as Go objects.', '(b) Easily Traverse Any Graph – run queries, aggregations and traverse any graph structure easily.', '(c) Statically Typed And Explicit API – 100% statically typed and explicit API using code generation.', '(d) Multi Storage Driver – supports MySQL, PostgreSQL, SQLite and Gremlin.'], ['Individuals typically “hire” communities to accomplish transitions that require human connection.', 'Why do people join communities?', '; Member quality determines community success; Design your community to spark quality interactions; The two levels of group cohesion; Recognizing and retaining key members; Growing your ranks; A Time to Build.'], ['grouped 10 design space dimensions into four major stages of a data science workflow: importing data into notebooks (data sources), editing code and prose (editor style, supported programming languages, versioning, collaboration), running code to generate outputs (cell execution order, liveness [6], execution environment, and cell outputs), and publishing notebook outputs.', 'an open library for computational embroidery with Processing.', 'Neuromorphic chips are packed with artificial neurons and artificial synapses that mimic the activity spikes that occur within the human brain—and they handle all this processing on the chip.', 'This results in smarter, far more energy-efficient computing systems.'], ['So you need to redesign your company’s data infrastructure.', 'Do you buy a solution from a big integration company like IBM, Cloudera, or Amazon?', 'Do you engage many small startups, each focused on one part of the problem?', 'A little of both?', 'We see trends shifting towards focused best-of-breed platforms.', 'That is, products that are laser-focused on one aspect of the data science and machine learning workflows, in contrast to all-in-one platforms that attempt to solve the entire space of data workflows.', 'This article, which examines this shift in more depth, is an opinionated result of countless conversations with data scientists about their needs in modern data science workflows.', 'Today we see two different kinds of offerings in the marketplace: Integrated all-in-one platforms assemble many tools together, and can therefore provide a full solution to common workflows.', 'They’re reliable and steady, but they tend not to be exceptional at any part of that workflow and they tend to move slowly.', 'For this reason, such platforms may be a good choice for companies that don’t have the culture or skills to assemble their own platform.', 'In contrast, best-of-breed products take a more craftsman approach: they do one thing well and move quickly (often they are the ones driving technological change).', 'They usually meet the needs of end users more effectively, are cheaper, and easier to work with.', 'However some assembly is required because they need to be used alongside other products to create full solutions.', 'Best-of-breed products require a DIY spirit that may not be appropriate for slow-moving companies.', 'Which path is best?', 'This is an open question, but we’re putting our money on best-of-breed products.', 'We’ll share why in a moment, but first, we want to look at a historical perspective with what happened to data warehouses and data engineering platforms.', 'Historically, companies bought Oracle, SAS, Teradata or other data all-in-one data warehousing solutions.', 'These were rock solid at what they did–and “what they did” includes offering packages that are valuable to other parts of the company, such as accounting–but it was difficult for customers to adapt to new workloads over time.', 'Next came data engineering platforms like Cloudera, Hortonworks, and MapR, which broke open the Oracle/SAS hegemony with open source tooling.', 'These provided a greater level of flexibility with Hadoop, Hive, and Spark.', 'However, while Cloudera, Hortonworks, and MapR worked well for a set of common data engineering workloads, they didn’t generalize well to workloads that didn’t fit the MapReduce paradigm, including deep learning and new natural language models.', 'As companies moved to cloud, embraced interactive Python, integrated GPUs, or moved to a greater diversity of data science and machine learning use cases, these data engineering platforms weren’t ideal.', 'Data scientists rejected these platforms and went back to working on their laptops where they had full control to play around and experiment with new libraries and hardware.', 'While data engineering platforms provided a great place for companies to start building data assets, their rigidity becomes especially challenging when companies embrace data science and machine learning, both of which are highly dynamic fields with heavy churn that require much more flexibility in order to stay relevant.', 'An all-in-one platform makes it easy to get started, but can become a problem when your data science practice outgrows it.', 'So if data engineering platforms like Cloudera displaced data warehousing platforms like SAS/Oracle, what will displace Cloudera as we move into the data science/machine learning age?', 'The worlds of data science and machine learning move at a much faster pace than data warehousing and much of data engineering.', 'All-in-one platforms are too large and rigid to keep up.', 'Additionally, the benefits of integration are less relevant today with technologies like Kubernetes.', 'Let’s dive into these reasons in more depth.', '“Data science” is an incredibly broad term that encompasses dozens of activities like ETL, machine learning, model management, and user interfaces, each of which have many rapidly evolving choices.', 'Only part of a data scientist’s workflow is typically supported by even the most mature data science platforms.', 'Any attempt to build a one-size-fits-all integrated platform would have to include such a wide range of features, and such a wide range of choices within each feature, that it would be extremely difficult to maintain and keep up to date.', 'What happens when you want to incorporate real-time data feeds?', 'What happens when you want to start analyzing time series data?', 'Yes, the all-in-one platforms will have tools to meet these needs; but will they be the tools you want, or the tools you’d choose if you had the opportunity?', 'Consider user interfaces.', 'Data scientists use many tools like Jupyter notebooks, IDEs, custom dashboards, text editors, and others throughout their day.', 'Platforms offering only “Jupyter notebooks in the cloud” cover only a small fraction of what actual data scientists use in a given day.', 'This leaves data scientists spending half of their time in the platform, half outside the platform, and a new third half migrating between the two environments.', 'Consider also the computational libraries that all-in-one platforms support, and the speed at which they go out of date quickly.', 'Famously, Cloudera ran Spark 1.6 for years after Spark 2.0 was released–even though (and perhaps because) Spark 2.0 was released only 6 months after 1.6.', 'It’s quite hard for a platform to stay on top of all of the rapid changes that are happening today.', 'They’re too broad and numerous to keep up with.', 'While the variety of data science has made all-in-one platforms harder, at the same time advances in infrastructure have made integrating best-of-breed products easier.', 'Cloudera, Hortonworks, and MapR were necessary at the time because Hadoop, Hive, and Spark were notoriously difficult to set up and coordinate.', 'Companies that lacked technical skills needed to buy an integrated solution.', 'But today things are different.', 'Modern data technologies are simpler to set up and configure.', 'Also, technologies like Kubernetes and the cloud help to commoditize configuration and reduce integration pains with many narrowly-scoped products.', 'Kubernetes lowers the barrier to integrating new products, which allows modern companies to assimilate and retire best-of-breed products on an as-needed basis without a painful onboarding process.', 'For example, Kubernetes helps data scientists deploy APIs that serve models (machine learning or otherwise), build machine learning workflow systems, and is an increasingly common substrate for web applications that allows data scientists to integrate OSS technologies, as reported here by Hamel Hussain, Staff Machine Learning Engineer at Github.', 'Kubernetes provides a common framework in which most deployment concerns can be specified programmatically.', 'This puts more control into the hands of library authors, rather than individual integrators.', 'As a result the work of integration is greatly reduced, often just specifying some configuration values and hitting deploy.', 'A good example here is the Zero to JupyterHub guide.', 'Anyone with modest computer skills can deploy JupyterHub on Kubernetes without knowing too much in about an hour.', 'Previously this would have taken a trained professional with pretty deep expertise several days.', 'We believe that companies that adopt a best-of-breed data platform will be more able to adapt to technology shifts that we know are coming.', 'Rather than being tied into a monolithic data science platform on a multi-year time scale, they will be able to adopt, use, and swap out products as their needs change.', 'Best of breed platforms enable companies to evolve and respond to today’s rapidly changing environment.', 'The rise of the data analyst, data scientist, machine learning engineer and all the satellite roles that tie the decision function of organizations to data, along with increasing amounts of automation and machine intelligence, require tooling that meet these end users’ needs.', 'These needs are rapidly evolving and tied to open source tooling that is also evolving rapidly.', 'Our strong opinion (strongly held) is that best-of-breed platforms are better positioned to serve these rapidly evolving needs by building on these OSS tools than all-in-platforms.', 'We look forward to finding out.', '1 Note that we’re discussing data platforms that are built on top of OSS technologies, rather than the OSS technologies themselves.', 'This is not another Dask vs Spark post, but a piece weighing up the utility of two distinct types of modern data platforms.'], ['Sinter uses the user-mode EndpointSecurity API to subscribe to and receive authorization callbacks from the macOS kernel, for a set of security-relevant event types.', 'The current version of Sinter supports allowing/denying process executions; in future versions we intend to support other types of events such as file, socket, and kernel events.', 'We hate being wronged, and it makes us vengeful.', 'On the other hand, we don’t necessarily love being “done right by,” and we don’t have a particular motivation that comes from it.', 'There’s no “positive” version of revenge.', 'These are useful steps but will not stop QAnon from spreading in social media comments or private chat groups or unmoderated forums.', 'It’s not something we can reasonably hope for, and I don’t think there’s any technological solution (e.g.', 'browser extensions) either.', 'The only way to stop people from mistaking speculation from fact is for them to want to stop.'], ['StackOverflow’s 2020 developer survey included a table showing the\\xa0 “most loved, dreaded, and wanted languages.” Loved and wanted languages are, well, sort of boring.', 'The programming languages we dread are a lot more interesting.', 'As Tolstoy said, “All happy families are alike; each unhappy family is unhappy in its own way.” So what are these unhappy, unloved languages, and why do programmers dread using them?', 'Given the chance it’s, well, hard to resist jumping in with some theories, and perhaps even saying something impolitic.', 'Or defending some languages that are disliked for the wrong reasons.', 'More precisely, StackOverflow tabulated the “% of developers who are developing with the language or technology but have not expressed interest in continuing to do so.” That doesn’t sound quite as dire as “dreaded”; “not expressing an interest in working with a language again” is a rather vague indication of dread.', 'There are lots of things I’ve done that I’d rather not do again, including writing troff macros that spawned shell scripts.', 'But we won’t worry about that, will we?', 'The list of least liked languages is similar to the lists of the most widely used languages, as indicated by Redmonk, Tiobe and, for that matter, searches on O’Reilly Learning.', 'That’s no surprise; Bjarne Stroustrup said that “there are only two kinds of languages: the ones people complain about and the ones nobody uses.” And that makes a lot of sense, at least in relation to this survey.', 'If you’ve got millions of users, it’s not hard to get a lot of people to dislike you.', 'So seeing perennials like C alongside relative newcomers like Java on the list of disliked languages isn’t surprising.', 'Kevlin Henney and I thought that the list of least liked languages also reflected the opinions of programmers who were working on large and legacy projects, as opposed to short programs.', 'Dislike of a language may be “guilt by association”: dislike of a large, antiquated codebase with minimal documentation, and an architectural style in which every bug fixed breaks something else.', 'Therefore, it’s not surprising to see languages that used to be widely used but have fallen from popularity on the list.', 'And it’s also easy to fall in love with a quirky language that was perfect for one project, but that you’ll never see again.', '(In my case, that’s Icon.', 'Try it; you might like it.', 'It’s not on anyone’s list.)', 'What’s most surprising is when a language is out of place: when it’s significantly more or less disliked than you expect.', 'That’s what I’d like to think about.', 'So, having disposed of the preliminaries, here are a few observations: There’s lots more that could be said.', 'There’s no surprise that VBA is #1 disliked language.', 'I’ll admit to complete ignorance on Objective C (#2), which I’ve never had any reason to play with.', 'Although I’m a Perl-hater from way back, I’m surprised that Perl is so widely disliked (#3), but some wounds never heal.', 'It will be interesting to see what happens after Perl 7 has been out for a few years.', 'Assembly (#4) is an acquired taste (and isn’t a single language).', 'If you don’t learn to love it, you pretty much have to hate it.', 'And if you don’t love it, you really shouldn’t be using it.', 'You can almost always avoid assembly, but when you need to work directly with the hardware, there’s no alternative.', 'C and C++ (#5 and #8, respectively) give you a lot of rope to hang yourself, but get you as close enough to the hardware for almost any project, without the pain of assembly.', 'Are they fading into the past, or will they be with us forever?', 'My guess is the latter; there are too many projects that demand C’s performance and ubiquity.', 'It’s the foundation for just about everything important in modern computing.', 'Speculating about languages and why they’re liked or disliked is fun.', 'It may or may not be useful.', 'Take it for what it’s worth.'], ['lightweight, high-speed immutable database for systems and applications.', 'Open Source and easy to integrate into any existing application.', 'Japanese startup Donut Robotics […] created a smart mask — a high-tech upgrade to standard face coverings, designed to make communication and social distancing easier.', 'In conjunction with an app, the C-Face Smart mask can transcribe dictation, amplify the wearer’s voice, and translate speech into eight different languages.', 'a Cyber Punk inspired, Text Based MMORPG Browser Game where gameplay interfaces are ‘Stealthily’ mimicking the VSCode interface.', 'Pysa performs iterative rounds of analysis to build summaries to determine which functions return data from a source and which functions have parameters that eventually reach a sink.', 'If Pysa finds that a source eventually connects to a sink, it reports an issue.'], ['I have a system with c servers, each of which can only handle a single concurrent request, and has no internal queuing.', 'The servers sit behind a load balancer, which contains an infinite queue.', 'An unlimited number of clients offer c * 0.8 requests per second to the load balancer on average.', 'In other words, we increase the offered load linearly with c to keep the per-server load constant.', 'Once a request arrives at a server, it takes one second to process, on average.', 'How does the client-observed mean request team vary with c?', 'Crush is an attempt to make a traditional command line shell that is also a modern programming language.', 'It has the features one would expect from a modern programming language like a type system, closures and lexical scoping, but with a syntax geared toward both batch and interactive shell usage.', 'DGL-KE is a high performance, easy-to-use, and scalable package for learning large-scale knowledge graph embeddings.', 'The package is implemented on the top of Deep Graph Library (DGL) and developers can run DGL-KE on CPU machine, GPU machine, as well as clusters'], ['With one of the devices up and running, you can point NyanSat’s antenna to specific coordinates in the sky and listen for the radio frequency transmissions coming from a satellite that’s out there.'], [' McIlroy keeps coming up.', 'He’s the smartest of all of us and the least remembered (or written down)… McIlroy sat there and wrote —on a piece of paper, now, not on a computer— TMG [a proprietary yacc-like program] written in TMG… And then!', 'He now has TMG written in TMG, he decided to give his piece of paper to his piece of paper and write down what came out (the code).', 'Which he did.', 'And then he came over to my editor and he typed in his code, assembled it, and (I won’t say without error, but with so few errors you’d be astonished) he came up with a TMG compiler, on the PDP-7, written in TMG.', 'And it’s the most basic, bare, impressive self-compilation I’ve ever seen in my life.', 'all of the ROS World videos, including all the lightning talks we propose a simple approach called Language Shaped Learning (LSL): if we have access to explanations at training time, we encourage the model to learn representations that are not only helpful for classification, but are predictive of the language explanations.', 'Mondays: Algorithms; Wednesdays: Theory of Computation; Fridays: Theory of Computation; Sundays: Livestream/bonus'], ['I thought July was going to be a dull month, but I’m wrong again.', 'COVID-specific technology seems to be drying up, though there’s a fascinating report about a DIY vaccine.', '(Developed by serious scientists, so don’t try this at home.)', 'There’s a lot of news about AI, and specifically, about the GPT-2 and GPT-3 language models.', 'And a few things that are just fun, like Festo’s Bionic Swifts.'], ['It’s typical that a log will be accessed zero times.', 'Collecting, aggregating, and indexing logs is usually a mistake made by people who aren’t clear on the use case for the logs.', 'an ultra-lightweight Virtual DOM, highly-optimized diff algorithm, and state management library obsessed with minimalism.', 'Update the database; scale vertically; leverage application code; use efficient data types; data normalization and denormalization; precompute data; leverage materialized views; use proper indexes; leverage the execution plan for query optimization; choose correct transaction isolation level; bulk INSERTs and UPDATEs; compress data for storage; make ALTER TABLEs work; manage concurrent connections; add read replicas; disk partitioning; use specialized extensions; sharding; don’t store everything in one table; process data outside the SQL database; be aware of the limitations of managed SQL databases.', 'We conclude that it appears next to impossible to find secret questions that are both secure and memorable.', 'Secret questions continue have some use when combined with other signals, but they should not be used alone and best practice should favor more reliable alternatives.'], ['tl;dr: I made a prototype IDE in which language semantics are specified in datalog, powered by a datalog interpreter written in TypeScript, running the browser.', 'Demo here.', 'Model Cards […] provide a structured framework for reporting on ML model provenance, usage, and ethics-informed evaluation and give a detailed overview of a model’s suggested uses and limitations.', '[…] To streamline the creation of Model Cards for all ML practitioners, we are sharing the Model Card Toolkit (MCT), a collection of tools that support developers in compiling the information that goes into a Model Card and that aid in the creation of interfaces that will be useful for different audiences.', 'Dafny is a language that is designed to make it easy to write correct code.', 'This means correct in the sense of not having any runtime errors, but also correct in actually doing what the programmer intended it to do.', 'To accomplish this, Dafny relies on high-level annotations to reason about and prove correctness of code.', 'The effect of a piece of code can be given abstractly, using a natural, high-level expression of the desired behavior, which is easier and less error prone to write.', 'Dafny then generates a proof that the code matches the annotations (assuming they are correct, of course!).', 'Dafny lifts the burden of writing bug-free code into that of writing bug-free annotations.', 'This is often easier than writing the code, because annotations are shorter and more direct.'], ['Prefer to push fixes upstream instead of working around problems downstream.', 'In October, 1953, I coined the word ‘software.’ In this article, we turn our attention to the process itself: how do you bring a product to market?'], ['   Product Managers are responsible for the successful development, testing, release, and adoption of a product, and for leading the team that implements those milestones.', 'Product managers for AI must satisfy these same responsibilities, tuned for the AI lifecycle.', 'In the first two articles in this series, we suggest that AI Product Managers (AI PMs) are responsible for: If you’re an AI product manager (or about to become one), that’s what you’re signing up for.', 'In this article, we turn our attention to the process itself: how do you bring a product to market?', 'The first step in building an AI solution is identifying the problem you want to solve, which includes defining the metrics that will demonstrate whether you’ve succeeded.', 'It sounds simplistic to state that AI product managers should develop and ship products that improve metrics the business cares about.', 'Though these concepts may be simple to understand, they aren’t as easy in practice.', 'It’s often difficult for businesses without a mature data or machine learning practice to define and agree on metrics.', 'Politics, personalities, and the tradeoff between short-term and long-term outcomes can all contribute to a lack of alignment.', 'Many companies face a problem that’s even worse: no one knows which levers contribute to the metrics that impact business outcomes, or which metrics are important to the company (such as those reported to Wall Street by publicly-traded companies).', 'Rachel Thomas writes about these challenges in “The problem with metrics is a big problem for AI.” There isn’t a simple fix for these problems, but for new companies, investing early in understanding the company’s metrics ecosystem will pay dividends in the future.', 'The worst case scenario is when a business doesn’t have any metrics.', 'In this case, the business probably got caught up in the hype about AI, but hasn’t done any of the preparation.', '(Fair warning: if the business lacks metrics, it probably also lacks discipline about data infrastructure, collection, governance, and much more.)', 'Work with senior management to design and align on appropriate metrics, and make sure that executive leadership agrees and consents to using them before starting your experiments and developing your AI products in earnest.', 'Getting this kind of agreement is much easier said than done, particularly because a company that doesn’t have metrics may never have thought seriously about what makes their business successful.', 'It may require intense negotiation between different divisions, each of which has its own procedures and its own political interests.', 'As Jez Humble said in a Velocity Conference training session, “Metrics should be painful: metrics should be able to make you change what you’re doing.” Don’t expect agreement to come simply.', 'Lack of clarity about metrics is technical debt worth paying down.', 'Without clarity in metrics, it’s impossible to do meaningful experimentation.', 'A product manager needs to think about ethics–and encourage the product team to think about ethics–throughout the whole product development process, but it’s particularly important when you’re defining the problem.', 'Is it a problem that should be solved?', 'How can the solution be abused?', 'Those are questions that every product team needs to think about.', 'There’s a substantial literature about ethics, data, and AI, so rather than repeat that discussion, we’ll leave you with a few resources.', 'Ethics and Data Science is a short book that helps developers think through data problems, and includes a checklist that team members should revisit throughout the process.', 'The Markkula Institute at the University of Santa Clara has an excellent list of resources, including an app to aid ethical decision-making.', 'The Ethical OS also provides excellent tools for thinking through the impact of technologies.', 'And finally–build a team that includes people of different backgrounds, and who will be affected by your products in different ways.', 'It’s surprising (and saddening) how many ethical problems could have been avoided if more people thought about how the products would be used.', 'AI is a powerful tool: use it for good.', 'Once you know which metrics are most important, and which levers affect them, you need to run experiments to be sure that the AI products you want to develop actually map to those business metrics.', 'Experiments allow AI PMs not only to test assumptions about the relevance and functionality of AI Products, but also to understand the effect (if any) of AI products on the business.', 'AI PMs must ensure that experimentation occurs during three phases of the product lifecycle: AI product managers need to understand how sensitive their project is to error.', 'This isn’t always simple, since it doesn’t just take into account technical risk; it also has to account for social risk and reputational damage.', 'As we mentioned in the first article of this series, an AI application for product recommendations can make a lot of mistakes before anyone notices (ignoring concerns about bias); this has business impact, of course, but doesn’t cause life-threatening harm.', 'On the other hand, an autonomous vehicle really can’t afford to make any mistakes; even if the autonomous vehicle is safer than a human driver, you (and your company) will take the blame for any accidents.', 'AI PMs have to make tough choices when deciding where to apply limited resources.', 'It’s the old “choose two” rule, where the parameters are Speed, Quality, and Features.', 'For example, for a mobile phone app that uses object detection to identify pets, speed is a requirement.', 'A product manager may sacrifice either a more diverse set of animals, or the accuracy of detection algorithms.', 'These decisions have dramatic implications on project length, resources, and goals.', 'Similarly, AI product managers often need to choose whether to prioritize the scale and impact of a product over the difficulty of product development.', 'Years ago a health and fitness technology company realized that its content moderators, used to manually detect and remediate offensive content on its platform, were experiencing extreme fatigue and very poor mental health outcomes.', 'Even beyond the humane considerations, moderator burnout was a serious product issue, in that the company’s platform was rapidly growing, thus exposing the average user to more potentially offensive or illegal content.', 'The difficulty of content moderation work was exacerbated by its repetitive nature, making it a candidate for automation via AI.', 'However, the difficulty of developing a robust content moderation system at the time was significant, and would have required years of development time and research.', 'Ultimately, the company decided to simply drop the most social components of the platform, a decision which limited overall growth.', 'This tradeoff between impact and development difficulty is particularly relevant for products based on deep learning: breakthroughs often lead to unique, defensible, and highly lucrative products, but investing in products with a high chance of failure is an obvious risk.', 'Products based on deep learning can be difficult (or even impossible) to develop; it’s a classic “high return versus high risk” situation, in which it is inherently difficult to calculate return on investment.', 'The final major tradeoff that AI product managers must evaluate is how much time to spend during the R&D and design phases.', 'With no restrictrictions on release dates, PMs and engineers alike would choose to spend as much time as necessary to nail the product goals.', 'But in the real world, products need to ship, and there’s rarely enough time to do the research necessary to ship the best possible product.', 'Therefore, product managers must make a judgment call about when to ship, and that call is usually based on incomplete experimental results.', 'It’s a balancing act, and admittedly, one that can be very tricky: achieving the product’s goals versus getting the product out there.', 'As with traditional software, the best way to achieve your goals is to put something out there and iterate.', 'This is particularly true for AI products.', 'Microsoft, LinkedIn, and Airbnb have been especially candid about their journeys towards building an experiment-driven culture and the technology required to support it.', 'Some of the best lessons are captured in Ron Kohavi, Diane Tang, and Ya Xu’s book: Trustworthy Online Controlled Experiments : A Practical Guide to A/B Testing.', 'The development phases for an AI project map nearly 1:1 to the AI Product Pipeline we described in the second article of this series.', 'AI projects require a “feedback loop” in both the product development process and the AI products themselves.', 'Because AI products are inherently research-based, experimentation and iterative development are necessary.', 'Unlike traditional software development, in which the inputs and results are often deterministic, the AI development cycle is probabilistic.', 'This requires several important modifications to how projects are set up and executed, regardless of the project management framework.', 'Product managers must ensure that AI projects gather qualitative information about customer behavior.', 'Because it might not be intuitive, it’s important to note that traditional data measurement tools are more effective at measuring magnitude than sentiment.', 'For most AI products, the product manager will be less interested in the click-through rate (CTR) and other quantitative metrics than they are in the utility of the AI product to the user.', 'Therefore, traditional product research teams must engage with the AI team to ensure that the correct intuition is applied to AI product development, as AI practitioners are likely to lack the appropriate skills and experience.', 'CTRs are easy to measure, but if you build a system designed to optimize these kinds of metrics, you might find that the system sacrifices actual usefulness and user satisfaction.', 'In this case, no matter how well the AI product contributes to such metrics, it’s output won’t ultimately serve the goals of the company.', 'It’s easy to focus on the wrong metric if you haven’t done the proper research.', 'One mid-sized digital media company we interviewed reported that their Marketing, Advertising, Strategy, and Product teams once wanted to build an AI-driven user traffic forecast tool.', 'The Marketing team built the first model, but because it was from marketing, the model optimized for CTR and lead conversion.', 'The Advertising team was more interested in cost per lead (CPL) and lifetime value (LTV), while the Strategy team was aligned to corporate metrics (revenue impact and total active users).', 'As a result, many of the tool’s users were dissatisfied, even though the AI functioned perfectly.', 'The ultimate result was the development of multiple models that optimize for different metrics, and the redesign of the tool so that it could present those outputs clearly and intuitively to different kinds of users.', 'Internally, AI PMs must engage stakeholders to ensure alignment with the most important decision-makers and top-line business metrics.', 'Put simply, no AI product will be successful if it never launches, and no AI product will launch unless the project is sponsored, funded, and connected to important business objectives.', 'This phase of an AI project is laborious and time consuming, but completing it is one of the strongest indicators of future success.', 'A product needs to balance the investment of resources against the risks of moving forward without a full understanding of the data landscape.', 'Acquiring data is often difficult, especially in regulated industries.', 'Once relevant data has been obtained, understanding what is valuable and what is simply noise requires statistical and scientific rigor.', 'AI product managers probably won’t do the research themselves; their role is to guide data scientists, analysts, and domain experts towards a product-centric evaluation of the data, and to inform meaningful experiment design.', 'The goal is to have a measurable signal for what data exists, solid insights into that data’s relevance, and a clear vision of where to concentrate efforts in designing features.', 'Data wrangling and feature engineering is the most difficult and important phase of every AI project.', 'It’s generally accepted that, during a typical product development cycle, 80% of a data scientist’s time is spent in feature engineering.', 'Trends and tools in AutoML and Deep Learning have certainly reduced the time, skills, and effort required to build a prototype, if not an actual product.', 'Nonetheless, building a superior feature pipeline or model architecture will always be worthwhile.', 'AI product managers should make sure project plans account for the time, effort, and people needed.', 'The modeling phase of an AI project is frustrating and difficult to predict.', 'The process is inherently iterative, and some AI projects fail (for good reason) at this point.', 'It’s easy to understand what makes this step difficult: there is rarely a sense of steady progress towards a goal.', 'You experiment until something works; that might happen on the first day, or the hundredth day.', 'An AI product manager must motivate the team members and stakeholders when there is no tangible “product” to show for everyone’s labor and investment.', 'One strategy for maintaining motivation is to push for short-term bursts to beat a performance baseline.', 'Another would be to start multiple threads (possibly even multiple projects), so that some will be able to demonstrate progress.', 'Unlike traditional software engineering projects, AI product managers must be heavily involved in the build process.', 'Engineering managers are usually responsible for making sure all the components of a software product are properly compiled into binaries, and for organizing build scripts meticulously by version to ensure reproducibility.', 'Many mature DevOps processes and tools, honed over years of successful software product releases, make these processes more manageable, but they were developed for traditional software products.', 'The equivalent tools and processes simply do not exist in the ML/AI ecosystem; when they do, they are rarely mature enough to use at scale.', 'As a result, AI PMs must take a high-touch, customized approach to guiding AI products through production, deployment, and release.', 'Like any other production software system, after an AI product is live it must be monitored.', 'However, for an AI product, both model performance and application performance must be monitored simultaneously.', 'Alerts that are triggered when the AI product performs out of specification may need to be routed differently; the in-place SRE team may not be able to diagnose technical issues with the model or data pipelines without support from the AI team.', 'Though it’s difficult to create the “perfect” project plan for monitoring, it’s important for AI PMs to ensure that project resources (especially engineering talent) aren’t immediately released when the product has been deployed.', 'Unlike a traditional software product, it’s hard to define when an AI product has been deployed successfully.', 'The development process is iterative, and it’s not over after the product has been deployed–though, post-deployment, the stakes are higher, and your options for dealing with issues are more limited.', 'Therefore, members of the development team must remain on the maintenance team to ensure that there is proper instrumentation for logging and monitoring the product’s health, and to ensure that there are resources available to deal with the inevitable problems that show up after deployment.', '(We call this “debugging” to distinguish it from the evaluation and testing that takes place during product development.', 'The final article in this series will be devoted to debugging.)', 'Among operations engineers, the idea of observability is gradually replacing monitoring.', 'Monitoring requires you to predict the metrics you need to watch in advance.', 'That ability is certainly important for AI products–we’ve talked all along about the importance of metrics.', 'Observability is critically different.', 'Observability is the ability to get the information you need to understand why the system behaved the way it does; it’s less about measuring known quantities, and more about the ability to diagnose “unknown unknowns.” We’ve spent a lot of time talking about planning.', 'Now let’s shift gears and discuss what’s needed to build a product.', 'After all, that’s the point.', 'AI Product Interface Design The AI product manager must be a member of the design team from the start, ensuring that the product provides the desired outcomes.', 'It’s important to account for the ways a product will be used.', 'In the best AI products, users can’t tell how the underlying models impact their experience.', 'They neither know or care that there is AI in the application.', 'Take Stitch Fix, which uses a multitude of algorithmic approaches to provide customized style recommendations.', 'When a Stitch Fix user interacts with its AI products, they interface with the prediction and recommendation engines.', 'The information they interact with during that experience is an AI product–but they neither know, nor care, that AI is behind everything they see.', 'If the algorithm makes a perfect prediction, but the user can’t imagine wearing the items they’re shown, the product is still a failure.', 'In reality, ML models are far from perfect, so it is even more imperative to nail the user experience.', 'To do so, product managers must ensure that design gets an equal seat at the table with engineering.', 'Designers are more attuned to qualitative research about user behavior.', 'What signals show user satisfaction?', 'How do you build products that delight users?', 'Apple’s sense of design, making things that “just work,” pioneered through the iPod, iPhone, and iPad products is the foundation of their business.', 'That’s what you need, and you need that input from the beginning.', 'Interface design isn’t an after-the-fact add-on.', 'Picking the Right Scope “Creeping featurism” is a problem with any software product, but it’s a particularly dangerous problem for AI.', 'Focus your product development effort on problems that are relevant to the business and consumer.', 'A successful AI product measurably (and positively) impacts metrics that matter to the business.', 'Therefore, limit the scope of an AI product to features that can create this impact.', 'To do so, start with a well-framed hypothesis that, upon validation through experimentation, will produce meaningful outcomes.', 'Doing this effectively means that AI PMs must learn to translate business intuitions into product development tools and processes.', 'For example, if the business seeks to understand more about its customer base in order to maximize lifetime value for a subscription product, an AI PM would do well to understand the tools available for customer and product-mix segmentation, recommendation engines, and time-series forecasting.', 'Then, when it comes to developing the AI product roadmap, the AI PM can focus engineering and AI teams on the right experiments, the correct outcomes,andthe smoothest path to production.', 'It is tempting to over-value the performance gains achieved through the use of more complex modeling techniques, leading to the dreaded “black box” problem: models for which it’s difficult (if not impossible) to understand the relationship between the input and the output.', 'Black box models are seldom useful in business environments for several reasons.', 'First, being able to explain how the model works is often a prerequisite for executive approval.', 'Ethical and regulatory considerations often require a detailed understanding of the data, derived features, pipelines and scoring mechanisms involved in the AI system.', 'Solving problems with the simplest model possible is always preferable, and not just because it leads to models that are interpretable.', 'In addition, simpler modeling approaches are more likely to be supported by a wide variety of frameworks, data platforms, and languages, increasing interoperability and decreasing technical debt.', 'Another scoping consideration concerns the processing engine that will power the product.', 'Problems that are real-time (or near real-time) in nature can only be addressed by highly performant stream processing architectures.', 'Examples of this include product recommendations in e-commerce systems or AI-enabled messaging.', 'Stream processing requires significant engineering effort, and it’s important to account for that effort at the beginning of development.', 'Some machine learning approaches (and many software engineering practices) are simply not appropriate for near-real time applications.', 'If the problem at hand is more flexible and less interactive (such as offline churn probability prediction), batch processing is probably a good approach, and is typically easier to integrate with the average data stack.', 'Prototypes and Data Product MVPs Entrepreneurial product managers are often associated with the phrase “Move Fast and Break Things.”\\xa0 AI product mangers live and die by “Experiment Fast So You Don’t Break Things Later.”\\xa0 Take any social media company that sells advertisements.', 'The timing, quantity, and type of ads displayed to segments of a company’s user population are overwhelmingly determined by algorithms.', 'Customers contract with the social media company for a certain fixed budget, expecting to achieve certain audience exposure thresholds that can be measured by relevant business metrics.', 'The budget that is actually spent successfully is referred to as fulfillment, and is directly related to the revenue that each customer generates.', 'Any change to the underlying models or data ecosystem, such as how certain demographic features are weighted, can have a dramatic impact on the social media company’s revenue.', 'Experimenting with new models is essential–but so is yanking an underperforming model out of production.', 'This is only one example of why rapid prototyping is important for teams building AI products.', 'AI PMs must create an environment in which continuous experimentation and failure are allowed (even celebrated), along with supporting the processes and tools that enable experimentation and learning through failure.', 'In a previous section, we introduced the importance of user research and interface design.', 'Qualitative data collection tools (such as SurveyMonkey, Qualtrics, and Google Forms) should be joined with interface prototyping tools (such as Invision and Balsamiq), and with data prototyping tools (such as Jupyter Notebooks) to form an ecosystem for product development and testing.', 'Once such an environment exists, it’s important for the product manager to codify what constitutes a “minimum viable” AI product (MVP).', 'This product should be robust enough to be used for user research and quantitative (model evaluation) experimentation, but simple enough that it can be quickly discarded or adjusted in favor of new iterations.', 'And, while the word “minimum” is important, don’t forget “viable.” An MVP needs to be a product that can stand on its own, something that customers will want and use.', 'If the product isn’t “viable” (i.e., if a user wouldn’t want it) you won’t be able to conduct good user research.', 'Again, it’s important to listen to data scientists, data engineers, software developers, and design team members when deciding on the MVP.', 'Data Quality and Standardization In most organizations, Data Quality is either an engineering or IT problem; it is rarely addressed by the product team until it blocks a downstream process or project.', 'This relationship is impossible for teams developing AI products.', '“Garbage in, garbage out” holds true for AI, so good AI PMs must concern themselves with data health.', 'There are many excellent resources on data quality and data governance.', 'The specifics are outside the scope of this article, but here are some core principles that should be incorporated into any product manager’s toolkit: Augmenting AI Product Management with Technical Leadership There is no intuitive way to predict what will work best in AI product development.', 'AI PMs can build amazing things, but this often comes largely from the right frameworks rather than the correct tactical actions.', 'Many new tech capabilities have the potential to enable software engineering using ML/AI techniques more quickly and accurately.', 'AI PMs will need to leverage new and emerging AI techniques (image upscaling, synthetic text generation using adversarial networks, reinforcement learning, and more), and partner with expert technologists to put these tools to use.', 'It’s unlikely that every AI PM will have world-class technical intuition in addition to excellent product sense, UI/X experience, customer knowledge, leadership skills, and so on.', 'But don’t let that breed pessimism.', 'Since one person can’t be an expert at everything, AI PMs need to form a partnership with a technology leader (e.g., a Technical Lead or Lead Scientist) who knows the state of the art and is familiar with current research, and trust that tech leader’s educated intuition.', 'Finding this critical technical partner can be difficult, especially in today’s competitive talent market.', 'However, all is not lost:\\xa0 there are many excellent technical product leaders out there masquerading as competent engineering managers.', 'Product manager Matt Brandwein suggests observing what potential tech leads do in their idle time, and taking note of which domains they find attractive.', 'Someone’s current role often doesn’t reveal where their interests and talent lie.', 'Most importantly, the AI PM should look for a tech lead who can mitigate their own weaknesses.', 'For example, if the AI PM is a visionary, picking a technical lead with operational experience is a good idea.', 'When a product is ready to ship, the PM will work with user research and engineering teams to develop a release plan that collects both qualitative and quantitative user feedback.', 'The bulk of this data will be concentrated on user interaction with the user interface and front end of the product.', 'AI PMs must also plan to collect data about the “hidden” functionality of the AI product, the part no user ever sees directly: model performance.', 'We’ve discussed the need for proper instrumentation at both the model and business levels to gauge the product’s effectiveness; this is where all of that planning and hard work pays off!', 'On the model side, performance metrics that were validated during development (predictive power, model fit, precision) must be constantly re-evaluated as the model is exposed to more and more unseen data.', 'A/B testing, which is frequently used in web-based software development, is useful for evaluating model performance in production.', 'Most companies already have a framework for A/B testing in their release process, but some may need to invest in testing infrastructure.', 'Such investments are well worth it.', 'It’s inevitable that the model will require adjustments over time, so AI PMs must ensure that whoever is responsible for the product post-launch has access to the development team in order to investigate and resolve issues.', 'Here, A/B testing has another benefit: the ability to run champion/challenger model evaluations.', 'This framework allows for a deployed model to run uninterrupted, while a second model is evaluated against a sample of the total population.', 'If the second model outperforms the original, it can simply be swapped out-often without any downtime!', 'Overall, AI PMs should remain closely involved in the early release lifecycle for AI products, taking responsibility for coordinating and managing A/B tests and user data collection, and resolving issues with the product’s functionality.', 'In this article, we’ve focused primarily on the AI product development process, and mapping the AI product manager’s responsibilities to each stage of that process.', 'As with many other digital product development cycles, AI PMs must first ensure that the problem to be solved is both a problem that ML/AI can solve and a problem that is vital to the business.', 'Once this criteria has been met, the AI PM must consider whether the product should be developed, considering the myriad of technical and ethical considerations at play when developing and releasing a production AI system.', 'We propose the AI Product Development Process as a blueprint for AI PMs of all industries, who may develop myriad different AI products.', 'Though this process is by no means exhaustive, it emphasizes the kind of critical thinking and cross-departmental collaboration necessary to success at each stage of the AI product lifecycle.', 'However, regardless of the process you use, experimentation is the key to success.', 'We’ve said that repeatedly, and we aren’t tired: the more experiments you can do, the more likely you are to build a product that works (i.e., positively impacts metrics the company cares about).', 'And don’t forget qualitative metrics that help you understand user behavior!', 'Once an AI system is released and in use, however, the AI PM has a somewhat unique role in product maintenance.', 'Unlike PMs for many other software products, AI PMs must ensure that robust testing frameworks are built and utilized not only during the development process, but also in post-production.', 'Our next article focuses on perhaps the most important phase of the AI product lifecycle: maintenance and debugging.'], ['A recent article in The Verge discussed PULSE, an algorithm for “upsampling” digital images.', 'PULSE, when applied to a low-resolution image of Barack Obama, recreated a White man’s face; applied to Alexandria Ocasio-Cortez, it built a White woman’s face.', 'It had similar problems with other images of Black and Hispanic people, frequently giving them White skin and facial features.', 'PULSE could be used for applications like upsampling video for 8K ultra high-definition, but I’m less interested in the algorithm and its applications than in the discussion about ethics that it provoked.', 'Is this just a problem with training data, as Yann LeCun said on Twitter?', 'Or is it a sign of larger systemic issues about bias and power, as Timnit Gebru argued?', 'The claim that this is only a problem with the data is tempting, but it is important to step back and see the bigger issues: nothing is “just” a problem with data.', 'That shift to a wider perspective is badly needed.', 'There’s no question that the training data was a problem.', 'If the algorithm were trained using a set of photos dominated by Black people, it would no doubt turn White faces into Black ones.', 'With the right training set and training process, we could presumably minimize errors.', 'When looked at this way, it’s largely a problem of mathematics and statistics.', 'That’s the position that Timnit Gebru rejects, because it obscures the bigger issues hiding behind the training set.', 'As organizations like Data For Black Lives, Black in AI, the Algorithmic Justice League, and others have been pointing out, it’s never just an issue of statistics.', 'It’s an issue of harms and of power.', 'Who stands to gain?', 'Who stands to lose?', 'That’s the point we really need to consider, particularly when we’re asking AI to create “information” where nothing existed before.', 'Who controls the erasure, or the creation, of color?', 'What are the assumptions that lie behind it?', 'I do not believe there are many AI researchers giggling about turning Black people into Whites (though there are no doubt some).', 'Nor do I believe there’s some kind of racist demon lurking in the mathematics implemented by neural networks.', 'But errors like this nevertheless happen; they happen all too frequently; the results are often harmful; and none of us are surprised that the transition was Black->White rather than the other way around.', 'We were not surprised when we found that products like COMPAS recommended tougher criminal sentences for Black people than for Whites; nor were we surprised when Timnit Gebru and Joy Buolamwini showed that facial recognition is much less accurate for Black people than White people, and particularly inaccurate for Black women.', 'So, how do we think about the problem of power and race in AI?', 'Timnit Gebru is right; saying that the problem is in the training data ignores the real problem.', 'As does being saddened and making vague promises about doing better in the future.', 'If we aren’t surprised, why?', 'What do we have to learn, and how do we put that learning into practice?', 'We can start by considering what “biased training data” means.', 'One of my favorite collections of essays about data is “Raw Data” is an Oxymoron.', 'There is no such thing as “raw data,” and hence, no pure, unadulterated, unbiased data.', 'Data is always historical and, as such, is the repository of historical bias.', 'Data doesn’t just grow, like trees; data is collected, and the process of data collection often has its own agenda.', 'Therefore, there are different ways of understanding data, different ways of telling stories about data–some of which account for its origin and relation to history, and some of which don’t.', 'Take, for example, housing data.', 'That data will show that, in most places in the US, Black people live in separate neighborhoods from White people.', 'But there are a number of stories we can tell about that data.', 'Here are two very different stories: There are many variations on those stories, but those two are enough.', 'Neither is entirely wrong—though the first story erases an important fact, that White people have generally had the power to prevent Black people from moving into their neighborhoods.', 'The second story doesn’t treat that data as an intransigent given; it critiques the data, asks that data how and why it came to be.', 'As I’ve argued, AI is capable of revealing our biases, and showing us where they are hidden.', 'It gives us an opportunity to learn about and critique our own institutions.', 'If you don’t look critically at the data, its origins, and its stories (something that’s not a part of most computer science curricula), you’re likely to institutionalize the bias embedded in the data behind a wall of mathwashing.', 'There are plenty of situations in which that critique is needed.', 'Here’s one: researchers looking at ride data from Chicago’s public data portal found that the dynamic pricing algorithms used by ride-hailing services (such as Uber and Lyft) charged more for rides to and from low-income, nonwhite areas.', 'This effect might not have been discovered without machine learning.', 'It means that it’s time to audit the services themselves, and find out exactly why their algorithms behave this way.', 'And it’s an opportunity to learn what stories the data is telling us.', 'The real issue is which of those stories we choose to tell.', 'I use the word “we” because the data doesn’t tell a story on its own, any more than a pixelated image of President Obama becomes a White man on its own.', 'Someone chooses what story to tell; someone releases the software; and that someone is a person, not an algorithm.', 'So if we really want to get to the bottom of the upsampling problem with PULSE, we need to be looking at people in addition to training data.', 'If PULSE needed more images of Black people in its training set, why didn’t it have them?', 'And why are we not surprised that these issues show up all the time, in applications ranging from COMPAS to the Google app that tagged Black people as gorillas?', 'That’s really a question about the teams of people who are creating and testing this software.', 'They are predominantly White and male.', 'I admit that if I wrote a program that upsampled images, it might not occur to me to test a Black person’s face.', 'Or to test whether jail sentences for White and Black people are comparable.', 'Or to test whether a real estate application will recommend that Black people consider buying homes in largely White neighborhoods.', 'These not-so-microaggressions are the substance from which greater abuses of power are made.', 'And we’re more likely to discover those microaggressions in time to stop them if the teams developing the software include people with Black and Brown faces, as well as White ones.', 'The problem isn’t limited to building teams that realize we need different training data, or that understand the need for testing against different kinds of bias.', 'We also need teams that can think about what applications should and shouldn’t be built.', 'Machine learning is complicit in many power structures.', 'Andrew Ng’s newsletter, The Batch, gives payday lending as an example.', 'An application might compute the optimal interest rate to charge any customer, and that app might easily be “fair” by some mathematical standard–although even that is problematic.', 'But the industry itself exists to take advantage of vulnerable, low-income people.', 'In this situation, it is impossible for an algorithm—even a “fair” one—to be fair.', 'Likewise, given the current power structures, along with the possibility for abuse, it is very difficult to imagine a face recognition application, no matter how accurate, that isn’t subject to abuse.', 'Fairness isn’t a mathematical construct that can be embodied by an algorithm; it has everything to do with the systems in which the algorithm is embedded.', 'The algorithms used to identify faces can also be used to identify bird species, detect diseased tomatoes on a farm, and the like.', 'The ethical problem isn’t the algorithm, it’s the context and the power structures, and those are issues that most software teams aren’t used to thinking about.', 'There’s an even better example close at hand: PULSE itself.', 'Obscuring faces by replacing them with low-resolution, pixelated images is a classic way of protecting the identity of the person in the photograph.', 'It’s something people do to protect themselves–a particularly important issue in these days of incognito armies.', 'Software like PULSE, even (especially) if it is trained correctly, undoes individuals’ efforts to protect their own privacy.', 'It tips the power relationship even further in the direction of the empowered.', 'And an application like Stanford’s #BlackLivesMatter PrivacyBot tips the balance back the other way.', 'There are many ways to address these issues of bias, fairness, and power, but they all start with building inclusive teams, and with taking a step back to look at the bigger issues involved.', 'You are more likely to detect bias if there are people on the team who have been victims of bias.', 'You are more likely to think about the abuse of power if the team includes people who have been abused by power.', 'And, as I’ve argued elsewhere, the job of “programming” is becoming less about writing code, and more about understanding the nature of the problem to be solved.', 'In the future, machines will write a lot of code for us.', 'Our task will be deciding what that software should do—not putting our heads down and grinding out lines of code.', 'And that task isn’t going to go well if our teams are monochromatic.'], ['‘Tech’, of course, has all of this complexity, but we’re having to work this out a lot more quickly.', 'It took 75 years for seatbelts to become compulsory, but tech has gone from interesting to crucial only in the last five to ten years.', 'That speed means we have to form opinions about things we didn’t grow up with and don’t always understand quite so well as, say, supermarkets.', 'uses extreme ultraviolet (EUV) light at a wavelength of 13.5 nm to make silicon features down to a few nanometers in size.', 'While quantum computing has garnered most of the recent headlines, quantum networking—especially with its promise of secure communication—actually is capturing the interest of a growing community across science, industry, and national security.', 'Today, many people recognize that building and scaling quantum-protected and enhanced communication networks are among the most important technological frontiers of the 21st century.', 'Although a general-purpose quantum computer still is many years away, the research community perceives a quantum Internet may be closer to realization.'], ['Taiwan and Audrey Tang occupy a unique spot in a world, where the ascendance of the internet and digital technology is marked by the twin dystopias of “post-truth” information chaos in the United States and China’s totalitarian, technologically mediated surveillance-and-censorship regime.', 'With Audrey Tang as the symbolic figurehead, the island nation is making the radical argument that digital tools can be effectively used to build stronger, more open, more accountable democracies.', 'Whether the challenge is fighting disinformation campaigns orchestrated by hostile powers or the existential threat of a virus run amok or simply figuring out how to regulate Uber, Taiwan is demonstrating the best ways technology can be used to marry the energy and talents of civil society with the administrative powers of government bureaucracy.', 'we found that the employees who averaged the most weekly one-on-one time with their managers experienced the smallest increase in working hours  if you have a fleet of healthy services written in a single application stack, then it’s a good idea to think twice before introducing a service mesh.', 'By simply introducing or evolving a shared RPC library, you’ll get the exact same benefits and avoid dealing with the downsides of maintaining service meshes.'], ['why aren’t we — ostensibly the people writing software — doing more with AI in our day-to-day?', 'Why are things like TabNine and Kite so often seen as curiosities instead of game-changers?', 'If you take seriously the idea that ai will fundamentally change the nature of many occupations in the coming decade, what reason do you have to believe that you’ll be immune from that because you work in software?', 'Looking at the code you’ve been paid to write over the past few years, how much of that can you honestly say is truly novel?', 'The truth is, I’ve come around to thinking that programming isn’t the most important thing for programmers to pay attention to right now.', 'represent our Domain Model declaratively, as an in-program data structure (a ‘meta-database’); derive the ‘machine’ behaviour generically from this representation.', 'We have developed a new methodology that retains the ease-of-use, familiarity, and (some of) the free-form nature of informal methods, while benefiting from the rigor, structure, and potential for automation characteristic of formal methods.', 'Our approach aims to foster thoughtful and timely analysis through the introduction of structure, and collaboration through access to the corporate memory of current and past analytic results.', 'This article covers the outcomes of research performed in 2019 on how engineers at Google debug production issues, including the types of tools, high-level strategies, and low-level tasks that engineers use in varying combinations to debug effectively.', 'It examines the research approach used to capture data, summarizing the common engineering journeys for production investigations and sharing examples of how experts debug complex distributed systems.', 'Finally, the article extends the Google specifics of this research to provide some practical strategies that you can apply in your organization.'], ['Our diff solution involves embedding each line into a low dimensional vector and (optionally “fine-tuning” or updating the embedding model at the same time), assigning it to a cluster, and identifying lines in different clusters as “different”.', 'Locality sensitive hashing is a probabilistic algorithm that permits constant time cluster assignment and near-constant time nearest neighbors search.', 'Good leaders can walk into a situation where people have lost track of their goals and get everyone aligned on a clear path forward.', 'They remove unimportant details, distill complex situations to their essence, and get the right decision-maker to make a call – even if it’s not them.', 'They’re able to not only stop bad plans before it’s too late, but get them moving again in the right direction.'], ['Apple was responsible for more edits in 2019 than Mapbox accounted for in its entire corporate history.', 'See also the 2020: Curious Cases of Corporations in OpenStreetMap talk from State of the Map.', '(via Simon Willison)     Azerbaijan, frustrated at a peace process that it felt delivered nothing, used its Caspian Sea oil wealth to buy arms, including a fleet of Turkish Bayraktar TB2 drones and Israeli kamikaze drones (also called loitering munitions, designed to hover in an area before diving on a target).', '[…] Azerbaijan used surveillance drones to spot targets and sent armed drones or kamikaze drones to destroy them, analysts said.', '[…] Their tally, which logs confirmed losses with photographs or videos, listed Armenian losses at 185 T-72 tanks; 90 armored fighting vehicles; 182 artillery pieces; 73 multiple rocket launchers; 26 surface-to-air missile systems, including a Tor system and five S-300s; 14 radars or jammers; one SU-25 war plane; four drones and 451 military vehicles.', 'an efficient, single-machine system for performing data mining tasks on large graphs.', 'Some graph mining applications include: Finding frequent subgraphs; Generating the motif/graphlet distribution; Finding all occurrences of a subgraph.', 'Peregrine is highly programmable, so you can easily develop your own graph mining applications using its novel, declarative, graph-pattern-centric API.', 'To write a Peregrine program, you describe which graph patterns you are interested in mining, and what to do with each occurrence of those patterns.', 'You provide the what and the runtime handles the how.', 'I found that the marginal returns of researchers are rapidly declining.', 'There is what’s called a “standing on toes” effect: researcher productivity declines as the field grows.', 'Because ML has recently grown very quickly, this makes better ML models much harder to find.'], ['Terminal/CLI Epub reader Simply put, ur-technical debt arises when my ideas diverge from my code.', 'That divergence is inevitable with an iterative process.', '[…] “[I]f you develop a program for a long period of time by only adding features and never reorganizing it to reflect your understanding of those features, then eventually that program simply does not contain any understanding and all efforts to work on it take longer and longer.” a real-time [REST and GraphQL] API and App dashboard for managing SQL database content.'], ['If software is such stuff as dreams are made on, how do we talk about nightmares?', 'Software is not the tangible, kickable stuff our senses are tuned to, so we draw on metaphor to communicate and reason about it.', 'The 1970s offered up spaghetti code to describe the tangle of unstructured control flow.', 'This has inspired many software-as-pasta descriptions, from lasagne for layered architectures to ravioli for—pick a decade—objects, components, modules, services, and microservices.', 'Beyond its disordered arrangement, however, spaghetti has little to offer us as a metaphor.', 'It doesn’t provide us with a useful mental model for talking about code, and has far too many positive associations.', 'If you love both ravioli and spaghetti, it’s not obvious that one of these is worse for your software architecture than the other.', 'A metaphor is a mapping that we use to describe one thing in terms of another—sometimes because we want to show something familiar from an unfamiliar angle, as in poetry, but sometimes because we want to show something unfamiliar or abstract in a more familiar light, as in software.', 'To be considered good, a metaphor has to offer a number of points of useful correspondence with what is being described.', 'Pasta doesn’t quite do this.', 'Another quality of a good metaphor is that it should not have too many obvious points of conflict.', 'It will never map its target perfectly—a metaphor is a conceit not an identity—but a good metaphor is one whose key qualities don’t contradict the very thing we are trying to say, whose points of difference don’t distract from the mental model being shared.', 'We sometimes talk about code decay and software rot.', 'These terms give a sense of degradation over time.', 'This seems accurate and relatable.', 'They also suggest a response: cleaning (we brush our teeth to reduce the chance of tooth decay) or treatment (we treat wood to avoid it rotting).', 'So far so good… but the problem with these metaphors is they refer to natural processes that happen independently of anything we do.', 'If you don’t brush your teeth, you will experience decay.', 'If you don’t touch code, it doesn’t intrinsically degrade.', 'The third quality of a metaphor that makes it effective is familiarity to its audience.', 'Explaining something unfamiliar in terms of something else that is also unfamiliar can be a long road to travel a short distance (or to end up where you started).', 'If you are familiar with the concept of entropy in statistical mechanics, with the second law of thermodynamics, and with the idea that work is needed to reduce entropy and increase order in a system, then software entropy might strike you as a descriptive metaphor—and not simply because the word work transfers happily from the world of thermodynamics to the day-to-day experience of developers.', 'If, however, these concepts are not accessible and require explanation, then, regardless of its other merits, software entropy may not be the best way to talk about accidental complexity in code.', 'Perhaps the most popular metaphor in use is based on financial debt, originating with Ward Cunningham in 1992.', 'As Martin Fowler described in 2003: Technical Debt is a wonderful metaphor developed by Ward Cunningham to help us think about this problem.', 'In this metaphor, doing things the quick and dirty way sets us up with a technical debt, which is similar to a financial debt.', 'Like a financial debt, the technical debt incurs interest payments, which come in the form of the extra effort that we have to do in future development because of the quick and dirty design choice.', 'When we look at technical debt, we see a metaphor that checks all three boxes: it has a number of useful points of correspondence; the points of difference don’t overwhelm the core idea; it is familiar.', 'Furthermore, it brings with it a useful working vocabulary.', 'For example, consider what the following debt-related words suggest to you in a software context: repayment, consolidation, creditworthiness, write-off, borrowing.', 'Although we know that by definition no metaphor is perfect, there are two common ways in which the metaphor is misapplied: assuming technical debt is necessarily something bad; equating technical debt with a financial debt value.', 'The emphasis of the former is misaligned and the latter is a category error.', 'If we are relying on the common experience of our audience, financial debt is almost always thought of as a burden.', 'If we take that together with the common experience of code quality and nudge it with leading descriptions such as “quick and dirty,” it is easy to see how in everyday use technical debt has become synonymous with poor code and poor practice.', 'We are, however, drawing too heavily on the wrong connotation.', 'Rather than reckless debt, such as from gambling, we should be thinking more along the lines of prudent debt, such as a mortgage.', 'A mortgage should be offered based on our credit history and our ability to pay and, in return, we are able to buy a house that might otherwise have been beyond our reach.', 'Similarly, Ward’s original motivation was to highlight how debt in code can be used for competitive advantage: Shipping first time code is like going into debt.', 'A little debt speeds development so long as it is paid back promptly with a rewrite.', 'This comes with a clear caveat and implication: a debt is a loan.', 'A debt is for repayment, not for running up: The danger occurs when the debt is not repaid.', 'Every minute spent on not-quite-right code counts as interest on that debt.', 'Entire engineering organizations can be brought to a stand-still under the debt load of an unconsolidated implementation.', 'As in the real world, how we run up debt and how we manage it turn out to be more complex than the simplicity of our best intentions.', 'There are teams that make time-saving decisions wisely, revisiting and addressing them later in a timely manner.', 'But in most cases where debt is incurred, discussed, and lamented, codebases reflect the firefight of different priorities, skills, and people.', 'It’s still technical debt, but it lacks the prudence and intention of Ward’s original purpose.', 'There are also teams and tools that embrace the debt metaphor so tightly that they forget it’s a metaphor.', 'They treat it literally and numerically, converting code quality into a currency value on a spreadsheet or dashboard.', 'The consequences of this thinko range from being a harmless fiction largely ignored by developers and managers to a more damaging numerology that, even though it’s well intentioned, can mislead development effort.', 'If we’re going to quantify it, what is it we’re quantifying?', 'Do we list off code smells?', 'What is the debt value of a code smell?', 'Is it constant per kind of code smell?', 'For example, is duplicate code characterised by a single cost?', 'And are code smells independent of one another?', 'Consider that, for example, duplication is sometimes used to reduce coupling, so the debit becomes a credit in that context.', 'We can conclude that a code smell is not an isolated thing with a single look-up debt value, so this is clearly a more complex problem dependent on many factors.', 'As a multivariable problem, what does it depend on?', 'And how?', 'And how do we know?', 'And what would the value or—more likely—value distribution reflect?', 'The cost of fixing?', 'Or, more honestly, an estimate of the cost of fixing?', 'But even if we are somehow able to conjure a number out of this ever-growing list of considerations—and even if that number has some relation to observed reality—we have put a number to the wrong quantity.', 'We have, in fact, missed the whole point of the metaphor.', 'Technical debt is not the cost of repaying the debt: it is the cost of owning the debt.', 'These are not the same.', 'That is the message of the technical debt metaphor: it is not simply a measure of the specific work needed to repay the debt; it is the additional time and effort added to all past, present, and future work that comes from having the debt in the first place.', 'By taking the metaphor literally, we have robbed it of its value.', 'Its value is to offer us a figure of speech not of currency, a mental model for talking and reasoning about qualities of our code that are not simply stated in code.', 'No matter how well meant, pushing any metaphor beyond its applicability leads to metaphor shear.', 'It is, after all, metaphor and not identity.'], ['Network and Distributed System Security Symposium We present Lifty, a domain-specific language for data-centric applications that manipulate sensitive data.', 'A Lifty programmer annotates the sources of sensitive data with declarative security policies, and the language statically and automatically verifies that the application handles the data according to the policies.', 'Moreover, if verification fails, Lifty suggests a provably correct repair, thereby easing the programmer burden of implementing policy enforcing code throughout the application.', 'Create a schema by using visual blocks system.', 'GraphQL Editor will transform them into code.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqT7V6zZ4JIu"
      },
      "source": [
        "### Word tokenize each sentence within each document.\n",
        "\n",
        "You should end up with a nested list structure where the outer list contains all the sentences in each document and the inner list contains the tokenized sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2vUMoXXkIP8"
      },
      "source": [
        "word_tk = []\n",
        "for doc in sents_tk:\n",
        "  temp = []\n",
        "  for sent in doc:\n",
        "    temp.append(word_tokenize(sent))\n",
        "  \n",
        "  word_tk.append(temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsAOrAq16I2I"
      },
      "source": [
        "word_tk[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpYc_WZwE-Ga",
        "outputId": "53e1d430-8f67-48b7-fb42-dfb3c20e3f10"
      },
      "source": [
        "len(word_tk)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDwEBkZHGjbT",
        "outputId": "57bdd97f-d97e-4adc-e9ea-bb9f45480250"
      },
      "source": [
        "len(word_tk[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17JMEoHe4JIv"
      },
      "source": [
        "#alternate way\n",
        "#word_tokenized = [[word_tokenize(sent) for sent in doc] for doc in sent_tokenized]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pk1r_y4C4JIz"
      },
      "source": [
        "### Tag each token with its part of speech."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4d92mPZ7cAl",
        "outputId": "39063757-9095-41f1-c6cd-2bb92ba5ee3c"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Flze4fS-4JI0"
      },
      "source": [
        "tagged = [[pos_tag(sent) for sent in doc] for doc in word_tk]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYMhycxp7sJb",
        "outputId": "c18c118b-6ba6-43c4-b740-8f76e2a52eeb"
      },
      "source": [
        "len(tagged[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GB59dDYE7f-Z",
        "outputId": "677553ee-0bd4-4b32-9512-eaed81258660"
      },
      "source": [
        "tagged[0][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('2020', 'CD'),\n",
              " ('has', 'VBZ'),\n",
              " ('been', 'VBN'),\n",
              " ('a', 'DT'),\n",
              " ('year', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('great', 'JJ'),\n",
              " ('challenges', 'NNS'),\n",
              " ('for', 'IN'),\n",
              " ('so', 'RB'),\n",
              " ('many', 'JJ'),\n",
              " (',', ','),\n",
              " ('but', 'CC'),\n",
              " ('it', 'PRP'),\n",
              " ('’', 'NNP'),\n",
              " ('s', 'VBZ'),\n",
              " ('not', 'RB'),\n",
              " ('all', 'DT'),\n",
              " ('negative', 'JJ'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmIltOA44JI5"
      },
      "source": [
        "### Word tokenize the raw text of each document and remove stop words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jr9QPUer9w11",
        "outputId": "63f07d3c-d78b-430c-d1a2-73ca342dc5fe"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZmHJRfOEh-5",
        "outputId": "b461a877-4c11-43c6-e115-28cb6234db12"
      },
      "source": [
        "len(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "179"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVH8eqHj_8TI"
      },
      "source": [
        "#no stopwords in docs\n",
        "no_sw = [[token.lower() for token in word_tokenize(doc) if token.lower() not in stopwords.words('english')] for doc in docs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xJukB-cH6nr",
        "outputId": "48c1272b-85a0-4984-e7c8-180c715a16af"
      },
      "source": [
        "no_sw[0][0:9]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['2020', 'year', 'great', 'challenges', 'many', ',', '’', 'negative', '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IMHWgXXI4ZW",
        "outputId": "e50c6432-c12e-4f85-e1dc-7901072eae8a"
      },
      "source": [
        "#comparing to orginial word tokenization\n",
        "word_tk[0][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['2020',\n",
              " 'has',\n",
              " 'been',\n",
              " 'a',\n",
              " 'year',\n",
              " 'of',\n",
              " 'great',\n",
              " 'challenges',\n",
              " 'for',\n",
              " 'so',\n",
              " 'many',\n",
              " ',',\n",
              " 'but',\n",
              " 'it',\n",
              " '’',\n",
              " 's',\n",
              " 'not',\n",
              " 'all',\n",
              " 'negative',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAZggM574JI_"
      },
      "source": [
        "### For every document, stem all the words in the document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXSDilN5DxQv"
      },
      "source": [
        "stemmer = SnowballStemmer('english')\n",
        "stemmed = [[stemmer.stem(token) for token in word_tokenize(doc)] for doc in docs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ukEpS0gJvyr",
        "outputId": "66930078-369f-4a20-bd3a-5262946e8337"
      },
      "source": [
        "stemmed[0][0:11]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['2020',\n",
              " 'has',\n",
              " 'been',\n",
              " 'a',\n",
              " 'year',\n",
              " 'of',\n",
              " 'great',\n",
              " 'challeng',\n",
              " 'for',\n",
              " 'so',\n",
              " 'mani']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2N4mHrZ4JJQ"
      },
      "source": [
        "### Iterate through each document, computing and printing the following document statistics for each.\n",
        "\n",
        "- Number of sentences\n",
        "- Average words per sentence\n",
        "- Vocabulary\n",
        "- Lexical Diversity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "7kBjbVZb4JJR"
      },
      "source": [
        "def doc_stats(doc):\n",
        "  sents = sent_tokenize(doc)\n",
        "  tokenized = [word_tokenize(sent) for sent in sents]\n",
        "  total_sents = len(sents)\n",
        "  avg_words_sent = sum([len(sent) for sent in tokenized])/total_sents\n",
        "  vocab = len(set([word.lower() for word in word_tokenize(doc)]))\n",
        "  lexicon_diversity = vocab / len(word_tokenize(doc))\n",
        "  print(f'Number of sentences: {total_sents}')\n",
        "  print(f'Avg. number of words per sentence: {avg_words_sent}')\n",
        "  print(f'Unique word count (vocab): {vocab}')\n",
        "  print(f'Lexicon diversity: {lexicon_diversity}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIn-ayWPKZkK",
        "outputId": "3d71492f-e4ab-45d0-cfce-fb3e358af27e"
      },
      "source": [
        "for i, doc in enumerate(docs):\n",
        "  print(f'Stats for document {i+1}')\n",
        "  doc_stats(doc)\n",
        "  print('------------')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stats for document 1\n",
            "Number of sentences: 15\n",
            "Avg. number of words per sentence: 27.466666666666665\n",
            "Unique word count (vocab): 198\n",
            "Lexicon diversity: 0.48058252427184467\n",
            "------------\n",
            "Stats for document 2\n",
            "Number of sentences: 77\n",
            "Avg. number of words per sentence: 24.25974025974026\n",
            "Unique word count (vocab): 555\n",
            "Lexicon diversity: 0.29710920770877947\n",
            "------------\n",
            "Stats for document 3\n",
            "Number of sentences: 21\n",
            "Avg. number of words per sentence: 14.476190476190476\n",
            "Unique word count (vocab): 183\n",
            "Lexicon diversity: 0.6019736842105263\n",
            "------------\n",
            "Stats for document 4\n",
            "Number of sentences: 42\n",
            "Avg. number of words per sentence: 23.595238095238095\n",
            "Unique word count (vocab): 380\n",
            "Lexicon diversity: 0.3834510595358224\n",
            "------------\n",
            "Stats for document 5\n",
            "Number of sentences: 7\n",
            "Avg. number of words per sentence: 19.571428571428573\n",
            "Unique word count (vocab): 91\n",
            "Lexicon diversity: 0.6642335766423357\n",
            "------------\n",
            "Stats for document 6\n",
            "Number of sentences: 2\n",
            "Avg. number of words per sentence: 27.0\n",
            "Unique word count (vocab): 45\n",
            "Lexicon diversity: 0.8333333333333334\n",
            "------------\n",
            "Stats for document 7\n",
            "Number of sentences: 10\n",
            "Avg. number of words per sentence: 26.3\n",
            "Unique word count (vocab): 161\n",
            "Lexicon diversity: 0.6121673003802282\n",
            "------------\n",
            "Stats for document 8\n",
            "Number of sentences: 4\n",
            "Avg. number of words per sentence: 29.25\n",
            "Unique word count (vocab): 74\n",
            "Lexicon diversity: 0.6324786324786325\n",
            "------------\n",
            "Stats for document 9\n",
            "Number of sentences: 11\n",
            "Avg. number of words per sentence: 20.636363636363637\n",
            "Unique word count (vocab): 139\n",
            "Lexicon diversity: 0.6123348017621145\n",
            "------------\n",
            "Stats for document 10\n",
            "Number of sentences: 11\n",
            "Avg. number of words per sentence: 14.545454545454545\n",
            "Unique word count (vocab): 117\n",
            "Lexicon diversity: 0.73125\n",
            "------------\n",
            "Stats for document 11\n",
            "Number of sentences: 205\n",
            "Avg. number of words per sentence: 23.619512195121953\n",
            "Unique word count (vocab): 1147\n",
            "Lexicon diversity: 0.23688558446922758\n",
            "------------\n",
            "Stats for document 12\n",
            "Number of sentences: 5\n",
            "Avg. number of words per sentence: 23.2\n",
            "Unique word count (vocab): 88\n",
            "Lexicon diversity: 0.7586206896551724\n",
            "------------\n",
            "Stats for document 13\n",
            "Number of sentences: 9\n",
            "Avg. number of words per sentence: 25.0\n",
            "Unique word count (vocab): 145\n",
            "Lexicon diversity: 0.6444444444444445\n",
            "------------\n",
            "Stats for document 14\n",
            "Number of sentences: 7\n",
            "Avg. number of words per sentence: 22.714285714285715\n",
            "Unique word count (vocab): 101\n",
            "Lexicon diversity: 0.6352201257861635\n",
            "------------\n",
            "Stats for document 15\n",
            "Number of sentences: 8\n",
            "Avg. number of words per sentence: 36.25\n",
            "Unique word count (vocab): 173\n",
            "Lexicon diversity: 0.596551724137931\n",
            "------------\n",
            "Stats for document 16\n",
            "Number of sentences: 111\n",
            "Avg. number of words per sentence: 29.486486486486488\n",
            "Unique word count (vocab): 950\n",
            "Lexicon diversity: 0.2902535899786129\n",
            "------------\n",
            "Stats for document 17\n",
            "Number of sentences: 11\n",
            "Avg. number of words per sentence: 21.636363636363637\n",
            "Unique word count (vocab): 134\n",
            "Lexicon diversity: 0.5630252100840336\n",
            "------------\n",
            "Stats for document 18\n",
            "Number of sentences: 57\n",
            "Avg. number of words per sentence: 22.92982456140351\n",
            "Unique word count (vocab): 446\n",
            "Lexicon diversity: 0.34123947972456004\n",
            "------------\n",
            "Stats for document 19\n",
            "Number of sentences: 8\n",
            "Avg. number of words per sentence: 29.25\n",
            "Unique word count (vocab): 143\n",
            "Lexicon diversity: 0.6111111111111112\n",
            "------------\n",
            "Stats for document 20\n",
            "Number of sentences: 1\n",
            "Avg. number of words per sentence: 70.0\n",
            "Unique word count (vocab): 51\n",
            "Lexicon diversity: 0.7285714285714285\n",
            "------------\n",
            "Stats for document 21\n",
            "Number of sentences: 6\n",
            "Avg. number of words per sentence: 19.166666666666668\n",
            "Unique word count (vocab): 71\n",
            "Lexicon diversity: 0.6173913043478261\n",
            "------------\n",
            "Stats for document 22\n",
            "Number of sentences: 8\n",
            "Avg. number of words per sentence: 17.0\n",
            "Unique word count (vocab): 102\n",
            "Lexicon diversity: 0.75\n",
            "------------\n",
            "Stats for document 23\n",
            "Number of sentences: 5\n",
            "Avg. number of words per sentence: 22.4\n",
            "Unique word count (vocab): 78\n",
            "Lexicon diversity: 0.6964285714285714\n",
            "------------\n",
            "Stats for document 24\n",
            "Number of sentences: 21\n",
            "Avg. number of words per sentence: 21.476190476190474\n",
            "Unique word count (vocab): 220\n",
            "Lexicon diversity: 0.4878048780487805\n",
            "------------\n",
            "Stats for document 25\n",
            "Number of sentences: 8\n",
            "Avg. number of words per sentence: 31.75\n",
            "Unique word count (vocab): 147\n",
            "Lexicon diversity: 0.5787401574803149\n",
            "------------\n",
            "Stats for document 26\n",
            "Number of sentences: 9\n",
            "Avg. number of words per sentence: 28.77777777777778\n",
            "Unique word count (vocab): 145\n",
            "Lexicon diversity: 0.5598455598455598\n",
            "------------\n",
            "Stats for document 27\n",
            "Number of sentences: 60\n",
            "Avg. number of words per sentence: 29.7\n",
            "Unique word count (vocab): 600\n",
            "Lexicon diversity: 0.3367003367003367\n",
            "------------\n",
            "Stats for document 28\n",
            "Number of sentences: 1\n",
            "Avg. number of words per sentence: 40.0\n",
            "Unique word count (vocab): 30\n",
            "Lexicon diversity: 0.75\n",
            "------------\n",
            "Stats for document 29\n",
            "Number of sentences: 3\n",
            "Avg. number of words per sentence: 41.333333333333336\n",
            "Unique word count (vocab): 84\n",
            "Lexicon diversity: 0.6774193548387096\n",
            "------------\n",
            "Stats for document 30\n",
            "Number of sentences: 42\n",
            "Avg. number of words per sentence: 23.357142857142858\n",
            "Unique word count (vocab): 378\n",
            "Lexicon diversity: 0.3853211009174312\n",
            "------------\n",
            "Stats for document 31\n",
            "Number of sentences: 3\n",
            "Avg. number of words per sentence: 16.0\n",
            "Unique word count (vocab): 46\n",
            "Lexicon diversity: 0.9583333333333334\n",
            "------------\n",
            "Stats for document 32\n",
            "Number of sentences: 6\n",
            "Avg. number of words per sentence: 19.666666666666668\n",
            "Unique word count (vocab): 84\n",
            "Lexicon diversity: 0.711864406779661\n",
            "------------\n",
            "Stats for document 33\n",
            "Number of sentences: 6\n",
            "Avg. number of words per sentence: 21.333333333333332\n",
            "Unique word count (vocab): 91\n",
            "Lexicon diversity: 0.7109375\n",
            "------------\n",
            "Stats for document 34\n",
            "Number of sentences: 6\n",
            "Avg. number of words per sentence: 24.166666666666668\n",
            "Unique word count (vocab): 104\n",
            "Lexicon diversity: 0.7172413793103448\n",
            "------------\n",
            "Stats for document 35\n",
            "Number of sentences: 4\n",
            "Avg. number of words per sentence: 23.0\n",
            "Unique word count (vocab): 68\n",
            "Lexicon diversity: 0.7391304347826086\n",
            "------------\n",
            "Stats for document 36\n",
            "Number of sentences: 209\n",
            "Avg. number of words per sentence: 24.124401913875598\n",
            "Unique word count (vocab): 1417\n",
            "Lexicon diversity: 0.2810392701309004\n",
            "------------\n",
            "Stats for document 37\n",
            "Number of sentences: 12\n",
            "Avg. number of words per sentence: 17.333333333333332\n",
            "Unique word count (vocab): 123\n",
            "Lexicon diversity: 0.5913461538461539\n",
            "------------\n",
            "Stats for document 38\n",
            "Number of sentences: 3\n",
            "Avg. number of words per sentence: 19.0\n",
            "Unique word count (vocab): 45\n",
            "Lexicon diversity: 0.7894736842105263\n",
            "------------\n",
            "Stats for document 39\n",
            "Number of sentences: 4\n",
            "Avg. number of words per sentence: 29.75\n",
            "Unique word count (vocab): 85\n",
            "Lexicon diversity: 0.7142857142857143\n",
            "------------\n",
            "Stats for document 40\n",
            "Number of sentences: 68\n",
            "Avg. number of words per sentence: 22.955882352941178\n",
            "Unique word count (vocab): 555\n",
            "Lexicon diversity: 0.3555413196668802\n",
            "------------\n",
            "Stats for document 41\n",
            "Number of sentences: 9\n",
            "Avg. number of words per sentence: 20.444444444444443\n",
            "Unique word count (vocab): 115\n",
            "Lexicon diversity: 0.625\n",
            "------------\n",
            "Stats for document 42\n",
            "Number of sentences: 37\n",
            "Avg. number of words per sentence: 23.10810810810811\n",
            "Unique word count (vocab): 331\n",
            "Lexicon diversity: 0.3871345029239766\n",
            "------------\n",
            "Stats for document 43\n",
            "Number of sentences: 7\n",
            "Avg. number of words per sentence: 21.0\n",
            "Unique word count (vocab): 105\n",
            "Lexicon diversity: 0.7142857142857143\n",
            "------------\n",
            "Stats for document 44\n",
            "Number of sentences: 10\n",
            "Avg. number of words per sentence: 20.8\n",
            "Unique word count (vocab): 126\n",
            "Lexicon diversity: 0.6057692307692307\n",
            "------------\n",
            "Stats for document 45\n",
            "Number of sentences: 1\n",
            "Avg. number of words per sentence: 39.0\n",
            "Unique word count (vocab): 34\n",
            "Lexicon diversity: 0.8717948717948718\n",
            "------------\n",
            "Stats for document 46\n",
            "Number of sentences: 8\n",
            "Avg. number of words per sentence: 30.375\n",
            "Unique word count (vocab): 131\n",
            "Lexicon diversity: 0.5390946502057613\n",
            "------------\n",
            "Stats for document 47\n",
            "Number of sentences: 5\n",
            "Avg. number of words per sentence: 18.2\n",
            "Unique word count (vocab): 64\n",
            "Lexicon diversity: 0.7032967032967034\n",
            "------------\n",
            "Stats for document 48\n",
            "Number of sentences: 6\n",
            "Avg. number of words per sentence: 34.666666666666664\n",
            "Unique word count (vocab): 140\n",
            "Lexicon diversity: 0.6730769230769231\n",
            "------------\n",
            "Stats for document 49\n",
            "Number of sentences: 11\n",
            "Avg. number of words per sentence: 25.181818181818183\n",
            "Unique word count (vocab): 147\n",
            "Lexicon diversity: 0.5306859205776173\n",
            "------------\n",
            "Stats for document 50\n",
            "Number of sentences: 2\n",
            "Avg. number of words per sentence: 23.0\n",
            "Unique word count (vocab): 40\n",
            "Lexicon diversity: 0.8695652173913043\n",
            "------------\n",
            "Stats for document 51\n",
            "Number of sentences: 209\n",
            "Avg. number of words per sentence: 25.803827751196174\n",
            "Unique word count (vocab): 1295\n",
            "Lexicon diversity: 0.2401260893751159\n",
            "------------\n",
            "Stats for document 52\n",
            "Number of sentences: 86\n",
            "Avg. number of words per sentence: 22.058139534883722\n",
            "Unique word count (vocab): 573\n",
            "Lexicon diversity: 0.30205587770163417\n",
            "------------\n",
            "Stats for document 53\n",
            "Number of sentences: 7\n",
            "Avg. number of words per sentence: 28.142857142857142\n",
            "Unique word count (vocab): 132\n",
            "Lexicon diversity: 0.6700507614213198\n",
            "------------\n",
            "Stats for document 54\n",
            "Number of sentences: 5\n",
            "Avg. number of words per sentence: 44.0\n",
            "Unique word count (vocab): 142\n",
            "Lexicon diversity: 0.6454545454545455\n",
            "------------\n",
            "Stats for document 55\n",
            "Number of sentences: 11\n",
            "Avg. number of words per sentence: 30.09090909090909\n",
            "Unique word count (vocab): 194\n",
            "Lexicon diversity: 0.5861027190332326\n",
            "------------\n",
            "Stats for document 56\n",
            "Number of sentences: 5\n",
            "Avg. number of words per sentence: 29.6\n",
            "Unique word count (vocab): 104\n",
            "Lexicon diversity: 0.7027027027027027\n",
            "------------\n",
            "Stats for document 57\n",
            "Number of sentences: 13\n",
            "Avg. number of words per sentence: 25.923076923076923\n",
            "Unique word count (vocab): 219\n",
            "Lexicon diversity: 0.6498516320474778\n",
            "------------\n",
            "Stats for document 58\n",
            "Number of sentences: 3\n",
            "Avg. number of words per sentence: 33.0\n",
            "Unique word count (vocab): 76\n",
            "Lexicon diversity: 0.7676767676767676\n",
            "------------\n",
            "Stats for document 59\n",
            "Number of sentences: 73\n",
            "Avg. number of words per sentence: 22.315068493150687\n",
            "Unique word count (vocab): 548\n",
            "Lexicon diversity: 0.336402701043585\n",
            "------------\n",
            "Stats for document 60\n",
            "Number of sentences: 5\n",
            "Avg. number of words per sentence: 19.4\n",
            "Unique word count (vocab): 64\n",
            "Lexicon diversity: 0.6597938144329897\n",
            "------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZMd7ucoMTqd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}