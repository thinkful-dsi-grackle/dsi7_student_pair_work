{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "name": "Angel_D74_L1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4I54bZzMImI"
      },
      "source": [
        "# Machine Learning: Text Classification Assignment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBhHdzKtMImK"
      },
      "source": [
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.pipeline import Pipeline\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "from nltk.corpus.reader.plaintext import CategorizedPlaintextCorpusReader"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2tVDJJaMImN"
      },
      "source": [
        "### Use the CategorizedPlaintextCorpusReader to import the AP_News corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNHEzKngizRM",
        "outputId": "f4a7d094-f078-437f-ff76-54df9cc8a9bf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVtiDKS9yqR6"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REtoZb_iMImO"
      },
      "source": [
        "PATH = '/content/drive/MyDrive/AP_News/'\r\n",
        "\r\n",
        "DOC_PATTERN = r'.*\\.txt'\r\n",
        "CAT_PATTERN = r'([\\w_\\s]+)/.*'\r\n",
        "\r\n",
        "corpus = CategorizedPlaintextCorpusReader(PATH, DOC_PATTERN, cat_pattern=CAT_PATTERN)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oFM6B8khm616",
        "outputId": "eb3ad401-c6c6-4068-d8c7-3fe48df9962d"
      },
      "source": [
        "corpus.fileids()[0]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'health/http-apnews-com-03bc406312384416843138b2b23dec14.txt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQMuBvquMImP"
      },
      "source": [
        "### Create two separate lists - one containing the text from each document and another containing the category of each article in the corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jU4ZNM-MImQ"
      },
      "source": [
        "docs = [corpus.raw(fileid) for fileid in corpus.fileids()]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzDg1qLHpgZF"
      },
      "source": [
        "categories = [corpus.categories(fileid)[0] \r\n",
        "              for fileid in corpus.fileids()]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Htdz3l84u52j",
        "outputId": "937aff56-6378-4936-d764-db8c5876cb35"
      },
      "source": [
        "categories[0]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'health'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGRABGW8MImR"
      },
      "source": [
        "### Preprocess the corpus, ensuring to include the following steps.\n",
        "\n",
        "- Word tokenize the documents.\n",
        "- Lemmatize, stem, and lowercase all tokens.\n",
        "- Remove punctuation and stop words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hk-Nlze1MImS"
      },
      "source": [
        "def preprocess(docs):\r\n",
        "  lemmatizer = WordNetLemmatizer()\r\n",
        "  stemmer = SnowballStemmer('english')\r\n",
        "  preprocessed = []\r\n",
        "\r\n",
        "  for doc in docs:\r\n",
        "    tokenized = word_tokenize(doc)\r\n",
        "\r\n",
        "    cleaned = [stemmer.stem(lemmatizer.lemmatize(token.lower())) for token in tokenized\r\n",
        "               if not token.lower() in stopwords.words('english')\r\n",
        "               if token.isalpha()\r\n",
        "               ]\r\n",
        "    untokenized = \" \".join(cleaned)\r\n",
        "    preprocessed.append(untokenized)\r\n",
        "  \r\n",
        "  return preprocessed"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTU2F0TcyXrt"
      },
      "source": [
        "preprocessed = preprocess(docs)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZE-q4ziMImT"
      },
      "source": [
        "### Split the data into training and testing sets with the size of the test set being 30% of the records."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKFyBgjBMImU"
      },
      "source": [
        "X_train, X_test, y_train, y_test = tts(preprocessed, categories, test_size=0.3)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reegQu_5MImV"
      },
      "source": [
        "### Construct a pipeline that TF-IDF vectorizes the text and trains a Random Forest classification model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3JJ3hjNMImW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3904bc1-6708-4e40-82d6-c75cc8e7fd3b"
      },
      "source": [
        "model = Pipeline([\r\n",
        "                  ('vect', CountVectorizer()),\r\n",
        "                  ('tfidf', TfidfTransformer()),\r\n",
        "                  ('clf', LogisticRegression()),\r\n",
        "])\r\n",
        "\r\n",
        "model.fit(X_train, y_train)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vect',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, vocabulary=None)),\n",
              "                ('tfidf',\n",
              "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
              "                                  sublinear_tf=False, use_idf=True)),\n",
              "                ('clf',\n",
              "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                    fit_intercept=True, intercept_scaling=1,\n",
              "                                    l1_ratio=None, max_iter=100,\n",
              "                                    multi_class='auto', n_jobs=None,\n",
              "                                    penalty='l2', random_state=None,\n",
              "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                                    warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFt6djjpMImX"
      },
      "source": [
        "### Generate predictions on the test set and print a classification report to evaluate how well the model performed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWT99tvHMImY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6867a0e6-c632-4908-b44f-45007bcbeb9d"
      },
      "source": [
        "predictions = model.predict(X_test)\r\n",
        "print(classification_report(y_test, predictions))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      health       0.89      0.84      0.86        19\n",
            "    politics       0.93      0.78      0.85        18\n",
            "      sports       0.94      1.00      0.97        17\n",
            "        tech       0.67      0.83      0.74        12\n",
            "\n",
            "    accuracy                           0.86        66\n",
            "   macro avg       0.86      0.86      0.86        66\n",
            "weighted avg       0.87      0.86      0.87        66\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2BwKeBNMImZ"
      },
      "source": [
        "### Perform 10-fold cross validation and obtain the averge F1 score across all the folds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n9cTvc6MIma",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57654768-e3c3-411b-d72c-61f59cd7b5cd"
      },
      "source": [
        "scores = cross_val_score(model, preprocessed, categories, cv=10, scoring='f1_macro')\r\n",
        "scores.mean()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8187741262005968"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1H9ERShiMImb"
      },
      "source": [
        "### Ingest, preprocess, and predict the topic of the article at the following URL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d2mfYGSMImc"
      },
      "source": [
        "url = 'https://www.nytimes.com/2019/11/25/business/uber-london.html'"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7piaPcKNMImd"
      },
      "source": [
        "import requests\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "\r\n",
        "def get_url_text(url):\r\n",
        "  response = requests.get(url)\r\n",
        "  content = response.text\r\n",
        "\r\n",
        "  TAGS = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'h7', 'p', 'li']\r\n",
        "  soup = BeautifulSoup(content, \"lxml\")\r\n",
        "  text_list = [tag.get_text() for tag in soup.find_all(TAGS)]\r\n",
        "  text = ' '.join(text_list)\r\n",
        "  return text.strip().replace('\\n', '')"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-lyVng832WSe",
        "outputId": "10a1d2ab-7b57-47bc-9c88-43149d720e24"
      },
      "source": [
        "text = get_url_text(url)\r\n",
        "cleaned = preprocess([text])\r\n",
        "model.predict(cleaned)[0]"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'tech'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    }
  ]
}