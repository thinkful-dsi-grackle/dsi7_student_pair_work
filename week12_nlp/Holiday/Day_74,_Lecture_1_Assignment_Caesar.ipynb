{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "name": "Day 74, Lecture 1: Assignment_Caesar.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4I54bZzMImI"
      },
      "source": [
        "# Machine Learning: Text Classification Assignment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9aUJzlw-RVQ",
        "outputId": "77607e1f-05f8-462f-ccfb-c25199f61ac1"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhDWP3ur9BBy",
        "outputId": "b0a43043-f5ea-4a3f-bc18-f3346b96dbcf"
      },
      "source": [
        "!python -m spacy download en_core_web_sm\r\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (51.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.3.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBhHdzKtMImK"
      },
      "source": [
        "import spacy\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.pipeline import Pipeline\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "from nltk.corpus.reader.plaintext import CategorizedPlaintextCorpusReader"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ad07BQGb95Nd",
        "outputId": "16be4927-2bb8-4384-e149-38aa05e7df1b"
      },
      "source": [
        "import nltk\r\n",
        "\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2tVDJJaMImN"
      },
      "source": [
        "### Use the CategorizedPlaintextCorpusReader to import the AP_News corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REtoZb_iMImO"
      },
      "source": [
        "DOC_PATTERN = r\".*\\.txt\"\r\n",
        "CAT_PATTERN = r\"([\\w_\\s]+)/.*\"\r\n",
        "path = \"./AP_News/\"\r\n",
        "\r\n",
        "corpus = CategorizedPlaintextCorpusReader(path, DOC_PATTERN,\r\n",
        "                                          cat_pattern=CAT_PATTERN)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQMuBvquMImP"
      },
      "source": [
        "### Create two separate lists - one containing the text from each document and another containing the category of each article in the corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jU4ZNM-MImQ"
      },
      "source": [
        "docs = [corpus.raw(id_) for id_ in corpus.fileids()]\r\n",
        "categories = [corpus.categories(id_)[0] for id_ in corpus.fileids()]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGRABGW8MImR"
      },
      "source": [
        "### Preprocess the corpus, ensuring to include the following steps.\n",
        "\n",
        "- Word tokenize the documents.\n",
        "- Lemmatize, stem, and lowercase all tokens.\n",
        "- Remove punctuation and stop words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hk-Nlze1MImS"
      },
      "source": [
        "def preprocess(docs):\r\n",
        "    lemmatizer = WordNetLemmatizer()\r\n",
        "    stemmer = SnowballStemmer(\"english\")\r\n",
        "    preprocessed = []\r\n",
        "\r\n",
        "    for doc in docs:\r\n",
        "        tokenized = word_tokenize(doc)\r\n",
        "        cleaned = [stemmer.stem(lemmatizer.lemmatize(token.lower()))\r\n",
        "        for token in tokenized\r\n",
        "        if not token.lower() in stopwords.words(\"english\")\r\n",
        "        if token.isalpha()]\r\n",
        "\r\n",
        "        untokenized = \" \".join(cleaned)\r\n",
        "        preprocessed.append(untokenized)\r\n",
        "\r\n",
        "    return preprocessed"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wWMHNE4DISA"
      },
      "source": [
        "processed_docs = preprocess(docs)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZE-q4ziMImT"
      },
      "source": [
        "### Split the data into training and testing sets with the size of the test set being 30% of the records."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKFyBgjBMImU"
      },
      "source": [
        "from sklearn.model_selection import train_test_split as tts\r\n",
        "\r\n",
        "X_train, X_test, y_train, y_test = tts(processed_docs, categories, test_size=0.2)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reegQu_5MImV"
      },
      "source": [
        "### Construct a pipeline that TF-IDF vectorizes the text and trains a Random Forest classification model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3JJ3hjNMImW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d843cfb-33a0-4f69-820e-f66fbf624558"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\r\n",
        "\r\n",
        "model = Pipeline(\r\n",
        "    [\r\n",
        "     ('vect', CountVectorizer()),\r\n",
        "     ('tfidf', TfidfTransformer()),\r\n",
        "     ('clf', LogisticRegression())\r\n",
        "    ])\r\n",
        "\r\n",
        "model.fit(X_train, y_train)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vect',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, vocabulary=None)),\n",
              "                ('tfidf',\n",
              "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
              "                                  sublinear_tf=False, use_idf=True)),\n",
              "                ('clf',\n",
              "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                    fit_intercept=True, intercept_scaling=1,\n",
              "                                    l1_ratio=None, max_iter=100,\n",
              "                                    multi_class='auto', n_jobs=None,\n",
              "                                    penalty='l2', random_state=None,\n",
              "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                                    warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFt6djjpMImX"
      },
      "source": [
        "### Generate predictions on the test set and print a classification report to evaluate how well the model performed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWT99tvHMImY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f13d8dc-17d4-4f87-9077-16e36514697d"
      },
      "source": [
        "pred = model.predict(X_test)\r\n",
        "print(classification_report(y_test, pred))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      health       0.88      0.70      0.78        10\n",
            "    politics       0.56      0.90      0.69        10\n",
            "      sports       1.00      0.69      0.82        13\n",
            "        tech       0.64      0.64      0.64        11\n",
            "\n",
            "    accuracy                           0.73        44\n",
            "   macro avg       0.77      0.73      0.73        44\n",
            "weighted avg       0.78      0.73      0.73        44\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2BwKeBNMImZ"
      },
      "source": [
        "### Perform 10-fold cross validation and obtain the averge F1 score across all the folds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n9cTvc6MIma",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "134c4140-6cf0-4d8a-f634-b2abb9fbdc9a"
      },
      "source": [
        "scores = cross_val_score(model, processed_docs, categories, cv=10, scoring='f1_macro')\r\n",
        "scores.mean()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8187741262005968"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1H9ERShiMImb"
      },
      "source": [
        "### Ingest, preprocess, and predict the topic of the article at the following URL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d2mfYGSMImc"
      },
      "source": [
        "url = 'https://www.nytimes.com/2019/11/25/business/uber-london.html'"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7piaPcKNMImd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2226aee2-4bb1-4a29-bdec-4e4a16403cff"
      },
      "source": [
        "import requests\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "\r\n",
        "def get_url_text(url):\r\n",
        "    response = requests.get(url)\r\n",
        "    content = response.text\r\n",
        "\r\n",
        "    TAGS = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'h7', 'p', 'li']\r\n",
        "    soup = BeautifulSoup(content, \"lxml\")\r\n",
        "    text_list = [tag.get_text() for tag in soup.find_all(TAGS)]\r\n",
        "    text = \" \".join(text_list)\r\n",
        "    return text\r\n",
        "\r\n",
        "text = get_url_text(url)\r\n",
        "cleaned = preprocess([text])\r\n",
        "model.predict(cleaned)[0]"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'tech'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qcOJO0YFFtg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}