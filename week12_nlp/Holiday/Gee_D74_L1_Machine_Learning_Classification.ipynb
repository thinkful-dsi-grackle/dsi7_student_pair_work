{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "name": "Gee_D74_L1_Machine_Learning_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "em8LGJvpdZSQ",
        "outputId": "21e70c42-0eee-496b-a7bf-acf3e9d2c36b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4I54bZzMImI"
      },
      "source": [
        "# Machine Learning: Text Classification Assignment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBhHdzKtMImK"
      },
      "source": [
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.pipeline import Pipeline\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "from nltk.corpus.reader.plaintext import CategorizedPlaintextCorpusReader"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qNqWxAnfQAf",
        "outputId": "9a18ef25-bd48-4634-f753-e1d7388844cb"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXS4LKQvfVU3",
        "outputId": "a70c9e78-827b-4d06-89f3-1852f4898dfc"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRUFiEvgfYs3",
        "outputId": "9c23e993-e129-4f81-b79a-cf456bf1c9c2"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2tVDJJaMImN"
      },
      "source": [
        "### Use the CategorizedPlaintextCorpusReader to import the AP_News corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REtoZb_iMImO"
      },
      "source": [
        "PATH = '/content/drive/MyDrive/python_for_data_scientists/AP_News'\r\n",
        "\r\n",
        "DOC_PATTERN = r'.*\\.txt'\r\n",
        "CAT_PATTERN = r'([\\w_\\s]+)/.*'\r\n",
        "\r\n",
        "corpus = CategorizedPlaintextCorpusReader(PATH, DOC_PATTERN, cat_pattern=CAT_PATTERN)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQMuBvquMImP"
      },
      "source": [
        "### Create two separate lists - one containing the text from each document and another containing the category of each article in the corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jU4ZNM-MImQ"
      },
      "source": [
        "docs = [corpus.raw(fileid) for fileid in corpus.fileids()]\r\n",
        "categories = [corpus.categories(fileid)[0] for fileid in corpus.fileids()]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGRABGW8MImR"
      },
      "source": [
        "### Preprocess the corpus, ensuring to include the following steps.\n",
        "\n",
        "- Word tokenize the documents.\n",
        "- Lemmatize, stem, and lowercase all tokens.\n",
        "- Remove punctuation and stop words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hk-Nlze1MImS"
      },
      "source": [
        "def preprocess(docs):\r\n",
        "  lemmatizer = WordNetLemmatizer()\r\n",
        "  stemmer = SnowballStemmer('english')\r\n",
        "  preprocessed = []\r\n",
        "\r\n",
        "  for doc in docs:\r\n",
        "    tokenized = word_tokenize(doc)\r\n",
        "    cleaned = [stemmer.stem(lemmatizer.lemmatize(token.lower())) \r\n",
        "               for token in tokenized \r\n",
        "               if not token.lower() in stopwords.words('english') \r\n",
        "               if token.isalpha()]\r\n",
        "\r\n",
        "    untokenized = ' '.join(cleaned)\r\n",
        "    preprocessed.append(untokenized)\r\n",
        "  return preprocessed"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMH6aY2IfHNQ"
      },
      "source": [
        "good_to_go = preprocess(docs)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReBvPoG_fhv_"
      },
      "source": [
        "good_to_go"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZE-q4ziMImT"
      },
      "source": [
        "### Split the data into training and testing sets with the size of the test set being 30% of the records."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKFyBgjBMImU"
      },
      "source": [
        "x_train, x_test, y_train, y_test = tts(good_to_go, categories, test_size=0.3)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reegQu_5MImV"
      },
      "source": [
        "### Construct a pipeline that TF-IDF vectorizes the text and trains a Random Forest classification model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3JJ3hjNMImW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30f8c9cf-53e6-46a2-ca8d-23e93b1f8aab"
      },
      "source": [
        "pline = Pipeline([\r\n",
        "                  ('vect', CountVectorizer()),\r\n",
        "                  ('tfidf', TfidfTransformer()),\r\n",
        "                  ('clf', RandomForestClassifier(n_estimators=100))\r\n",
        "])\r\n",
        "\r\n",
        "pline.fit(x_train, y_train)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vect',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, vocabulary=Non...\n",
              "                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
              "                                        class_weight=None, criterion='gini',\n",
              "                                        max_depth=None, max_features='auto',\n",
              "                                        max_leaf_nodes=None, max_samples=None,\n",
              "                                        min_impurity_decrease=0.0,\n",
              "                                        min_impurity_split=None,\n",
              "                                        min_samples_leaf=1, min_samples_split=2,\n",
              "                                        min_weight_fraction_leaf=0.0,\n",
              "                                        n_estimators=100, n_jobs=None,\n",
              "                                        oob_score=False, random_state=None,\n",
              "                                        verbose=0, warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFt6djjpMImX"
      },
      "source": [
        "### Generate predictions on the test set and print a classification report to evaluate how well the model performed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWT99tvHMImY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "addbe2c7-b52c-4fab-b765-5dd61877a2e0"
      },
      "source": [
        "preds = pline.predict(x_test)\r\n",
        "print(classification_report(y_test, preds))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      health       0.85      0.92      0.88        12\n",
            "    politics       0.83      0.91      0.87        22\n",
            "      sports       0.86      0.80      0.83        15\n",
            "        tech       0.87      0.76      0.81        17\n",
            "\n",
            "    accuracy                           0.85        66\n",
            "   macro avg       0.85      0.85      0.85        66\n",
            "weighted avg       0.85      0.85      0.85        66\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2BwKeBNMImZ"
      },
      "source": [
        "### Perform 10-fold cross validation and obtain the averge F1 score across all the folds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n9cTvc6MIma",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f08ba20c-552d-4b9a-9833-d30a6434693e"
      },
      "source": [
        "scores = cross_val_score(pline, good_to_go, categories, cv=10, scoring='f1_macro')\r\n",
        "scores.mean()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7828500502765209"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1H9ERShiMImb"
      },
      "source": [
        "### Ingest, preprocess, and predict the topic of the article at the following URL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7piaPcKNMImd"
      },
      "source": [
        "import requests\r\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d2mfYGSMImc"
      },
      "source": [
        "url = 'https://www.nytimes.com/2019/11/25/business/uber-london.html'\r\n",
        "url_ = 'https://www.npr.org/2020/12/29/951010863/u-s-announces-new-rules-for-drones-and-their-operators'"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiLexUKIlQzQ"
      },
      "source": [
        "def get_text(url):\r\n",
        "  response = requests.get(url)\r\n",
        "  content = response.text\r\n",
        "\r\n",
        "  TAGS = [ 'h1','h2','h3','h4','h5','h6','h7','p','li']\r\n",
        "  soup = BeautifulSoup(content, 'lxml')\r\n",
        "  text_list = [tag.get_text() for tag in soup.find_all(TAGS)]\r\n",
        "  text = ' '.join(text_list)\r\n",
        "  return text"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Qkb36CK9mBru",
        "outputId": "b851b0d4-d677-4776-ab26-528592ee6d4d"
      },
      "source": [
        "text = get_text(url_)\r\n",
        "cleaned = preprocess([text])\r\n",
        "pline.predict(cleaned)[0]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'health'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2XAmnmWm_Tj"
      },
      "source": [
        "Accurate predicting the first article but not the second. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4NhCDfim-yz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}