{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "name": "NR Day 74, Lecture 1",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4I54bZzMImI"
      },
      "source": [
        "# Machine Learning: Text Classification Assignment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBhHdzKtMImK"
      },
      "source": [
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.pipeline import Pipeline\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "from nltk.corpus.reader.plaintext import CategorizedPlaintextCorpusReader\n",
        "import pandas as pd"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2tVDJJaMImN"
      },
      "source": [
        "### Use the CategorizedPlaintextCorpusReader to import the AP_News corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REtoZb_iMImO"
      },
      "source": [
        "path = 'AP_News/'\r\n",
        "DOC_PATTERN = r'.*\\.txt'\r\n",
        "CAT_PATTERN = r'([\\w_\\s]+)/.*'\r\n",
        "corpus = CategorizedPlaintextCorpusReader(path, DOC_PATTERN, cat_pattern=CAT_PATTERN)\r\n",
        "corpus.fileids()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQMuBvquMImP"
      },
      "source": [
        "### Create two separate lists - one containing the text from each document and another containing the category of each article in the corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jU4ZNM-MImQ"
      },
      "source": [
        "docs = [corpus.raw(fileid) for fileid in corpus.fileids()]\r\n",
        "docs[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gmg-36Sp4Onp"
      },
      "source": [
        "corpus.categories(corpus.fileids()[0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5r-4fnuZ4RoM"
      },
      "source": [
        "categories = [corpus.categories(fileid)[0] for fileid in corpus.fileids()]\r\n",
        "categories"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGRABGW8MImR"
      },
      "source": [
        "### Preprocess the corpus, ensuring to include the following steps.\n",
        "\n",
        "- Word tokenize the documents.\n",
        "- Lemmatize, stem, and lowercase all tokens.\n",
        "- Remove punctuation and stop words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hk-Nlze1MImS"
      },
      "source": [
        "def preprocess(docs):\r\n",
        "    lemmatizer = WordNetLemmatizer()\r\n",
        "    stemmer = SnowballStemmer('english')\r\n",
        "    preprocessed = []\r\n",
        "\r\n",
        "    for doc in docs:\r\n",
        "        tokenized = word_tokenize(doc)\r\n",
        "        cleaned = [stemmer.stem(lemmatizer.lemmatize(token.lower())) \r\n",
        "                   for token in tokenized \r\n",
        "                   if token.lower() not in stopwords.words('english')\r\n",
        "                  if token.isalpha()]\r\n",
        "        untokenized = \" \".join(cleaned)\r\n",
        "        preprocessed.append(untokenized)\r\n",
        "    \r\n",
        "    return preprocessed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzXpl1Ek4WP7"
      },
      "source": [
        "preprocessed = preprocess(docs)\r\n",
        "preprocessed[0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZE-q4ziMImT"
      },
      "source": [
        "### Split the data into training and testing sets with the size of the test set being 30% of the records."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKFyBgjBMImU"
      },
      "source": [
        "print(len(categories))\r\n",
        "len(preprocessed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqP19tP84dNz"
      },
      "source": [
        "X_train, X_test, y_train, y_test = tts(preprocessed, categories, test_size=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reegQu_5MImV"
      },
      "source": [
        "### Construct a pipeline that TF-IDF vectorizes the text and trains a Random Forest classification model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3JJ3hjNMImW"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "tfidf = TfidfVectorizer()\r\n",
        "rfc = RandomForestClassifier(n_estimators=100)\r\n",
        "pipe = Pipeline([\r\n",
        "    ('vect', tfidf),\r\n",
        "    ('clf', rfc)\r\n",
        "])\r\n",
        "\r\n",
        "pipe.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFt6djjpMImX"
      },
      "source": [
        "### Generate predictions on the test set and print a classification report to evaluate how well the model performed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWT99tvHMImY"
      },
      "source": [
        "y_pred_test = pipe.predict(X_test)\r\n",
        "print(classification_report(y_test, y_pred_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2BwKeBNMImZ"
      },
      "source": [
        "### Perform 10-fold cross validation and obtain the averge F1 score across all the folds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n9cTvc6MIma"
      },
      "source": [
        "\r\n",
        "scores = cross_val_score(pipe, preprocessed, categories, cv=10, scoring='f1_macro')\r\n",
        "scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1MX9vNX4liP"
      },
      "source": [
        "scores.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1H9ERShiMImb"
      },
      "source": [
        "### Ingest, preprocess, and predict the topic of the article at the following URL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "mJetDI1w4rQ3",
        "outputId": "c1824b6c-088f-472c-d191-f2ebbded602d"
      },
      "source": [
        "pipe.fit(preprocessed, categories)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d841004bb637>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'pipe' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d2mfYGSMImc"
      },
      "source": [
        "url = 'https://www.nytimes.com/2019/11/25/business/uber-london.html'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7piaPcKNMImd"
      },
      "source": [
        "\r\n",
        "import requests\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "\r\n",
        "result = requests.get(url)\r\n",
        "soup = BeautifulSoup(result.text)\r\n",
        "doc = soup.find('section', attrs={'name':'articleBody'}).text\r\n",
        "preprocessed = preprocess([doc])\r\n",
        "pipe.predict(preprocessed)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}