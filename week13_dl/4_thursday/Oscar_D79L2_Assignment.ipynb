{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "Oscar_D79L2_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQfS-31Jh-bL"
      },
      "source": [
        "## Recurrent Neural Networks\n",
        "\n",
        "In this assignment, we will learn about recurrent neural networks. We will create an RNN and learn to classify text data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOy993_Xh-bM"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToNRsZf5h-bP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9fb325c-cad4-4afb-f88a-9e68656c97f2"
      },
      "source": [
        "yelp = pd.read_csv('https://tf-assets-prod.s3.amazonaws.com/tf-curric/data-science/yelp_labeled.csv', error_bad_lines=False)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'Skipping line 281: expected 2 fields, saw 3\\nSkipping line 290: expected 2 fields, saw 3\\nSkipping line 296: expected 2 fields, saw 3\\nSkipping line 322: expected 2 fields, saw 3\\nSkipping line 373: expected 2 fields, saw 3\\nSkipping line 417: expected 2 fields, saw 3\\nSkipping line 427: expected 2 fields, saw 3\\nSkipping line 429: expected 2 fields, saw 3\\nSkipping line 577: expected 2 fields, saw 3\\nSkipping line 578: expected 2 fields, saw 3\\nSkipping line 611: expected 2 fields, saw 3\\nSkipping line 677: expected 2 fields, saw 3\\nSkipping line 771: expected 2 fields, saw 3\\nSkipping line 930: expected 2 fields, saw 3\\nSkipping line 979: expected 2 fields, saw 4\\nSkipping line 980: expected 2 fields, saw 3\\n'\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fKQfObOh-bX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "33713ed4-9a98-43b5-f8a8-05d21d048664"
      },
      "source": [
        "yelp"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wow... Loved this place.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Crust is not good.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Not tasty and the texture was just nasty.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Stopped by during the late May bank holiday of...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The selection on the menu was great and so wer...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>971</th>\n",
              "      <td>I think food should have flavor and texture an...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>972</th>\n",
              "      <td>Appetite instantly gone.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>973</th>\n",
              "      <td>Overall I was not impressed and would not go b...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>974</th>\n",
              "      <td>The whole experience was underwhelming and I t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>975</th>\n",
              "      <td>Then as if I hadn't wasted enough of my life t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>976 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  text  sentiment\n",
              "0                             Wow... Loved this place.          1\n",
              "1                                   Crust is not good.          0\n",
              "2            Not tasty and the texture was just nasty.          0\n",
              "3    Stopped by during the late May bank holiday of...          1\n",
              "4    The selection on the menu was great and so wer...          1\n",
              "..                                                 ...        ...\n",
              "971  I think food should have flavor and texture an...          0\n",
              "972                           Appetite instantly gone.          0\n",
              "973  Overall I was not impressed and would not go b...          0\n",
              "974  The whole experience was underwhelming and I t...          0\n",
              "975  Then as if I hadn't wasted enough of my life t...          0\n",
              "\n",
              "[976 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXyJFyo9h-bZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "896ea01a-65c2-44df-a4d9-1e7d14ab4675"
      },
      "source": [
        "yelp.sentiment.value_counts()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    494\n",
              "0    482\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bp0a4qU9h-bb"
      },
      "source": [
        "We have loaded a Yelp review dataset above. A positive sentiment is classified as 1 and a negative sentiment is classified as 0. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zyboex_Vh-bb"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def remove_stopwords(input_text):\n",
        "        stopwords_list = stopwords.words('english')\n",
        "        # Some words which might indicate a certain sentiment are kept via a whitelist\n",
        "        whitelist = [\"n't\", \"not\", \"no\"]\n",
        "        words = input_text.split() \n",
        "        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
        "        return \" \".join(clean_words)       \n",
        "\n",
        "def stem_list(word_list):\n",
        "    stemmed = []\n",
        "    for word in word_list:\n",
        "        stemmedword = stemmer.stem(word)\n",
        "        stemmed.append(stemmedword)\n",
        "    return stemmed\n",
        "\n",
        "def normalize(terms):\n",
        "    terms = terms.lower()\n",
        "    terms = remove_stopwords(terms)\n",
        "    word_delimiters = u'[\\\\[\\\\]\\n.!?,;:\\t\\\\-\\\\\"\\\\(\\\\)\\\\\\'\\u2019\\u2013 ]'\n",
        "    term_list = re.split(word_delimiters, terms)\n",
        "    trimmed = [x.rstrip() for x in term_list]\n",
        "    stemmed = stem_list(trimmed)\n",
        "    space = ' '\n",
        "    normed = space.join(stemmed)\n",
        "    normed = normed.replace('  ', ' ')\n",
        "    return normed"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsuGlOW8h-bd"
      },
      "source": [
        "In the code block above, we have functions to remove stopwords, stem, and normalize the text (remove special characters and trim white space). Apply the normalize function to every yelp review and assign the normalized text to a new column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDWbek-Yh-be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a26b0be-413a-457f-87d2-a5976e32dc11"
      },
      "source": [
        "# Answer below:\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8n859Dm8yASk"
      },
      "source": [
        "yelp['normalized_text'] = yelp['text'].map(lambda x: normalize(x))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tywlufjyh-bf"
      },
      "source": [
        "Next, use the one hot function for text encoding and encode the normalized text. Determine the vocabulary size to perform the encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUmqacGF6LZO",
        "outputId": "ca822358-5a03-4c06-ed69-4e6d9dfeaa77"
      },
      "source": [
        "from nltk import word_tokenize\n",
        "\n",
        "words = []\n",
        "for doc in yelp['normalized_text']:\n",
        "  for word in word_tokenize(doc):\n",
        "      words.append(word)\n",
        "\n",
        "unique = set(words)\n",
        "nb_unique_words = len(unique)\n",
        "nb_unique_words"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1626"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWKnOSd_h-bg"
      },
      "source": [
        "# Answer below:\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "\n",
        "vocab_size = nb_unique_words\n",
        "yelp['encoded'] = yelp['normalized_text'].apply(lambda x: one_hot(x, vocab_size))\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnRMy2S0h-bh"
      },
      "source": [
        "Convert the encoded sequences into a numpy array and make sure all reviews are the same length using the `pad_sequences` function in Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pk24Behch-bi"
      },
      "source": [
        "# Answer below:\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "encoded = np.array(pad_sequences(yelp['encoded']))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIclszsJh-bj"
      },
      "source": [
        "Split the data into train and test. Use 20% for test. The sentiment column should be used as the target variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlaZSo-Yh-bk"
      },
      "source": [
        "# Answer below:\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = encoded\n",
        "y = yelp['sentiment']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74pxWEsl2jyd",
        "outputId": "34ddd974-fec0-4a4a-ea4c-05e7df34dc2f"
      },
      "source": [
        "len(X_train[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "83"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orZwRX6o2t0a",
        "outputId": "ebaf55e8-8ef0-4724-86bb-6a6c22641085"
      },
      "source": [
        "len(X_test[0])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "83"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAsqg_REh-bm"
      },
      "source": [
        "Create a sequential model. The model should contain an embedding layer with input dim that is the size of the largest encoding in the vocabulary. The output dim should be 100, the input length is the number of columns in the training data. \n",
        "After the embedding layer, add a SimpleRNN layer with unit size 32, a dense layer of size 8 and a dense output layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssougKQUh-bm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9882225b-fc52-46db-9558-44fe4e035e3b"
      },
      "source": [
        "# Answer below:\n",
        "from tensorflow.keras.models import Sequential \n",
        "from tensorflow.keras.layers import Dense, Embedding, SimpleRNN\n",
        "\n",
        "max_words = nb_unique_words # grabs the largest numeric encoded value\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, output_dim=100, input_length=X_train.shape[1]))\n",
        "model.add(SimpleRNN(32))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 83, 100)           162600    \n",
            "_________________________________________________________________\n",
            "simple_rnn (SimpleRNN)       (None, 32)                4256      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 8)                 264       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 167,129\n",
            "Trainable params: 167,129\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjgqWnioh-bo"
      },
      "source": [
        "Compile using the optimizer of your choice, use crossentropy for your loss function. Fit the model using a batch size of 128 and 50 epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewNbf3fKh-bo"
      },
      "source": [
        "# Answer below:\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jk88z8R8h-bq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9592634-307d-4e82-f6fb-f128c7a756f5"
      },
      "source": [
        "model.fit(X_train, y_train, epochs=50, batch_size=128, validation_data=(X_test, y_test))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "7/7 [==============================] - 2s 115ms/step - loss: 0.6956 - accuracy: 0.4857 - val_loss: 0.6907 - val_accuracy: 0.5204\n",
            "Epoch 2/50\n",
            "7/7 [==============================] - 0s 37ms/step - loss: 0.6747 - accuracy: 0.7516 - val_loss: 0.6872 - val_accuracy: 0.5612\n",
            "Epoch 3/50\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 0.6511 - accuracy: 0.7856 - val_loss: 0.6857 - val_accuracy: 0.5510\n",
            "Epoch 4/50\n",
            "7/7 [==============================] - 0s 36ms/step - loss: 0.6210 - accuracy: 0.8626 - val_loss: 0.6733 - val_accuracy: 0.6276\n",
            "Epoch 5/50\n",
            "7/7 [==============================] - 0s 35ms/step - loss: 0.5725 - accuracy: 0.9019 - val_loss: 0.6988 - val_accuracy: 0.5153\n",
            "Epoch 6/50\n",
            "7/7 [==============================] - 0s 36ms/step - loss: 0.5363 - accuracy: 0.7801 - val_loss: 0.6652 - val_accuracy: 0.5408\n",
            "Epoch 7/50\n",
            "7/7 [==============================] - 0s 37ms/step - loss: 0.4807 - accuracy: 0.8668 - val_loss: 0.6296 - val_accuracy: 0.7194\n",
            "Epoch 8/50\n",
            "7/7 [==============================] - 0s 37ms/step - loss: 0.4078 - accuracy: 0.9676 - val_loss: 0.6130 - val_accuracy: 0.7245\n",
            "Epoch 9/50\n",
            "7/7 [==============================] - 0s 36ms/step - loss: 0.3446 - accuracy: 0.9646 - val_loss: 0.6100 - val_accuracy: 0.6276\n",
            "Epoch 10/50\n",
            "7/7 [==============================] - 0s 36ms/step - loss: 0.2998 - accuracy: 0.9509 - val_loss: 0.5834 - val_accuracy: 0.6888\n",
            "Epoch 11/50\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 0.2391 - accuracy: 0.9808 - val_loss: 0.6566 - val_accuracy: 0.5816\n",
            "Epoch 12/50\n",
            "7/7 [==============================] - 0s 35ms/step - loss: 0.2401 - accuracy: 0.9398 - val_loss: 0.5885 - val_accuracy: 0.7143\n",
            "Epoch 13/50\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 0.1787 - accuracy: 0.9839 - val_loss: 0.5699 - val_accuracy: 0.7245\n",
            "Epoch 14/50\n",
            "7/7 [==============================] - 0s 36ms/step - loss: 0.1450 - accuracy: 0.9856 - val_loss: 0.5702 - val_accuracy: 0.7092\n",
            "Epoch 15/50\n",
            "7/7 [==============================] - 0s 37ms/step - loss: 0.1193 - accuracy: 0.9895 - val_loss: 0.5633 - val_accuracy: 0.7398\n",
            "Epoch 16/50\n",
            "7/7 [==============================] - 0s 39ms/step - loss: 0.1034 - accuracy: 0.9872 - val_loss: 0.5622 - val_accuracy: 0.7194\n",
            "Epoch 17/50\n",
            "7/7 [==============================] - 0s 36ms/step - loss: 0.0822 - accuracy: 0.9927 - val_loss: 0.5928 - val_accuracy: 0.7143\n",
            "Epoch 18/50\n",
            "7/7 [==============================] - 0s 37ms/step - loss: 0.0759 - accuracy: 0.9918 - val_loss: 0.5715 - val_accuracy: 0.7092\n",
            "Epoch 19/50\n",
            "7/7 [==============================] - 0s 37ms/step - loss: 0.0623 - accuracy: 0.9965 - val_loss: 0.5740 - val_accuracy: 0.7194\n",
            "Epoch 20/50\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 0.0484 - accuracy: 0.9964 - val_loss: 0.5870 - val_accuracy: 0.7245\n",
            "Epoch 21/50\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 0.0604 - accuracy: 0.9875 - val_loss: 0.5986 - val_accuracy: 0.7041\n",
            "Epoch 22/50\n",
            "7/7 [==============================] - 0s 39ms/step - loss: 0.0481 - accuracy: 0.9954 - val_loss: 0.5960 - val_accuracy: 0.7194\n",
            "Epoch 23/50\n",
            "7/7 [==============================] - 0s 36ms/step - loss: 0.0460 - accuracy: 0.9909 - val_loss: 0.6068 - val_accuracy: 0.7347\n",
            "Epoch 24/50\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 0.0335 - accuracy: 0.9979 - val_loss: 0.6196 - val_accuracy: 0.7296\n",
            "Epoch 25/50\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 0.0288 - accuracy: 0.9963 - val_loss: 0.6299 - val_accuracy: 0.7194\n",
            "Epoch 26/50\n",
            "7/7 [==============================] - 0s 57ms/step - loss: 0.0367 - accuracy: 0.9903 - val_loss: 0.6585 - val_accuracy: 0.7194\n",
            "Epoch 27/50\n",
            "7/7 [==============================] - 0s 36ms/step - loss: 0.0321 - accuracy: 0.9963 - val_loss: 0.6250 - val_accuracy: 0.7194\n",
            "Epoch 28/50\n",
            "7/7 [==============================] - 0s 36ms/step - loss: 0.0335 - accuracy: 0.9917 - val_loss: 0.6100 - val_accuracy: 0.7296\n",
            "Epoch 29/50\n",
            "7/7 [==============================] - 0s 40ms/step - loss: 0.0299 - accuracy: 0.9946 - val_loss: 0.6306 - val_accuracy: 0.7194\n",
            "Epoch 30/50\n",
            "7/7 [==============================] - 0s 40ms/step - loss: 0.0244 - accuracy: 0.9981 - val_loss: 0.6476 - val_accuracy: 0.7143\n",
            "Epoch 31/50\n",
            "7/7 [==============================] - 0s 39ms/step - loss: 0.0225 - accuracy: 0.9976 - val_loss: 0.6464 - val_accuracy: 0.7092\n",
            "Epoch 32/50\n",
            "7/7 [==============================] - 0s 35ms/step - loss: 0.0190 - accuracy: 0.9982 - val_loss: 0.6576 - val_accuracy: 0.7143\n",
            "Epoch 33/50\n",
            "7/7 [==============================] - 0s 36ms/step - loss: 0.0162 - accuracy: 0.9981 - val_loss: 0.6688 - val_accuracy: 0.7143\n",
            "Epoch 34/50\n",
            "7/7 [==============================] - 0s 36ms/step - loss: 0.0166 - accuracy: 0.9970 - val_loss: 0.6825 - val_accuracy: 0.7143\n",
            "Epoch 35/50\n",
            "7/7 [==============================] - 0s 37ms/step - loss: 0.0185 - accuracy: 0.9946 - val_loss: 0.6929 - val_accuracy: 0.7143\n",
            "Epoch 36/50\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 0.0169 - accuracy: 0.9968 - val_loss: 0.6884 - val_accuracy: 0.7092\n",
            "Epoch 37/50\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 0.0143 - accuracy: 0.9970 - val_loss: 0.6962 - val_accuracy: 0.7245\n",
            "Epoch 38/50\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 0.0121 - accuracy: 0.9988 - val_loss: 0.6974 - val_accuracy: 0.7194\n",
            "Epoch 39/50\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 0.0107 - accuracy: 0.9992 - val_loss: 0.7466 - val_accuracy: 0.7245\n",
            "Epoch 40/50\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 0.0288 - accuracy: 0.9899 - val_loss: 0.7255 - val_accuracy: 0.7143\n",
            "Epoch 41/50\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 0.0166 - accuracy: 0.9963 - val_loss: 0.7373 - val_accuracy: 0.7041\n",
            "Epoch 42/50\n",
            "7/7 [==============================] - 0s 37ms/step - loss: 0.0146 - accuracy: 0.9964 - val_loss: 0.7354 - val_accuracy: 0.7041\n",
            "Epoch 43/50\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.0145 - accuracy: 0.9969 - val_loss: 0.7494 - val_accuracy: 0.7143\n",
            "Epoch 44/50\n",
            "7/7 [==============================] - 0s 39ms/step - loss: 0.0105 - accuracy: 0.9979 - val_loss: 0.7585 - val_accuracy: 0.7143\n",
            "Epoch 45/50\n",
            "7/7 [==============================] - 0s 37ms/step - loss: 0.0125 - accuracy: 0.9954 - val_loss: 0.7651 - val_accuracy: 0.7245\n",
            "Epoch 46/50\n",
            "7/7 [==============================] - 0s 37ms/step - loss: 0.0134 - accuracy: 0.9979 - val_loss: 0.7462 - val_accuracy: 0.7143\n",
            "Epoch 47/50\n",
            "7/7 [==============================] - 0s 40ms/step - loss: 0.0137 - accuracy: 0.9979 - val_loss: 0.7585 - val_accuracy: 0.7296\n",
            "Epoch 48/50\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 0.0108 - accuracy: 0.9970 - val_loss: 0.7580 - val_accuracy: 0.7092\n",
            "Epoch 49/50\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 0.0138 - accuracy: 0.9956 - val_loss: 0.7706 - val_accuracy: 0.7194\n",
            "Epoch 50/50\n",
            "7/7 [==============================] - 0s 36ms/step - loss: 0.0087 - accuracy: 0.9984 - val_loss: 0.7673 - val_accuracy: 0.7194\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f914b86f3c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJ2BnIIL5DlQ"
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    }
  ]
}