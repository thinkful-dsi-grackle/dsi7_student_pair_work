{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "Day_79_L2_Assignment_Barrett.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQfS-31Jh-bL"
      },
      "source": [
        "## Recurrent Neural Networks\n",
        "\n",
        "In this assignment, we will learn about recurrent neural networks. We will create an RNN and learn to classify text data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOy993_Xh-bM"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, SimpleRNN\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYuMUpHUdlFJ",
        "outputId": "f7fff088-1e57-4236-a231-e75517b4f6cc"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToNRsZf5h-bP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc6c7077-f1cb-44a5-c429-26710b0f8186"
      },
      "source": [
        "yelp = pd.read_csv('https://tf-assets-prod.s3.amazonaws.com/tf-curric/data-science/yelp_labeled.csv', error_bad_lines=False)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'Skipping line 281: expected 2 fields, saw 3\\nSkipping line 290: expected 2 fields, saw 3\\nSkipping line 296: expected 2 fields, saw 3\\nSkipping line 322: expected 2 fields, saw 3\\nSkipping line 373: expected 2 fields, saw 3\\nSkipping line 417: expected 2 fields, saw 3\\nSkipping line 427: expected 2 fields, saw 3\\nSkipping line 429: expected 2 fields, saw 3\\nSkipping line 577: expected 2 fields, saw 3\\nSkipping line 578: expected 2 fields, saw 3\\nSkipping line 611: expected 2 fields, saw 3\\nSkipping line 677: expected 2 fields, saw 3\\nSkipping line 771: expected 2 fields, saw 3\\nSkipping line 930: expected 2 fields, saw 3\\nSkipping line 979: expected 2 fields, saw 4\\nSkipping line 980: expected 2 fields, saw 3\\n'\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fKQfObOh-bX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "41fbd0a4-3e07-48b5-cea3-95e6ab71503d"
      },
      "source": [
        "yelp.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wow... Loved this place.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Crust is not good.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Not tasty and the texture was just nasty.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Stopped by during the late May bank holiday of...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The selection on the menu was great and so wer...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  sentiment\n",
              "0                           Wow... Loved this place.          1\n",
              "1                                 Crust is not good.          0\n",
              "2          Not tasty and the texture was just nasty.          0\n",
              "3  Stopped by during the late May bank holiday of...          1\n",
              "4  The selection on the menu was great and so wer...          1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXyJFyo9h-bZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b708d73-b645-4834-fd98-920936314234"
      },
      "source": [
        "yelp.sentiment.value_counts()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    494\n",
              "0    482\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bp0a4qU9h-bb"
      },
      "source": [
        "We have loaded a Yelp review dataset above. A positive sentiment is classified as 1 and a negative sentiment is classified as 0. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zyboex_Vh-bb"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def remove_stopwords(input_text):\n",
        "        stopwords_list = stopwords.words('english')\n",
        "        # Some words which might indicate a certain sentiment are kept via a whitelist\n",
        "        whitelist = [\"n't\", \"not\", \"no\"]\n",
        "        words = input_text.split() \n",
        "        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
        "        return \" \".join(clean_words)       \n",
        "\n",
        "def stem_list(word_list):\n",
        "    stemmed = []\n",
        "    for word in word_list:\n",
        "        stemmedword = stemmer.stem(word)\n",
        "        stemmed.append(stemmedword)\n",
        "    return stemmed\n",
        "\n",
        "def normalize(terms):\n",
        "    terms = terms.lower()\n",
        "    terms = remove_stopwords(terms)\n",
        "    word_delimiters = u'[\\\\[\\\\]\\n.!?,;:\\t\\\\-\\\\\"\\\\(\\\\)\\\\\\'\\u2019\\u2013 ]'\n",
        "    term_list = re.split(word_delimiters, terms)\n",
        "    trimmed = [x.strip() for x in term_list]\n",
        "    stemmed = stem_list(trimmed)\n",
        "    space = ' '\n",
        "    normed = space.join(stemmed)\n",
        "    normed = normed.replace('  ', ' ').replace('  ', ' ')\n",
        "    return normed.strip()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsuGlOW8h-bd"
      },
      "source": [
        "In the code block above, we have functions to remove stopwords, stem, and normalize the text (remove special characters and trim white space). Apply the normalize function to every yelp review and assign the normalized text to a new column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDWbek-Yh-be"
      },
      "source": [
        "# Answer below:\n",
        "yelp['normalized'] = yelp['text'].apply(normalize)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXuvTyAPgrg8",
        "outputId": "6c4ca709-6818-4def-b37b-0fb6ec6c3906"
      },
      "source": [
        "yelp['normalized']"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                         wow love place\n",
              "1                                         crust not good\n",
              "2                                 not tasti textur nasti\n",
              "3      stop late may bank holiday rick steve recommen...\n",
              "4                                select menu great price\n",
              "                             ...                        \n",
              "971                        think food flavor textur lack\n",
              "972                               appetit instantli gone\n",
              "973                 overal not impress would not go back\n",
              "974    whole experi underwhelm think we ll go ninja s...\n",
              "975    wast enough life pour salt wound draw time too...\n",
              "Name: normalized, Length: 976, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tywlufjyh-bf"
      },
      "source": [
        "Next, use the one hot function for text encoding and encode the normalized text. Determine the vocabulary size to perform the encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWKnOSd_h-bg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8eea655e-41cf-4b08-d4b7-5b2a9de6819a"
      },
      "source": [
        "# Answer below:\n",
        "vocab_list = list(set([word for sent in [sent.split(' ') for sent in yelp['normalized']] for word in sent]))\n",
        "vocab_size = len(vocab_list)\n",
        "vocab_size"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1629"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0FwsKW2kDRX",
        "outputId": "6453cb93-3e9c-4d70-e399-3857375cd84a"
      },
      "source": [
        "encoded_sents = [one_hot(sent, vocab_size) for sent in yelp['normalized']]\n",
        "encoded_sents[:5]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[840, 436, 54],\n",
              " [1234, 1069, 1167],\n",
              " [1069, 1241, 774, 663],\n",
              " [1336, 879, 19, 625, 639, 1237, 949, 913, 436, 1020],\n",
              " [688, 375, 284, 1469]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnRMy2S0h-bh"
      },
      "source": [
        "Convert the encoded sequences into a numpy array and make sure all reviews are the same length using the `pad_sequences` function in Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pk24Behch-bi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b59f634-3219-4a74-d46c-ca24e3643155"
      },
      "source": [
        "# Answer below:\n",
        "ind_vars = pad_sequences(encoded_sents)\n",
        "ind_vars[:5]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,  840,  436,   54],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0, 1234, 1069, 1167],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0, 1069, 1241,  774,  663],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0, 1336,  879,   19,  625,\n",
              "         639, 1237,  949,  913,  436, 1020],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,  688,  375,  284, 1469]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIclszsJh-bj"
      },
      "source": [
        "Split the data into train and test. Use 20% for test. The sentiment column should be used as the target variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlaZSo-Yh-bk"
      },
      "source": [
        "# Answer below:\n",
        "X_train, X_test, y_train, y_test = train_test_split(ind_vars, yelp['sentiment'], test_size=0.2)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pi5GBoqYlyMe",
        "outputId": "1c51daf2-bc65-4970-bb8d-a5c44f0f90f0"
      },
      "source": [
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((780, 83), (196, 83), (780,), (196,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAsqg_REh-bm"
      },
      "source": [
        "Create a sequential model. The model should contain an embedding layer with input dim that is the size of the largest encoding in the vocabulary. The output dim should be 100, the input length is the number of columns in the training data. \n",
        "After the embedding layer, add a SimpleRNN layer with unit size 32, a dense layer of size 8 and a dense output layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssougKQUh-bm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac35d274-ff48-4380-812c-5a3a390f8dc6"
      },
      "source": [
        "# Answer below:\n",
        "max_words = np.max(ind_vars) #grabs largest numeric encoded value\n",
        "max_words"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1627"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BM4LrsN-lhh-",
        "outputId": "9222b242-4d1d-426d-acd7-12833aad3d71"
      },
      "source": [
        "ind_vars.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(976, 83)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWIDy4VVlaso",
        "outputId": "ec5c1afd-3941-4306-bb19-0a2bb62eccf4"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100, input_length = ind_vars.shape[1])) # 100 is output of embedding layer\n",
        "model.add(SimpleRNN(32))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 83, 100)           162900    \n",
            "_________________________________________________________________\n",
            "simple_rnn (SimpleRNN)       (None, 32)                4256      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 8)                 264       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 167,429\n",
            "Trainable params: 167,429\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjgqWnioh-bo"
      },
      "source": [
        "Compile using the optimizer of your choice, use crossentropy for your loss function. Fit the model using a batch size of 128 and 50 epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewNbf3fKh-bo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5160d36-ffbf-402e-e6cc-02b9565f0863"
      },
      "source": [
        "# Answer below:\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=128,\n",
        "          validation_data=(X_test, y_test))\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "7/7 [==============================] - 2s 155ms/step - loss: 0.6960 - accuracy: 0.4672 - val_loss: 0.6937 - val_accuracy: 0.5408\n",
            "Epoch 2/50\n",
            "7/7 [==============================] - 1s 75ms/step - loss: 0.6651 - accuracy: 0.6707 - val_loss: 0.6751 - val_accuracy: 0.5561\n",
            "Epoch 3/50\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.6223 - accuracy: 0.7642 - val_loss: 0.6685 - val_accuracy: 0.5867\n",
            "Epoch 4/50\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.5893 - accuracy: 0.7887 - val_loss: 0.6591 - val_accuracy: 0.6173\n",
            "Epoch 5/50\n",
            "7/7 [==============================] - 1s 77ms/step - loss: 0.5324 - accuracy: 0.9041 - val_loss: 0.6453 - val_accuracy: 0.6684\n",
            "Epoch 6/50\n",
            "7/7 [==============================] - 1s 85ms/step - loss: 0.4819 - accuracy: 0.9256 - val_loss: 0.6259 - val_accuracy: 0.6888\n",
            "Epoch 7/50\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.4202 - accuracy: 0.9524 - val_loss: 0.6038 - val_accuracy: 0.7245\n",
            "Epoch 8/50\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.3521 - accuracy: 0.9691 - val_loss: 0.6029 - val_accuracy: 0.6939\n",
            "Epoch 9/50\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.2873 - accuracy: 0.9648 - val_loss: 0.5717 - val_accuracy: 0.7296\n",
            "Epoch 10/50\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.2306 - accuracy: 0.9761 - val_loss: 0.5622 - val_accuracy: 0.7296\n",
            "Epoch 11/50\n",
            "7/7 [==============================] - 1s 79ms/step - loss: 0.1911 - accuracy: 0.9852 - val_loss: 0.5630 - val_accuracy: 0.7092\n",
            "Epoch 12/50\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.1519 - accuracy: 0.9835 - val_loss: 0.5645 - val_accuracy: 0.7347\n",
            "Epoch 13/50\n",
            "7/7 [==============================] - 1s 81ms/step - loss: 0.1250 - accuracy: 0.9869 - val_loss: 0.5837 - val_accuracy: 0.7245\n",
            "Epoch 14/50\n",
            "7/7 [==============================] - 1s 85ms/step - loss: 0.1094 - accuracy: 0.9851 - val_loss: 0.5651 - val_accuracy: 0.7296\n",
            "Epoch 15/50\n",
            "7/7 [==============================] - 1s 85ms/step - loss: 0.0951 - accuracy: 0.9884 - val_loss: 0.5811 - val_accuracy: 0.7041\n",
            "Epoch 16/50\n",
            "7/7 [==============================] - 1s 77ms/step - loss: 0.0744 - accuracy: 0.9945 - val_loss: 0.6048 - val_accuracy: 0.6888\n",
            "Epoch 17/50\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0650 - accuracy: 0.9901 - val_loss: 0.6485 - val_accuracy: 0.7296\n",
            "Epoch 18/50\n",
            "7/7 [==============================] - 1s 83ms/step - loss: 0.0751 - accuracy: 0.9907 - val_loss: 0.5863 - val_accuracy: 0.7143\n",
            "Epoch 19/50\n",
            "7/7 [==============================] - 1s 86ms/step - loss: 0.0552 - accuracy: 0.9949 - val_loss: 0.6070 - val_accuracy: 0.6837\n",
            "Epoch 20/50\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0565 - accuracy: 0.9921 - val_loss: 0.6006 - val_accuracy: 0.7143\n",
            "Epoch 21/50\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0450 - accuracy: 0.9946 - val_loss: 0.6238 - val_accuracy: 0.6990\n",
            "Epoch 22/50\n",
            "7/7 [==============================] - 1s 75ms/step - loss: 0.0385 - accuracy: 0.9959 - val_loss: 0.6491 - val_accuracy: 0.6735\n",
            "Epoch 23/50\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0340 - accuracy: 0.9960 - val_loss: 0.6544 - val_accuracy: 0.7245\n",
            "Epoch 24/50\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0326 - accuracy: 0.9988 - val_loss: 0.6929 - val_accuracy: 0.6735\n",
            "Epoch 25/50\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0277 - accuracy: 0.9951 - val_loss: 0.6913 - val_accuracy: 0.6939\n",
            "Epoch 26/50\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0210 - accuracy: 0.9990 - val_loss: 0.7201 - val_accuracy: 0.6786\n",
            "Epoch 27/50\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0269 - accuracy: 0.9944 - val_loss: 0.7269 - val_accuracy: 0.6888\n",
            "Epoch 28/50\n",
            "7/7 [==============================] - 1s 86ms/step - loss: 0.0207 - accuracy: 0.9951 - val_loss: 0.7161 - val_accuracy: 0.6939\n",
            "Epoch 29/50\n",
            "7/7 [==============================] - 1s 85ms/step - loss: 0.0204 - accuracy: 0.9978 - val_loss: 0.7283 - val_accuracy: 0.6786\n",
            "Epoch 30/50\n",
            "7/7 [==============================] - 0s 73ms/step - loss: 0.0213 - accuracy: 0.9951 - val_loss: 0.7416 - val_accuracy: 0.6786\n",
            "Epoch 31/50\n",
            "7/7 [==============================] - 1s 81ms/step - loss: 0.0218 - accuracy: 0.9964 - val_loss: 0.7346 - val_accuracy: 0.7041\n",
            "Epoch 32/50\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0187 - accuracy: 0.9970 - val_loss: 0.7471 - val_accuracy: 0.6990\n",
            "Epoch 33/50\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0177 - accuracy: 0.9976 - val_loss: 0.7667 - val_accuracy: 0.6888\n",
            "Epoch 34/50\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0161 - accuracy: 0.9970 - val_loss: 0.7731 - val_accuracy: 0.6888\n",
            "Epoch 35/50\n",
            "7/7 [==============================] - 1s 76ms/step - loss: 0.0130 - accuracy: 0.9981 - val_loss: 0.7764 - val_accuracy: 0.6888\n",
            "Epoch 36/50\n",
            "7/7 [==============================] - 1s 86ms/step - loss: 0.0144 - accuracy: 0.9973 - val_loss: 0.7896 - val_accuracy: 0.6888\n",
            "Epoch 37/50\n",
            "7/7 [==============================] - 1s 79ms/step - loss: 0.0131 - accuracy: 0.9983 - val_loss: 0.7990 - val_accuracy: 0.6888\n",
            "Epoch 38/50\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0127 - accuracy: 0.9970 - val_loss: 0.8039 - val_accuracy: 0.6837\n",
            "Epoch 39/50\n",
            "7/7 [==============================] - 1s 85ms/step - loss: 0.0112 - accuracy: 0.9976 - val_loss: 0.8168 - val_accuracy: 0.6888\n",
            "Epoch 40/50\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0105 - accuracy: 0.9978 - val_loss: 0.8392 - val_accuracy: 0.6888\n",
            "Epoch 41/50\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0144 - accuracy: 0.9936 - val_loss: 0.8636 - val_accuracy: 0.6939\n",
            "Epoch 42/50\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0107 - accuracy: 0.9970 - val_loss: 0.8373 - val_accuracy: 0.7041\n",
            "Epoch 43/50\n",
            "7/7 [==============================] - 1s 78ms/step - loss: 0.0107 - accuracy: 0.9984 - val_loss: 0.8519 - val_accuracy: 0.6888\n",
            "Epoch 44/50\n",
            "7/7 [==============================] - 1s 85ms/step - loss: 0.0086 - accuracy: 0.9986 - val_loss: 0.8578 - val_accuracy: 0.6888\n",
            "Epoch 45/50\n",
            "7/7 [==============================] - 1s 75ms/step - loss: 0.0082 - accuracy: 0.9983 - val_loss: 0.8558 - val_accuracy: 0.6888\n",
            "Epoch 46/50\n",
            "7/7 [==============================] - 1s 76ms/step - loss: 0.0104 - accuracy: 0.9968 - val_loss: 0.8630 - val_accuracy: 0.6939\n",
            "Epoch 47/50\n",
            "7/7 [==============================] - 1s 81ms/step - loss: 0.0096 - accuracy: 0.9983 - val_loss: 0.8837 - val_accuracy: 0.6888\n",
            "Epoch 48/50\n",
            "7/7 [==============================] - 1s 87ms/step - loss: 0.0106 - accuracy: 0.9968 - val_loss: 0.8851 - val_accuracy: 0.6990\n",
            "Epoch 49/50\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0111 - accuracy: 0.9960 - val_loss: 0.9067 - val_accuracy: 0.6837\n",
            "Epoch 50/50\n",
            "7/7 [==============================] - 1s 75ms/step - loss: 0.0092 - accuracy: 0.9978 - val_loss: 0.9042 - val_accuracy: 0.6939\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f56a51dd780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jk88z8R8h-bq"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzoY9LxssDBw"
      },
      "source": [
        "#Lecture Notes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GT_9_TuGEPeM"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import one_hot"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaXQyeHxEUOo"
      },
      "source": [
        "# define documents\n",
        "docs = ['Well done!',\n",
        "        'Good work',\n",
        "        'Great effort',\n",
        "        'nice work',\n",
        "        'Excellent!',\n",
        "        'Weak',\n",
        "        'Poor effort!',\n",
        "        'not good',\n",
        "        'poor work',\n",
        "        'Could have done better.']"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rdtzm643Gf2C"
      },
      "source": [
        "target = [1,1,1,1,1,0,0,0,0,0]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3i5H7pPUEoa1"
      },
      "source": [
        "vocab_size = 100\n",
        "encoded_docs = [one_hot(doc, vocab_size) for doc in docs]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0krgAvSGEvhF",
        "outputId": "42a0b3b2-fb08-477a-d6cc-f5be3dc75dc1"
      },
      "source": [
        "encoded_docs"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[32, 44],\n",
              " [67, 75],\n",
              " [53, 74],\n",
              " [77, 75],\n",
              " [29],\n",
              " [88],\n",
              " [67, 74],\n",
              " [68, 67],\n",
              " [67, 75],\n",
              " [18, 38, 44, 83]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mk7P-VhfEwfK"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bC9hkqJFCCv"
      },
      "source": [
        "ind_vars = pad_sequences(encoded_docs)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpxTrCGaFFet",
        "outputId": "6d5ead91-c693-4d1e-b1dd-957e9cdd1e57"
      },
      "source": [
        "ind_vars"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0, 32, 44],\n",
              "       [ 0,  0, 67, 75],\n",
              "       [ 0,  0, 53, 74],\n",
              "       [ 0,  0, 77, 75],\n",
              "       [ 0,  0,  0, 29],\n",
              "       [ 0,  0,  0, 88],\n",
              "       [ 0,  0, 67, 74],\n",
              "       [ 0,  0, 68, 67],\n",
              "       [ 0,  0, 67, 75],\n",
              "       [18, 38, 44, 83]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVXo_hi_GIb_",
        "outputId": "0a307482-b1ff-4ff3-fcd5-520e91f47700"
      },
      "source": [
        "ind_vars.shape"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd5rUZ3iFGMr"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, SimpleRNN"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZheOT8nFfL2",
        "outputId": "5da2a3a2-d601-46a7-f205-db4f5cd0b716"
      },
      "source": [
        "max_words = np.max(ind_vars) #grabs largest numeric encoded value\n",
        "max_words"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "88"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9tNhH7tFqxQ",
        "outputId": "43666375-b7ad-456b-f4e4-e689105c4148"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(max_words, 100, input_length = ind_vars.shape[1])) # 100 is output of embedding layer\n",
        "model.add(SimpleRNN(32))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 4, 100)            8800      \n",
            "_________________________________________________________________\n",
            "simple_rnn_1 (SimpleRNN)     (None, 32)                4256      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 8)                 264       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 13,329\n",
            "Trainable params: 13,329\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHdXzmAEGXcH"
      },
      "source": [
        "# Fake data so doesn't work, but this is the main idea.\n",
        "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# model.fit(ind_vars, np.array(target), epochs=1, batch_size=1)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRIuK7KrHi3y"
      },
      "source": [
        ""
      ],
      "execution_count": 30,
      "outputs": []
    }
  ]
}