{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Kalika Kay 52 Lecture 2 Pipeline Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzcLuZowoMar"
      },
      "source": [
        "# Machine Learning Best Practices Assignment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpBBfm3toMat"
      },
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfqS_cMWoMa1"
      },
      "source": [
        "### Import the [Pima Indians Diabetes data set](https://tf-assets-prod.s3.amazonaws.com/tf-curric/data-science/pima_indians_diabetes.csv)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qO-4LVM7oMa1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6010c5d0-f367-4e6b-c514-5dbedda288f3"
      },
      "source": [
        "data = pd.read_csv('https://tf-assets-prod.s3.amazonaws.com/tf-curric/data-science/pima_indians_diabetes.csv')\n",
        "data.shape"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(768, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtNhl2RCoMa5"
      },
      "source": [
        "### Split the data into training and test sets, with the target variable being the outcome column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awGD68-2oMa5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9617ef24-3b62-4576-d309-46d43669881e"
      },
      "source": [
        "y = data['outcome']\n",
        "X = data.drop(columns=['outcome']).copy()\n",
        "\n",
        "#Split the data.\n",
        "SIZE = 0.2\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=SIZE)\n",
        "print('There are {:d} training samples and {:d} test samples'.format(X_train.shape[0], X_test.shape[0]))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 614 training samples and 154 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6uDW6gSoMa9"
      },
      "source": [
        "### Train a Random Forest Classifier on the data without doing any transformations and print a classification report.\n",
        "\n",
        "This will provide us with a basis for comparison."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_TU-W51oMa-"
      },
      "source": [
        "model = RandomForestClassifier(random_state=1)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9o8P7OeooMbC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aab4372d-56ab-4131-bbf9-4d60f3388db1"
      },
      "source": [
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.83      0.85       105\n",
            "           1       0.67      0.73      0.70        49\n",
            "\n",
            "    accuracy                           0.80       154\n",
            "   macro avg       0.77      0.78      0.77       154\n",
            "weighted avg       0.81      0.80      0.80       154\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44gMhQKRoMbG"
      },
      "source": [
        "### Reduce the data down to 3 dimensions using PCA. Then do the train-test split, fit the model, and print a classification report.\n",
        "\n",
        "Compare these results to the previous ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbJpUBrgoMbH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f67014f-f742-4df5-aaee-4b265d76c0a2"
      },
      "source": [
        "#principle component analysis, scree plot.\n",
        "pca = PCA(n_components = 3).fit_transform(X)\n",
        "\n",
        "#Split the data.\n",
        "SIZE = 0.2\n",
        "pca_train, pca_test, y_train, y_test = train_test_split(pca, y, test_size=SIZE)\n",
        "print('There are {:d} training samples and {:d} test samples'.format(pca_train.shape[0], pca_test.shape[0]))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 614 training samples and 154 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVsxmGop7BDu",
        "outputId": "e3d97899-3af2-47ca-b0e5-a18ea17d98f5"
      },
      "source": [
        "model.fit(pca_train, y_train)\n",
        "y_pred = model.predict(pca_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.83      0.80        99\n",
            "           1       0.65      0.56      0.60        55\n",
            "\n",
            "    accuracy                           0.73       154\n",
            "   macro avg       0.71      0.70      0.70       154\n",
            "weighted avg       0.73      0.73      0.73       154\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PSJ9pLRoMbL"
      },
      "source": [
        "### Fit the model and print a classification report again, but this time, perform the train-test split before you transform the data using PCA.\n",
        "\n",
        "Compare these results to the previous ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2kjPbPKoMbM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49f3886b-8b8b-4287-c759-b526d17a3729"
      },
      "source": [
        "#Split the data.\n",
        "SIZE = 0.2\n",
        "pca_train, pca_test, y_train, y_test = train_test_split(X, y, test_size=SIZE)\n",
        "print('There are {:d} training samples and {:d} test samples'.format(pca_train.shape[0], pca_test.shape[0]))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 614 training samples and 154 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTIaexBT7jQl",
        "outputId": "26b7d142-1699-4df8-e5a3-87b1c95f5331"
      },
      "source": [
        "#principle component analysis\n",
        "pca = PCA(n_components = 3)\n",
        "train_comp = pca.fit_transform(pca_train)\n",
        "test_comp = pca.transform(pca_test)\n",
        "\n",
        "model.fit(pca_train, y_train)\n",
        "y_pred = model.predict(pca_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.88      0.84        99\n",
            "           1       0.74      0.62      0.67        55\n",
            "\n",
            "    accuracy                           0.79       154\n",
            "   macro avg       0.77      0.75      0.76       154\n",
            "weighted avg       0.78      0.79      0.78       154\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5QOmml0oMbO"
      },
      "source": [
        "### Using the Random Forest Classifier, perform 10-fold cross validation on the training set and print the mean cross validation score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ci-V57_oMbP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bb49913-4398-49c5-bcf2-bc15d8873409"
      },
      "source": [
        "score_train = cross_val_score(model, X_train, y_train, cv=10)\n",
        "print(\"training: {}\".format(score_train.mean()))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training: 0.6596509783183502\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7W_6glHPoMbT"
      },
      "source": [
        "### Create a pipeline with a PCA step and a Random Forest Classifier step. Perform the train-test split again, fit the pipeline, and then generate a classification report.\n",
        "\n",
        "Compare these results to the previous ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sy4DFzsoMbU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8b30c41-4dab-406c-d5c9-6befb11939d4"
      },
      "source": [
        "pipeline = Pipeline([('pca', PCA(n_components=3)),\n",
        "                     ('rf', RandomForestClassifier(random_state=1))])\n",
        "\n",
        "#Split the data.\n",
        "SIZE = 0.2\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=SIZE)\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.87      0.80       101\n",
            "           1       0.64      0.43      0.52        53\n",
            "\n",
            "    accuracy                           0.72       154\n",
            "   macro avg       0.69      0.65      0.66       154\n",
            "weighted avg       0.71      0.72      0.70       154\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeeU5gVq-ywx"
      },
      "source": [
        "I have to omit the very first one due to the support balance. In comparison o  the others - the score looks better; actually. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0RpnV0xoMbX"
      },
      "source": [
        "### Using the pipeline you built, perform 10-fold cross validation on the training set and print the mean cross validation score.\n",
        "\n",
        "How does this score compare to the previous one?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IKeBBZXoMbY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d03618e5-cdba-4448-e207-5f9855145d48"
      },
      "source": [
        "score_train = cross_val_score(pipeline, X_train, y_train, cv=10)\n",
        "print(\"training: {}\".format(score_train.mean()))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training: 0.7246694870438921\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DLDuyOqoMba"
      },
      "source": [
        "### Use GridSearchCV to find the optimal set of parameters from the ones below.\n",
        "\n",
        "- PCA Number of Components: 2, 3, 4, 5, 6, 7, 8\n",
        "- Random Forest Number of Estimators: 10, 20, 50, 100, 200"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbsRhIdZoMbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d3de745-754c-4e73-85c9-bdf69e66de0d"
      },
      "source": [
        "pipeline = Pipeline([('pca', PCA()),\n",
        "                     ('rf', RandomForestClassifier(random_state=1))])\n",
        "params = { 'pca__n_components' :  [2, 3, 4, 5, 6, 7, 8],\n",
        "           'rf__n_estimators' : [10, 20, 50, 100, 200]\n",
        "          }\n",
        "\n",
        "search = GridSearchCV(pipeline, params, cv=10)\n",
        "search.fit(X_train, y_train)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=10, error_score=nan,\n",
              "             estimator=Pipeline(memory=None,\n",
              "                                steps=[('pca',\n",
              "                                        PCA(copy=True, iterated_power='auto',\n",
              "                                            n_components=None,\n",
              "                                            random_state=None,\n",
              "                                            svd_solver='auto', tol=0.0,\n",
              "                                            whiten=False)),\n",
              "                                       ('rf',\n",
              "                                        RandomForestClassifier(bootstrap=True,\n",
              "                                                               ccp_alpha=0.0,\n",
              "                                                               class_weight=None,\n",
              "                                                               criterion='gini',\n",
              "                                                               max_depth=None,\n",
              "                                                               max_features='auto',\n",
              "                                                               max_leaf_nodes=None,\n",
              "                                                               max_sampl...\n",
              "                                                               min_samples_split=2,\n",
              "                                                               min_weight_fraction_leaf=0.0,\n",
              "                                                               n_estimators=100,\n",
              "                                                               n_jobs=None,\n",
              "                                                               oob_score=False,\n",
              "                                                               random_state=1,\n",
              "                                                               verbose=0,\n",
              "                                                               warm_start=False))],\n",
              "                                verbose=False),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'pca__n_components': [2, 3, 4, 5, 6, 7, 8],\n",
              "                         'rf__n_estimators': [10, 20, 50, 100, 200]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cVJ994qoMbd"
      },
      "source": [
        "### Using the best estimator pipeline from above, fit the pipeline to the training set and generate a classification report showing the results.\n",
        "\n",
        "Compare these results to the previous ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0N6Mv3KoMbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "087e55c8-dd74-4669-f370-6cbbd3ac4177"
      },
      "source": [
        "pipeline = search.best_estimator_\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.83      0.80       101\n",
            "           1       0.61      0.51      0.56        53\n",
            "\n",
            "    accuracy                           0.72       154\n",
            "   macro avg       0.69      0.67      0.68       154\n",
            "weighted avg       0.71      0.72      0.71       154\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5SijtopEQ8Q"
      },
      "source": [
        "Better than some worse than a few others. Not bad. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loHniiPLoMbj"
      },
      "source": [
        "### Fit the best estimator pipeline to the entire data set and save your model to disk using pickle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Y8ESntxoMbk"
      },
      "source": [
        "pipeline.fit(X,y)\n",
        "\n",
        "with open(\"model.pkl\", 'wb') as f:\n",
        "  pickle.dump(pipeline, f)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HdA-rQGoMbn"
      },
      "source": [
        "### Load the model you saved to disk, create a copy of the features in the data, and generate a set of predictions for those features. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBnP7dKhoMbn"
      },
      "source": [
        "from sklearn.utils import resample\n",
        "new_data = resample(data, random_state=42, replace=True, n_samples=2800)\n",
        "py = new_data['outcome']\n",
        "nX = new_data.drop(columns='outcome').copy()\n",
        "\n",
        "with open(\"model.pkl\", 'rb') as f:\n",
        "  loaded_pipe = pickle.load(f)\n",
        "\n",
        "preds = loaded_pipe.predict(nX)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxKSO9PeFY75",
        "outputId": "630d3fd1-c978-4b1d-da84-3eaaa56e6890"
      },
      "source": [
        "print(classification_report(py, preds))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      1812\n",
            "           1       1.00      1.00      1.00       988\n",
            "\n",
            "    accuracy                           1.00      2800\n",
            "   macro avg       1.00      1.00      1.00      2800\n",
            "weighted avg       1.00      1.00      1.00      2800\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cG4f8kY_FxHD"
      },
      "source": [
        "This would probably be more fun with a larger data set, one that had a subset prior to running through this whole process.\n",
        "\n",
        "I forgot how large you said a good size of data is to work with - but whatever that is, double it. Then split it in two. Do all the necessary work on one set and then have a look at how this pickle does on the untouched data. I can even run a classification report in that scenario. Sigh. \n",
        "\n",
        "Otherwise, I just get to witness how I'm still going to overfit the data if I don't have any new data. \n",
        "\n",
        "I can assume it's a hundred percent correct. \n",
        "\n",
        ":'(\n",
        "\n",
        "Oh. It is 100% correct, right? Because I resampled it and then ran the report?\n",
        "\n",
        "Sigh. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwYINJCrGrtF"
      },
      "source": [
        "#just wanted to look at something. \n",
        "results = pd.DataFrame(search.cv_results_)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "dX9j5qDOLc9H",
        "outputId": "f08af4e1-878d-444d-e200-caff2acf6f14"
      },
      "source": [
        "results.describe(include=\"all\")"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean_fit_time</th>\n",
              "      <th>std_fit_time</th>\n",
              "      <th>mean_score_time</th>\n",
              "      <th>std_score_time</th>\n",
              "      <th>param_pca__n_components</th>\n",
              "      <th>param_rf__n_estimators</th>\n",
              "      <th>params</th>\n",
              "      <th>split0_test_score</th>\n",
              "      <th>split1_test_score</th>\n",
              "      <th>split2_test_score</th>\n",
              "      <th>split3_test_score</th>\n",
              "      <th>split4_test_score</th>\n",
              "      <th>split5_test_score</th>\n",
              "      <th>split6_test_score</th>\n",
              "      <th>split7_test_score</th>\n",
              "      <th>split8_test_score</th>\n",
              "      <th>split9_test_score</th>\n",
              "      <th>mean_test_score</th>\n",
              "      <th>std_test_score</th>\n",
              "      <th>rank_test_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>35.000000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>35.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>35</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>35.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>35</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>{'pca__n_components': 6, 'rf__n_estimators': 100}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.163131</td>\n",
              "      <td>0.003184</td>\n",
              "      <td>0.008249</td>\n",
              "      <td>0.000528</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.765438</td>\n",
              "      <td>0.728571</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.745622</td>\n",
              "      <td>0.733021</td>\n",
              "      <td>0.755972</td>\n",
              "      <td>0.748009</td>\n",
              "      <td>0.715222</td>\n",
              "      <td>0.674005</td>\n",
              "      <td>0.724590</td>\n",
              "      <td>0.739045</td>\n",
              "      <td>0.046238</td>\n",
              "      <td>17.971429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.134631</td>\n",
              "      <td>0.004355</td>\n",
              "      <td>0.005505</td>\n",
              "      <td>0.001130</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.034845</td>\n",
              "      <td>0.034876</td>\n",
              "      <td>0.035682</td>\n",
              "      <td>0.033213</td>\n",
              "      <td>0.023037</td>\n",
              "      <td>0.066623</td>\n",
              "      <td>0.053059</td>\n",
              "      <td>0.049514</td>\n",
              "      <td>0.035512</td>\n",
              "      <td>0.042786</td>\n",
              "      <td>0.023769</td>\n",
              "      <td>0.008632</td>\n",
              "      <td>10.239731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.021069</td>\n",
              "      <td>0.000173</td>\n",
              "      <td>0.002504</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.693548</td>\n",
              "      <td>0.661290</td>\n",
              "      <td>0.725806</td>\n",
              "      <td>0.677419</td>\n",
              "      <td>0.704918</td>\n",
              "      <td>0.655738</td>\n",
              "      <td>0.655738</td>\n",
              "      <td>0.639344</td>\n",
              "      <td>0.590164</td>\n",
              "      <td>0.606557</td>\n",
              "      <td>0.685431</td>\n",
              "      <td>0.029493</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.048068</td>\n",
              "      <td>0.001011</td>\n",
              "      <td>0.004104</td>\n",
              "      <td>0.000137</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.741935</td>\n",
              "      <td>0.709677</td>\n",
              "      <td>0.790323</td>\n",
              "      <td>0.725806</td>\n",
              "      <td>0.721311</td>\n",
              "      <td>0.688525</td>\n",
              "      <td>0.704918</td>\n",
              "      <td>0.672131</td>\n",
              "      <td>0.655738</td>\n",
              "      <td>0.704918</td>\n",
              "      <td>0.723678</td>\n",
              "      <td>0.040129</td>\n",
              "      <td>9.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.128226</td>\n",
              "      <td>0.001949</td>\n",
              "      <td>0.005746</td>\n",
              "      <td>0.000219</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.758065</td>\n",
              "      <td>0.725806</td>\n",
              "      <td>0.806452</td>\n",
              "      <td>0.741935</td>\n",
              "      <td>0.721311</td>\n",
              "      <td>0.770492</td>\n",
              "      <td>0.754098</td>\n",
              "      <td>0.721311</td>\n",
              "      <td>0.672131</td>\n",
              "      <td>0.721311</td>\n",
              "      <td>0.742597</td>\n",
              "      <td>0.046761</td>\n",
              "      <td>18.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.232955</td>\n",
              "      <td>0.004120</td>\n",
              "      <td>0.009755</td>\n",
              "      <td>0.000318</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.790323</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.822581</td>\n",
              "      <td>0.774194</td>\n",
              "      <td>0.754098</td>\n",
              "      <td>0.819672</td>\n",
              "      <td>0.786885</td>\n",
              "      <td>0.770492</td>\n",
              "      <td>0.704918</td>\n",
              "      <td>0.745902</td>\n",
              "      <td>0.759704</td>\n",
              "      <td>0.051483</td>\n",
              "      <td>26.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.423691</td>\n",
              "      <td>0.025908</td>\n",
              "      <td>0.020006</td>\n",
              "      <td>0.006158</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.822581</td>\n",
              "      <td>0.790323</td>\n",
              "      <td>0.854839</td>\n",
              "      <td>0.806452</td>\n",
              "      <td>0.770492</td>\n",
              "      <td>0.852459</td>\n",
              "      <td>0.852459</td>\n",
              "      <td>0.803279</td>\n",
              "      <td>0.721311</td>\n",
              "      <td>0.786885</td>\n",
              "      <td>0.776838</td>\n",
              "      <td>0.066077</td>\n",
              "      <td>35.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        mean_fit_time  std_fit_time  ...  std_test_score  rank_test_score\n",
              "count       35.000000     35.000000  ...       35.000000        35.000000\n",
              "unique            NaN           NaN  ...             NaN              NaN\n",
              "top               NaN           NaN  ...             NaN              NaN\n",
              "freq              NaN           NaN  ...             NaN              NaN\n",
              "mean         0.163131      0.003184  ...        0.046238        17.971429\n",
              "std          0.134631      0.004355  ...        0.008632        10.239731\n",
              "min          0.021069      0.000173  ...        0.029493         1.000000\n",
              "25%          0.048068      0.001011  ...        0.040129         9.500000\n",
              "50%          0.128226      0.001949  ...        0.046761        18.000000\n",
              "75%          0.232955      0.004120  ...        0.051483        26.500000\n",
              "max          0.423691      0.025908  ...        0.066077        35.000000\n",
              "\n",
              "[11 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGsLx8VTMDCX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}