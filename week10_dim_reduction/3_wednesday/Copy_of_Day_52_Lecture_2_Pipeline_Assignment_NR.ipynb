{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Copy of Day 52 Lecture 2 Pipeline Assignment NR",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzcLuZowoMar"
      },
      "source": [
        "# Machine Learning Best Practices Assignment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpBBfm3toMat"
      },
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfqS_cMWoMa1"
      },
      "source": [
        "### Import the [Pima Indians Diabetes data set](https://tf-assets-prod.s3.amazonaws.com/tf-curric/data-science/pima_indians_diabetes.csv)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qO-4LVM7oMa1"
      },
      "source": [
        "data = pd.read_csv('https://tf-assets-prod.s3.amazonaws.com/tf-curric/data-science/pima_indians_diabetes.csv')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtNhl2RCoMa5"
      },
      "source": [
        "### Split the data into training and test sets, with the target variable being the outcome column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awGD68-2oMa5"
      },
      "source": [
        "\n",
        "X = data.drop(\"outcome\", axis=1)\n",
        "y = data.outcome\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6uDW6gSoMa9"
      },
      "source": [
        "### Train a Random Forest Classifier on the data without doing any transformations and print a classification report.\n",
        "\n",
        "This will provide us with a basis for comparison."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_TU-W51oMa-"
      },
      "source": [
        "model = RandomForestClassifier(random_state=1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9o8P7OeooMbC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2d2003c-d96c-4e01-90f3-894cf226708b"
      },
      "source": [
        "\n",
        "model.fit(X_train, y_train)\n",
        "y_pred_train = model.predict(X_train)\n",
        "y_pred_test = model.predict(X_test)\n",
        "print(\"Train Report\\n\")\n",
        "print(classification_report(y_train, y_pred_train))\n",
        "\n",
        "print(\"Test Report\\n\")\n",
        "print(classification_report(y_test, y_pred_test))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Report\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       380\n",
            "           1       1.00      1.00      1.00       196\n",
            "\n",
            "    accuracy                           1.00       576\n",
            "   macro avg       1.00      1.00      1.00       576\n",
            "weighted avg       1.00      1.00      1.00       576\n",
            "\n",
            "Test Report\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.87      0.82       120\n",
            "           1       0.72      0.57      0.64        72\n",
            "\n",
            "    accuracy                           0.76       192\n",
            "   macro avg       0.74      0.72      0.73       192\n",
            "weighted avg       0.75      0.76      0.75       192\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44gMhQKRoMbG"
      },
      "source": [
        "### Reduce the data down to 3 dimensions using PCA. Then do the train-test split, fit the model, and print a classification report.\n",
        "\n",
        "Compare these results to the previous ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbJpUBrgoMbH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11c7f847-b187-4786-df34-b05ecc38a28e"
      },
      "source": [
        "\n",
        "pca = PCA(n_components=3)\n",
        "X_pca = pca.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.25)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred_train = model.predict(X_train)\n",
        "y_pred_test = model.predict(X_test)\n",
        "print(\"Train Report\\n\")\n",
        "print(classification_report(y_train, y_pred_train))\n",
        "\n",
        "print(\"Test Report\\n\")\n",
        "print(classification_report(y_test, y_pred_test))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Report\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       372\n",
            "           1       1.00      1.00      1.00       204\n",
            "\n",
            "    accuracy                           1.00       576\n",
            "   macro avg       1.00      1.00      1.00       576\n",
            "weighted avg       1.00      1.00      1.00       576\n",
            "\n",
            "Test Report\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.80      0.79       128\n",
            "           1       0.57      0.52      0.54        64\n",
            "\n",
            "    accuracy                           0.71       192\n",
            "   macro avg       0.67      0.66      0.66       192\n",
            "weighted avg       0.70      0.71      0.70       192\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJN4OXS_t5WI"
      },
      "source": [
        "#went down 5 points from 76 to 71"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PSJ9pLRoMbL"
      },
      "source": [
        "### Fit the model and print a classification report again, but this time, perform the train-test split before you transform the data using PCA.\n",
        "\n",
        "Compare these results to the previous ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2kjPbPKoMbM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da45b9fa-67d8-415c-9e13-60f458aba8ae"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
        "\n",
        "pca = PCA(n_components=3)\n",
        "X_pca_train = pca.fit_transform(X_train)\n",
        "X_pca_test = pca.transform(X_test)\n",
        "\n",
        "model.fit(X_pca_train, y_train)\n",
        "\n",
        "y_pred_train = model.predict(X_pca_train)\n",
        "y_pred_test = model.predict(X_pca_test)\n",
        "print(\"Train Report\\n\")\n",
        "print(classification_report(y_train, y_pred_train))\n",
        "\n",
        "print(\"Test Report\\n\")\n",
        "print(classification_report(y_test, y_pred_test))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Report\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       371\n",
            "           1       1.00      1.00      1.00       205\n",
            "\n",
            "    accuracy                           1.00       576\n",
            "   macro avg       1.00      1.00      1.00       576\n",
            "weighted avg       1.00      1.00      1.00       576\n",
            "\n",
            "Test Report\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.85      0.80       129\n",
            "           1       0.59      0.43      0.50        63\n",
            "\n",
            "    accuracy                           0.71       192\n",
            "   macro avg       0.67      0.64      0.65       192\n",
            "weighted avg       0.70      0.71      0.70       192\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5QOmml0oMbO"
      },
      "source": [
        "### Using the Random Forest Classifier, perform 10-fold cross validation on the training set and print the mean cross validation score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ci-V57_oMbP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e8a6443-54b5-4ed9-a3dd-8ed44fdeff76"
      },
      "source": [
        "scores = cross_val_score(model, X_pca_train, y_train, cv=10)\n",
        "scores.mean()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7154567453115547"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7W_6glHPoMbT"
      },
      "source": [
        "### Create a pipeline with a PCA step and a Random Forest Classifier step. Perform the train-test split again, fit the pipeline, and then generate a classification report.\n",
        "\n",
        "Compare these results to the previous ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sy4DFzsoMbU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0fdceb6-8b76-4a76-cce1-182f86a83dc0"
      },
      "source": [
        "steps = [(\"pca\", PCA(n_components=3)), (\"rf\", model)]\n",
        "\n",
        "pipeline = Pipeline(steps)\n",
        "print(pipeline)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "y_pred_train = pipeline.predict(X_train)\n",
        "y_pred_test = pipeline.predict(X_test)\n",
        "print(\"Train Report\\n\")\n",
        "print(classification_report(y_train, y_pred_train))\n",
        "\n",
        "print(\"Test Report\\n\")\n",
        "print(classification_report(y_test, y_pred_test))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pipeline(memory=None,\n",
            "         steps=[('pca',\n",
            "                 PCA(copy=True, iterated_power='auto', n_components=3,\n",
            "                     random_state=None, svd_solver='auto', tol=0.0,\n",
            "                     whiten=False)),\n",
            "                ('rf',\n",
            "                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
            "                                        class_weight=None, criterion='gini',\n",
            "                                        max_depth=None, max_features='auto',\n",
            "                                        max_leaf_nodes=None, max_samples=None,\n",
            "                                        min_impurity_decrease=0.0,\n",
            "                                        min_impurity_split=None,\n",
            "                                        min_samples_leaf=1, min_samples_split=2,\n",
            "                                        min_weight_fraction_leaf=0.0,\n",
            "                                        n_estimators=100, n_jobs=None,\n",
            "                                        oob_score=False, random_state=1,\n",
            "                                        verbose=0, warm_start=False))],\n",
            "         verbose=False)\n",
            "Train Report\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       375\n",
            "           1       1.00      1.00      1.00       201\n",
            "\n",
            "    accuracy                           1.00       576\n",
            "   macro avg       1.00      1.00      1.00       576\n",
            "weighted avg       1.00      1.00      1.00       576\n",
            "\n",
            "Test Report\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.82      0.77       125\n",
            "           1       0.57      0.43      0.49        67\n",
            "\n",
            "    accuracy                           0.69       192\n",
            "   macro avg       0.65      0.63      0.63       192\n",
            "weighted avg       0.67      0.69      0.68       192\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0RpnV0xoMbX"
      },
      "source": [
        "### Using the pipeline you built, perform 10-fold cross validation on the training set and print the mean cross validation score.\n",
        "\n",
        "How does this score compare to the previous one?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IKeBBZXoMbY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "344f0edd-770b-42fe-d3c2-12a6bb299ddb"
      },
      "source": [
        "scores = cross_val_score(pipeline, X_train, y_train, cv=10)\n",
        "scores.mean()\n",
        "#different because we re run the same data"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7203569267997579"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DLDuyOqoMba"
      },
      "source": [
        "### Use GridSearchCV to find the optimal set of parameters from the ones below.\n",
        "\n",
        "- PCA Number of Components: 2, 3, 4, 5, 6, 7, 8\n",
        "- Random Forest Number of Estimators: 10, 20, 50, 100, 200"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbsRhIdZoMbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fd82633-1b03-4452-d9bb-732f7d3dc736"
      },
      "source": [
        "steps = [(\"pca\", PCA(n_components=3)), (\"rf\", model)]\n",
        "\n",
        "pipeline = Pipeline(steps)\n",
        "print(pipeline)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
        "\n",
        "grid_params = {\n",
        "    \"pca__n_components\": [2, 3, 4, 5, 6, 7, 8],\n",
        "    \"rf__n_estimators\": [10, 20, 50, 100, 200],\n",
        "}\n",
        "grid = GridSearchCV(pipeline, param_grid=grid_params, cv=10, verbose=True, n_jobs=-1)\n",
        "grid.fit(X_train, y_train)\n",
        "grid.best_params_"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pipeline(memory=None,\n",
            "         steps=[('pca',\n",
            "                 PCA(copy=True, iterated_power='auto', n_components=3,\n",
            "                     random_state=None, svd_solver='auto', tol=0.0,\n",
            "                     whiten=False)),\n",
            "                ('rf',\n",
            "                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
            "                                        class_weight=None, criterion='gini',\n",
            "                                        max_depth=None, max_features='auto',\n",
            "                                        max_leaf_nodes=None, max_samples=None,\n",
            "                                        min_impurity_decrease=0.0,\n",
            "                                        min_impurity_split=None,\n",
            "                                        min_samples_leaf=1, min_samples_split=2,\n",
            "                                        min_weight_fraction_leaf=0.0,\n",
            "                                        n_estimators=100, n_jobs=None,\n",
            "                                        oob_score=False, random_state=1,\n",
            "                                        verbose=0, warm_start=False))],\n",
            "         verbose=False)\n",
            "Fitting 10 folds for each of 35 candidates, totalling 350 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  72 tasks      | elapsed:    7.8s\n",
            "[Parallel(n_jobs=-1)]: Done 350 out of 350 | elapsed:   43.4s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'pca__n_components': 8, 'rf__n_estimators': 200}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cVJ994qoMbd"
      },
      "source": [
        "### Using the best estimator pipeline from above, fit the pipeline to the training set and generate a classification report showing the results.\n",
        "\n",
        "Compare these results to the previous ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0N6Mv3KoMbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cd7f723-9374-409e-bb37-1f25b49df943"
      },
      "source": [
        "grid.best_estimator_.fit(X_train, y_train)\n",
        "\n",
        "y_pred_train = grid.best_estimator_.predict(X_train)\n",
        "y_pred_test = grid.best_estimator_.predict(X_test)\n",
        "print(\"Train Report\\n\")\n",
        "print(classification_report(y_train, y_pred_train))\n",
        "\n",
        "print(\"Test Report\\n\")\n",
        "print(classification_report(y_test, y_pred_test))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Report\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       383\n",
            "           1       1.00      1.00      1.00       193\n",
            "\n",
            "    accuracy                           1.00       576\n",
            "   macro avg       1.00      1.00      1.00       576\n",
            "weighted avg       1.00      1.00      1.00       576\n",
            "\n",
            "Test Report\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.90      0.81       117\n",
            "           1       0.76      0.51      0.61        75\n",
            "\n",
            "    accuracy                           0.74       192\n",
            "   macro avg       0.75      0.70      0.71       192\n",
            "weighted avg       0.75      0.74      0.73       192\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loHniiPLoMbj"
      },
      "source": [
        "### Fit the best estimator pipeline to the entire data set and save your model to disk using pickle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Y8ESntxoMbk"
      },
      "source": [
        "final_model = grid.best_estimator_.fit(X, y)\n",
        "\n",
        "with open(\"final_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump(final_model, f)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HdA-rQGoMbn"
      },
      "source": [
        "### Load the model you saved to disk, create a copy of the features in the data, and generate a set of predictions for those features. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBnP7dKhoMbn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fd0b7d6-a734-41cc-d89d-1de1dce28a9b"
      },
      "source": [
        "with open(\"final_model.pkl\", \"rb\") as f:\n",
        "    imported_model = pickle.load(f)\n",
        "\n",
        "# pretend new data\n",
        "new_data = X.copy()\n",
        "\n",
        "preds = imported_model.predict(new_data)\n",
        "preds"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,\n",
              "       1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
              "       1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
              "       0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,\n",
              "       1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
              "       1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
              "       1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
              "       1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
              "       0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
              "       1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
              "       0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
              "       0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
              "       1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
              "       1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
              "       1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
              "       1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
              "       0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
              "       0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,\n",
              "       0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
              "       1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    }
  ]
}