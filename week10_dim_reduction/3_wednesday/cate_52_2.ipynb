{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "cate_52_2.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzcLuZowoMar"
      },
      "source": [
        "# Machine Learning Best Practices Assignment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpBBfm3toMat"
      },
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfqS_cMWoMa1"
      },
      "source": [
        "### Import the [Pima Indians Diabetes data set](https://tf-assets-prod.s3.amazonaws.com/tf-curric/data-science/pima_indians_diabetes.csv)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qO-4LVM7oMa1"
      },
      "source": [
        "data = pd.read_csv('https://tf-assets-prod.s3.amazonaws.com/tf-curric/data-science/pima_indians_diabetes.csv')"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtNhl2RCoMa5"
      },
      "source": [
        "### Split the data into training and test sets, with the target variable being the outcome column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awGD68-2oMa5"
      },
      "source": [
        "X = data.drop('outcome', axis=1)\n",
        "y = data['outcome']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6uDW6gSoMa9"
      },
      "source": [
        "### Train a Random Forest Classifier on the data without doing any transformations and print a classification report.\n",
        "\n",
        "This will provide us with a basis for comparison."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_TU-W51oMa-"
      },
      "source": [
        "model = RandomForestClassifier(random_state=1)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9o8P7OeooMbC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "058be343-1b51-4e4d-f1d0-7b3f6c317245"
      },
      "source": [
        "model.fit(X_train, y_train)\n",
        "preds = model.predict(X_test)\n",
        "print(classification_report(y_test, preds))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.85      0.80       114\n",
            "           1       0.73      0.58      0.64        78\n",
            "\n",
            "    accuracy                           0.74       192\n",
            "   macro avg       0.74      0.71      0.72       192\n",
            "weighted avg       0.74      0.74      0.73       192\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44gMhQKRoMbG"
      },
      "source": [
        "### Reduce the data down to 3 dimensions using PCA. Then do the train-test split, fit the model, and print a classification report.\n",
        "\n",
        "Compare these results to the previous ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbJpUBrgoMbH"
      },
      "source": [
        "pca = PCA(n_components=3)\n",
        "pca_components = pca.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(pca_components, y)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGwroIxU3DpL",
        "outputId": "88fe2c7a-72db-4107-d2e7-bbde40ccba03"
      },
      "source": [
        "model = RandomForestClassifier(random_state=1)\n",
        "model.fit(X_train, y_train)\n",
        "preds = model.predict(X_test)\n",
        "print(classification_report(preds, y_test))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.77      0.78       124\n",
            "           1       0.59      0.62      0.60        68\n",
            "\n",
            "    accuracy                           0.71       192\n",
            "   macro avg       0.69      0.69      0.69       192\n",
            "weighted avg       0.72      0.71      0.71       192\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PSJ9pLRoMbL"
      },
      "source": [
        "### Fit the model and print a classification report again, but this time, perform the train-test split before you transform the data using PCA.\n",
        "\n",
        "Compare these results to the previous ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2kjPbPKoMbM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be74939f-73ce-43c3-ae18-7c2d62e81a31"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "pca = PCA(n_components=3)\n",
        "pca_X_train = pca.fit_transform(X_train)\n",
        "pca_X_test = pca.transform(X_test)\n",
        "model = RandomForestClassifier(random_state=1)\n",
        "model.fit(pca_X_train, y_train)\n",
        "preds = model.predict(pca_X_test)\n",
        "print(classification_report(y_test, preds))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.84      0.80       127\n",
            "           1       0.62      0.49      0.55        65\n",
            "\n",
            "    accuracy                           0.72       192\n",
            "   macro avg       0.69      0.67      0.67       192\n",
            "weighted avg       0.71      0.72      0.72       192\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5QOmml0oMbO"
      },
      "source": [
        "### Using the Random Forest Classifier, perform 10-fold cross validation on the training set and print the mean cross validation score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ci-V57_oMbP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc9fbeb1-68b4-4ac9-f78c-1b9196d1cdb9"
      },
      "source": [
        "scores = cross_val_score(model, pca_X_train, y_train, cv=10)\n",
        "print('10-fold cross validation mean score: ', round(scores.mean(), 2))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10-fold cross validation mean score:  0.73\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7W_6glHPoMbT"
      },
      "source": [
        "### Create a pipeline with a PCA step and a Random Forest Classifier step. Perform the train-test split again, fit the pipeline, and then generate a classification report.\n",
        "\n",
        "Compare these results to the previous ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sy4DFzsoMbU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42809ccc-2b79-4db6-de40-ebaef4c34a25"
      },
      "source": [
        "pipeline = Pipeline([('pca', PCA(n_components=3)),\n",
        "                     ('rf', RandomForestClassifier(random_state=1))])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "preds = pipeline.predict(X_test)\n",
        "\n",
        "print(classification_report(preds, y_test))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.76      0.78       139\n",
            "           1       0.46      0.55      0.50        53\n",
            "\n",
            "    accuracy                           0.70       192\n",
            "   macro avg       0.64      0.65      0.64       192\n",
            "weighted avg       0.72      0.70      0.71       192\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0RpnV0xoMbX"
      },
      "source": [
        "### Using the pipeline you built, perform 10-fold cross validation on the training set and print the mean cross validation score.\n",
        "\n",
        "How does this score compare to the previous one?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IKeBBZXoMbY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5a805a3-a0dd-453b-a409-4a6aef70f5c9"
      },
      "source": [
        "scores = cross_val_score(pipeline, X_train, y_train, cv=10)\n",
        "print('10-fold cross validation mean score: ', round(scores.mean(), 2))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10-fold cross validation mean score:  0.74\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7tcGdfD6qrd"
      },
      "source": [
        "This score is slightly higer than for the last model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DLDuyOqoMba"
      },
      "source": [
        "### Use GridSearchCV to find the optimal set of parameters from the ones below.\n",
        "\n",
        "- PCA Number of Components: 2, 3, 4, 5, 6, 7, 8\n",
        "- Random Forest Number of Estimators: 10, 20, 50, 100, 200"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbsRhIdZoMbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cb9a90e-f01a-4129-ae21-203b909a2fb9"
      },
      "source": [
        "pipeline = Pipeline([('pca', PCA()),\n",
        "                     ('rf', RandomForestClassifier(random_state=1))])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
        "\n",
        "parameters = {'pca__n_components': [2, 3, 4, 5, 6, 7, 8],\n",
        "              'rf__n_estimators': [10, 20, 50, 100, 200],\n",
        "              }\n",
        "search = GridSearchCV(pipeline, parameters, cv=10)\n",
        "search.fit(X_train, y_train)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=10, error_score=nan,\n",
              "             estimator=Pipeline(memory=None,\n",
              "                                steps=[('pca',\n",
              "                                        PCA(copy=True, iterated_power='auto',\n",
              "                                            n_components=None,\n",
              "                                            random_state=None,\n",
              "                                            svd_solver='auto', tol=0.0,\n",
              "                                            whiten=False)),\n",
              "                                       ('rf',\n",
              "                                        RandomForestClassifier(bootstrap=True,\n",
              "                                                               ccp_alpha=0.0,\n",
              "                                                               class_weight=None,\n",
              "                                                               criterion='gini',\n",
              "                                                               max_depth=None,\n",
              "                                                               max_features='auto',\n",
              "                                                               max_leaf_nodes=None,\n",
              "                                                               max_sampl...\n",
              "                                                               min_samples_split=2,\n",
              "                                                               min_weight_fraction_leaf=0.0,\n",
              "                                                               n_estimators=100,\n",
              "                                                               n_jobs=None,\n",
              "                                                               oob_score=False,\n",
              "                                                               random_state=1,\n",
              "                                                               verbose=0,\n",
              "                                                               warm_start=False))],\n",
              "                                verbose=False),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'pca__n_components': [2, 3, 4, 5, 6, 7, 8],\n",
              "                         'rf__n_estimators': [10, 20, 50, 100, 200]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cVJ994qoMbd"
      },
      "source": [
        "### Using the best estimator pipeline from above, fit the pipeline to the training set and generate a classification report showing the results.\n",
        "\n",
        "Compare these results to the previous ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0N6Mv3KoMbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6264b7b-0698-44f8-f973-94233894152f"
      },
      "source": [
        "pipeline = search.best_estimator_\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "preds = pipeline.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, preds))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       126\n",
            "           1       0.78      0.68      0.73        66\n",
            "\n",
            "    accuracy                           0.82       192\n",
            "   macro avg       0.81      0.79      0.80       192\n",
            "weighted avg       0.82      0.82      0.82       192\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7_eiXtf8U64"
      },
      "source": [
        "These results are significantly better than any of the previous ones, which makes sense given we've used the best estimator from GridSearchCV. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loHniiPLoMbj"
      },
      "source": [
        "### Fit the best estimator pipeline to the entire data set and save your model to disk using pickle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Y8ESntxoMbk"
      },
      "source": [
        "import pickle\n",
        "\n",
        "pipeline.fit(X,y)\n",
        "\n",
        "with open('pipeline.pkl', 'wb') as f:\n",
        "  pickle.dump(pipeline, f)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HdA-rQGoMbn"
      },
      "source": [
        "### Load the model you saved to disk, create a copy of the features in the data, and generate a set of predictions for those features. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBnP7dKhoMbn"
      },
      "source": [
        "copy = X.copy()\n",
        "\n",
        "with open('pipeline.pkl', 'rb') as f:\n",
        "  loaded_pipe = pickle.load(f)\n",
        "\n",
        "preds = loaded_pipe.predict(copy)"
      ],
      "execution_count": 42,
      "outputs": []
    }
  ]
}